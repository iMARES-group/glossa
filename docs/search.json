[
  {
    "objectID": "under_construction.html",
    "href": "under_construction.html",
    "title": "Under construction",
    "section": "",
    "text": "Our team is hard at work to provide you with comprehensive guides and resources.\nThank you for your patience. Stay tuned for updates!"
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html",
    "title": "Example 1",
    "section": "",
    "text": "This example demonstrates how to use the GLOSSA framework to model the suitable habitat of Thunnus albacares (yellowfin tuna) on a global scale under different climate scenarios. We will use occurrence data from 1850 to 2014, downloaded from OBIS (Accessed on 26/08/2024), and historical and future environmental projections from ISIMIP (https://data.isimip.org/). The goal is to predict how habitat suitability for Thunnus albacares may change under the SSP1-2.6 (sustainable development) and SSP5-8.5 (high emissions) climate scenarios.\nFirst, we will load the glossa package, as well as terra and sf to work with spatial rasters and vector data. We also load robis for downloading species occurrences. Additionally, the dplyr package will be used for data manipulation.\n\nlibrary(glossa)\nlibrary(robis)\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)"
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#introduction",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#introduction",
    "title": "Example 1",
    "section": "",
    "text": "This example demonstrates how to use the GLOSSA framework to model the suitable habitat of Thunnus albacares (yellowfin tuna) on a global scale under different climate scenarios. We will use occurrence data from 1850 to 2014, downloaded from OBIS (Accessed on 26/08/2024), and historical and future environmental projections from ISIMIP (https://data.isimip.org/). The goal is to predict how habitat suitability for Thunnus albacares may change under the SSP1-2.6 (sustainable development) and SSP5-8.5 (high emissions) climate scenarios.\nFirst, we will load the glossa package, as well as terra and sf to work with spatial rasters and vector data. We also load robis for downloading species occurrences. Additionally, the dplyr package will be used for data manipulation.\n\nlibrary(glossa)\nlibrary(robis)\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)"
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#data-preparation",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#data-preparation",
    "title": "Example 1",
    "section": "Data preparation",
    "text": "Data preparation\n\nDownload occurrence data\nWe will download occurrence data for Thunnus albacares from the OBIS database (https://obis.org/taxon/127027) using the robis package. After that, we’ll select the necessary columns for GLOSSA (decimalLongitude, decimalLatitude, timestamp, and pa), and since all records indicate presences, we’ll replace them with a value of 1 (where 1 represents presence and 0 absence, as required by GLOSSA). We will remove records with missing values and filter the data to include only records from 1850 to 2014, aligning with the temporal coverage of our environmental layers.\n\n# Download data from OBIS\nalbacares &lt;- robis::occurrence(scientificname = \"Thunnus albacares\")\n\n# Format data to fit GLOSSA\n# Select and rename columns of interest\nalbacares &lt;- albacares[, c(\"decimalLongitude\", \"decimalLatitude\", \n                           \"date_year\", \"occurrenceStatus\")]\ncolnames(albacares) &lt;- c(\"decimalLongitude\", \"decimalLatitude\", \"timestamp\", \"pa\")\n\n# Convert presence data to 1 (for presences)\ntable(albacares$pa)\n# In this case, we only have presences\nalbacares &lt;- albacares[albacares$pa %in% c(\"present\", \"Present\", \"Presente\", \"P\"), ] \nalbacares$pa &lt;- 1\n\n# Remove incomplete records (with NA values)\nalbacares &lt;- albacares[complete.cases(albacares), ]\n\n# Filter study period to match environmental variables (1850-2014)\nalbacares &lt;- albacares[albacares$timestamp &gt;= 1850 & albacares$timestamp &lt;= 2014, ]\n\n# Save to file\nwrite.table(as.data.frame(albacares), file = \"data/Thunnus_albacares_occ.csv\", \n            sep = \"\\t\", dec = \".\", quote = FALSE)\n\n\n\nDownload environmental data\nNext, we will download environmental data from ISIMIP, using the GFDL-ESM4 Earth System Model from the ISIMIP3b climate dataset. The variables we’ll download include:\n\nSea surface temperature (tos)\nSea surface water salinity (so)\nPhytoplankton content vertically integrated over all oceans levels (phyc)\n\nAdditionally, we will download bathymetry data from the ETOPO 2022 Global Relief Model by NOAA to include in our analysis. We downloaded the bedrock elevation netCDF version ETOPO 2022 with a 60 arc-second resolution.\n\n# Load environmental data layers\nenv_data &lt;- list(\n  tos = terra::rast(\"data/gfdl-esm4_r1i1p1f1_historical_tos_60arcmin_global_monthly_1850_2014.nc\"),\n  so = terra::rast(\"data/gfdl-esm4_r1i1p1f1_historical_so-surf_60arcmin_global_monthly_1850_2014.nc\"),\n  phyc = terra::rast(\"data/gfdl-esm4_r1i1p1f1_historical_phyc-vint_60arcmin_global_monthly_1850_2014.nc\")\n)\n\n# Process data to calculate annual means for each variable\nenv_data_year &lt;- list()\nn &lt;- 1\nfor (i in seq(from = 1056, to = (1980 - 12), by = 12)){\n  for (j in names(env_data)){\n    env_data_year[[j]][[n]] &lt;- terra::mean(env_data[[j]][[i:(i+11)]])\n  }\n  n &lt;- n + 1\n}\n\n# Prepare bathymetry data and resample to match environmental data resolution\nbat &lt;- terra::rast(\"data/ETOPO_2022_v1_60s_N90W180_bed.nc\")\nbat &lt;- -1*bat\nbat &lt;- terra::aggregate(bat, fact = 60, fun = \"mean\")\nr &lt;- terra::rast(terra::ext(env_data_year[[1]][[1]]), \n                 res = terra::res(env_data_year[[1]][[1]]))\nbat &lt;- terra::resample(bat, r)\nfor (i in seq_len(length(env_data_year[[1]]))){\n  env_data_year[[\"bat\"]][[i]] &lt;- bat\n}\n\n# Save processed layers to files\ndir.create(\"data/fit_layers\")\ndir.create(\"data/fit_layers/bat\")\ndir.create(\"data/fit_layers/tos\")\ndir.create(\"data/fit_layers/so\")\ndir.create(\"data/fit_layers/phyc\")\nfor (i in seq_len(length(env_data_year[[1]]))){\n  for (j in names(env_data_year)){\n    terra::writeRaster(\n      env_data_year[[j]][[i]], \n      filename = paste0(\"data/fit_layers/\", j ,\"/\", j, \"_\", i, \".tif\")\n    )\n  }\n}\n\n# Zip the output files\nzip(zipfile = \"data/fit_layers.zip\", files = \"data/fit_layers\")\n\n\n\nClimate projections: SSP1-2.6 and SSP5-8.5\nWe will now download climate projections for the SSP1-2.6 and SSP5-8.5 scenarios, which represent two different future climate pathways. The SSP1-2.6 scenario assumes sustainable development and slower reductions in CO2 emissions, while the SSP5-8.5 scenario assumes a high-emissions future.\n\nSSP1-2.6\nThe following environmental variables will be downloaded for the SSP1-2.6 scenario:\n\ntos: Sea surface temperature\nso: Sea surface water salinity\nphyc: Phytoplankton content\n\n\n# Load SSP1-2.6 climate projection data\nenv_data &lt;- list(\n  tos = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp126_tos_60arcmin_global_monthly_2015_2100.nc\"),\n  so = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp126_so-surf_60arcmin_global_monthly_2015_2100.nc\"),\n  phyc = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp126_phyc-vint_60arcmin_global_monthly_2015_2100.nc\")\n)\n\n# Extract data for 2025, 2050, and 2100\nenv_data_year &lt;- list()\nn &lt;- 1\nfor (i in c(121, 421, 1021)){\n  for (j in names(env_data)){\n    env_data_year[[j]][[n]] &lt;- terra::mean(env_data[[j]][[i:(i+11)]])\n  }\n  n &lt;- n + 1\n}\n\n# Add bathymetry data to the layers\nfor (i in seq_len(length(env_data_year[[1]]))){\n  env_data_year[[\"bat\"]][[i]] &lt;- bat\n}\n\n# Save projection data to files\ndir.create(\"data/proj_ssp126\")\ndir.create(\"data/proj_ssp126/bat\")\ndir.create(\"data/proj_ssp126/tos\")\ndir.create(\"data/proj_ssp126/so\")\ndir.create(\"data/proj_ssp126/phyc\")\nfor (i in seq_len(length(env_data_year[[1]]))){\n  for (j in names(env_data_year)){\n    terra::writeRaster(\n      env_data_year[[j]][[i]], \n      filename = paste0(\"data/proj_ssp126/\", j ,\"/\", j, \"_\", i, \".tif\")\n    )\n  }\n}\n\n# Zip the output files\nzip(zipfile = \"data/proj_ssp126.zip\", files = \"data/proj_ssp126\")\n\n\n\nSSP5-8.5\nSimilarly, we will download environmental data for the SSP5-8.5 scenario.\n\ntos: Sea surface temperature\nso: Sea surface water salinity\nphyc: Phytoplankton content\n\n\n# Load SSP5-8.5 climate projection data\nenv_data &lt;- list(\n  tos = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp585_tos_60arcmin_global_monthly_2015_2100.nc\"),\n  so = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp585_so-surf_60arcmin_global_monthly_2015_2100.nc\"),\n  phyc = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp585_phyc-vint_60arcmin_global_monthly_2015_2100.nc\")\n)\n\n# Extract data for 2025, 2050, and 2100\nenv_data_year &lt;- list()\nn &lt;- 1\nfor (i in c(121, 421, 1021)){\n  for (j in names(env_data)){\n    env_data_year[[j]][[n]] &lt;- terra::mean(env_data[[j]][[i:(i+11)]])\n  }\n  n &lt;- n + 1\n}\n\n# Add bathymetry data to the layers\nfor (i in seq_len(length(env_data_year[[1]]))){\n  env_data_year[[\"bat\"]][[i]] &lt;- bat\n}\n\n# Save projection data to files\ndir.create(\"data/proj_ssp585\")\ndir.create(\"data/proj_ssp585/bat\")\ndir.create(\"data/proj_ssp585/tos\")\ndir.create(\"data/proj_ssp585/so\")\ndir.create(\"data/proj_ssp585/phyc\")\nfor (i in seq_len(length(env_data_year[[1]]))){\n  for (j in names(env_data_year)){\n    terra::writeRaster(\n      env_data_year[[j]][[i]], \n      filename = paste0(\"data/proj_ssp585/\", j ,\"/\", j, \"_\", i, \".tif\")\n    )\n  }\n}\n\n# Zip the output files\nzip(zipfile = \"data/proj_ssp585.zip\", files = \"data/proj_ssp585\")"
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#glossa-modeling",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#glossa-modeling",
    "title": "Example 1",
    "section": "GLOSSA modeling",
    "text": "GLOSSA modeling\nNow that we have our environmental data ready, we can proceed with fitting the habitat suitability model using GLOSSA.\n\nrun_glossa()\n\nWe upload the occurrence file for T. albacares along with the environmental layers for model fitting and the two projection scenarios. Since our goal is to compute suitable habitat, we select the Model fitting and Model projection options from the Suitable habitat model. Additionally, we enable Functional responses in the Others section to compute the response curves.\nIn the advanced options, we adjust the settings as follows: we select a thinning precision of 2 and standardize the environmental data. We choose the BART model (Chipman, et al., 2010; Dorie, 2024) and set the seed to 4572 for reproducibility.\n\n\n\nAnalysis options set in GLOSSA for modeling the distribution of Thunnus albacares."
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#results",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#results",
    "title": "Example 1",
    "section": "Results",
    "text": "Results\nIn the Reports section, we can observe the suitable habitat projections for both climate scenarios (SSP1-2.6 and SSP5-8.5). The model was built using a total of 91508 presence records. In the model summary section, we can see the ROC curve with a high AUC value, indicating good model performance. The middle plot shows the optimal cutoff point, and the rightmost plot displays the distribution of the fitted values.\n\n\n\nModel summary.\n\n\nAfter selecting the desired climate scenario in the GLOSSA predictions box, we can view the evolution of the potential suitable area over time for the projected layers (2025, 2050, and 2100) in the top section. For both climate scenarios, a reduction in the potential suitable habitat area is evident, with a more pronounced decline under the high-emission scenario (20% reduction) compared to the low-emission scenario (8.3% reduction).\n\n\n\nDecrease in potential suitable habitat.\n\n\nThe GLOSSA predictions tab allows us to explore projections for the uploaded layers across different years and climate scenarios. The results reveal a decline in occurrence probability from 2025 to 2100 for both SSP1-2.6 (low-emission) and SSP5-8.5 (high-emission) scenarios. Under the SSP5-8.5 scenario, the decrease in occurrence probability is not only more pronounced, but the species’ distribution has also shifted away from equatorial and tropical regions towards subtropical zones, highlighting the impact of higher emissions on species distributions.\n\n\n\nThunnus albacares GLOSSA results. a) Mean of posterior distribution of occurrence probability for suitable habitat model in both climate scenarios. b) Variable importance. c) Response curve for sea surface temperature.\n\n\nThe Variable importance plot shows which predictor variables are playing a more important role in predicting the species occurrences. Sea surface temperature appears as the variable that most improves predictive performance, followed by salinity and bathymetry, which also contribute meaningfully to the model’s accuracy. In contrast, primary productivity has a lower importance score, suggesting it provides less predictive power compared to the other variables in this model.\nFinally, we have computed the response curve for each predictor variable. The response curve for sea surface temperature indicates that the probability of species presence peaks at around 25°C, illustrating the species’ preference for moderate water temperatures. As temperatures deviate from this optimal range, the probability of occurrence declines sharply, underscoring the species’ vulnerability to temperature changes."
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#conclusion",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#conclusion",
    "title": "Example 1",
    "section": "Conclusion",
    "text": "Conclusion\nThis example shows how GLOSSA can be used to model the global distribution of Thunnus albacares and assess potential shifts in its habitat under different climate scenarios."
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#computation-time",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#computation-time",
    "title": "Example 1",
    "section": "Computation time",
    "text": "Computation time\nThe analysis was performed on a single Windows 11 machine equipped with 64 GB of RAM and an Intel(R) Core(TM) i7-1165G7 processor. This processor features 4 cores and 8 threads, with a base clock speed of 2.80 GHz. The following table summarizes the computation times for various stages of the GLOSSA analysis. This provides an overview of the computational resources required for each step in the analysis.\n\nTable 1. Computation times for different stages of the GLOSSA analysis for the global scale example.\n\n\n\n\n\n\n\n\nTask\nExecution Time\n\n\n\n\nLoading input data\n6.38 secs\n\n\nProcessing P/A coordinates\n0.69 secs\n\n\nProcessing covariate layers\n3.93 secs\n\n\nBuilding model matrix\n121.76 secs\n\n\nFitting native range models\n5.26 mins\n\n\nVariable importance (native range)\n165.21 mins\n\n\nP/A cutoff (native range)\n4.23 mins\n\n\nProjections on fit layers (native range)\n165.90 mins\n\n\nNative range projections\n4.15 mins\n\n\nNative range\n0.004 mins\n\n\nFitting suitable habitat models\n5.26 mins\n\n\nVariable importance (suitable habitat)\n165.21 mins\n\n\nP/A cutoff (suitable habitat)\n4.23 mins\n\n\nProjections on fit layers (suitable habitat)\n165.90 mins\n\n\nSuitable habitat projections\n4.15 mins\n\n\nHabitat suitability\n0.004 mins\n\n\nSuitable habitat\n179.54 mins\n\n\nComputing functional responses\n273.79 mins\n\n\nCross-validation\n89.25 mins\n\n\nModel summary\n3.93 mins\n\n\nTotal GLOSSA analysis\n548.72 mins"
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#references",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#references",
    "title": "Example 1",
    "section": "References",
    "text": "References\n\nChipman, H. A., George, E. I., & McCulloch, R. E. (2010). BART: Bayesian additive regression trees.\nDorie V (2024). dbarts: Discrete Bayesian Additive Regression Trees Sampler. R package version 0.9-28, https://CRAN.R-project.org/package=dbarts.\nISIMIP Project, 2024. Climate projections data. Available at https://data.isimip.org/.\nOBIS (2024) Ocean Biodiversity Information System. Intergovernmental Oceanographic Commission of UNESCO. https://obis.org.\nNOAA National Centers for Environmental Information, 2022. ETOPO 2022 Global Relief Model. Available at https://www.ncei.noaa.gov/products/etopo-global-relief-model."
  },
  {
    "objectID": "pages/tutorials_examples/index.html",
    "href": "pages/tutorials_examples/index.html",
    "title": "Tutorials and examples",
    "section": "",
    "text": "Example 1\n\n\nWorldwide suitable habitat of Thunnus albacares\n\n\n13 min\n\n\nThis example demonstrates how to use the GLOSSA framework to model the suitable habitat of Thunnus albacares (yellowfin tuna) on a global scale under different climate…\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\n\n\nDistribution of the loggerhead sea turtle in the Mediterranean Sea\n\n\n17 min\n\n\nThis vignette provides a detailed example of fitting a single-species distribution model within a user-defined region.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3\n\n\nPotential risk areas of Siganus luridus in the Greek Seas\n\n\n12 min\n\n\nThis example shows how to use GLOSSA to predict the suitable habitat of Siganus luridus in the Greek Seas and identify potential areas at risk of invasion.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/documentation/working_with_input_data.html",
    "href": "pages/documentation/working_with_input_data.html",
    "title": "Preparing your data",
    "section": "",
    "text": "To ensure smooth and accurate analyses in GLOSSA, your data must be formatted correctly. This guide will walk you through preparing the necessary files: species occurrence data, environmental data, and optional projection layers and study area polygons."
  },
  {
    "objectID": "pages/documentation/working_with_input_data.html#species-occurrence-data",
    "href": "pages/documentation/working_with_input_data.html#species-occurrence-data",
    "title": "Preparing your data",
    "section": "Species occurrence data",
    "text": "Species occurrence data\nSpecies occurrence data must be provided in a tab-separated file (TSV, tab-separated CSV, etc.). This file should include the following columns:\n\ndecimalLongitude: Longitude of the occurrence point in decimal degrees.\ndecimalLatitude: Latitude of the occurrence point in decimal degrees.\npa: Presence (1) or absence (0) of the species. If this column is missing, GLOSSA will assume all rows represent presences (1, presence-only) and will generate pseudo-absences before the modeling step.\ntimestamp: The time when the occurrence was recorded. This column is optional. If used, GLOSSA will match each occurrence to the environmental data from that specific time period. If omitted, GLOSSA will assume all occurrences occurred at the same time.\n\n\nExample:\ndecimalLongitude decimalLatitude pa   timestamp\n5.42909          43.20937        1    1\n-43.05000        49.03000        0    1\n-2.52369         47.29234        1    2\n34.05400         -26.91300       1    3\n\n\n\n\n\n\nTip\n\n\n\n\nEnsure that all occurrence points fall within the study area defined by your environmental data to avoid losing data points due to missing covariate values.\nDouble-check for formatting errors or missing columns before uploading to avoid processing issues."
  },
  {
    "objectID": "pages/documentation/working_with_input_data.html#environmental-data",
    "href": "pages/documentation/working_with_input_data.html#environmental-data",
    "title": "Preparing your data",
    "section": "Environmental data",
    "text": "Environmental data\nEnvironmental data is provided as raster layers in formats like .tif or .nc (NetCDF). These layers are used as predictors for species distributions and should represent variables like temperature, salinity, or other relevant data. All environmental layers must be uploaded as a ZIP file, with each variable organized into separate subdirectories. Each subdirectory should contain raster files corresponding to the relevant time periods.\nHere you have some guidelines for when you prepare your files:\n\nConsistent resolution and extent: Ensure that all raster layers have the same resolution and geographic extent. GLOSSA will check for mismatches and notify you with warnings or errors:\n\nIf the geographic extents differ, GLOSSA will extend the smaller layers with missing values to match the largest raster.\nIf the resolutions differ, GLOSSA will stop and return an error, as all layers need to be at the same resolution.\n\nTemporal alignment: If your occurrence data contains multiple time periods (timestamp column in the occurrence data), the rasters must align with those timestamps. GLOSSA expects the raster files to be ordered alphabetically by time. For example, if your occurrence data includes years 1 and 3, you should have raster files for each environmental variable for years 1, 2, and 3 (even if you don’t have occurrence data for year 2). In this case, you can provide a blank or duplicate raster for year 2, but the file must exist to ensure proper indexing for year 3.\n\n\nExample ZIP structure:\nenvironmental_data.zip\n    ├── temperature\n    │     ├── temp_1.tif\n    │     ├── temp_2.tif\n    │     └── temp_3.tif\n    ├── salinity\n    │     ├── sal_1.tif\n    │     ├── sal_2.tif\n    │     └── sal_3.tif\n\n\n\n\n\n\nTip\n\n\n\n\nLarge raster files may slow down the processing. For testing purposes, consider using lower-resolution rasters or aggregating the cells using the terra::aggregate() function in R\nUse clear and consistent file names, especially when handling multiple time periods or variables."
  },
  {
    "objectID": "pages/documentation/working_with_input_data.html#projection-layers-optional-but-likely-your-primary-interest",
    "href": "pages/documentation/working_with_input_data.html#projection-layers-optional-but-likely-your-primary-interest",
    "title": "Preparing your data",
    "section": "Projection layers (optional, but likely your primary interest)",
    "text": "Projection layers (optional, but likely your primary interest)\nProjection layers allow you to forecast species distributions under different environmental conditions, such as future climate scenarios. These layers should follow the exact format as the environmental data, with identical subdirectory names and matching variable names.\nSome guidelines for preparing your files:\n\nFile order: Ensure that the files within each subdirectory are ordered consistently across variables. The first file of each variable (e.g., temperature, salinity) will be treated as corresponding to the same time period, and GLOSSA will stack and project them together as part of the same time series. That is, in the following example, GLOSSA will make one projection for the temp_projection_1.tif and sal_projection_1.tif scenario, and a different projection for the conditions of temp_projection_2.tif and sal_projection_2.tif.\nMultiple scenarios: If you’re working with multiple independent scenarios (e.g., two different climate models), upload each scenario in a separate ZIP file. This way, they won’t be included in the same time series, allowing you to compare scenarios separately during plotting and exporting.\n\n\nExample projection ZIP structure:\nprojection_layers.zip\n    ├── temperature\n    │     ├── temp_projection_1.tif\n    │     └── temp_projection_2.tif\n    ├── salinity\n    │     ├── sal_projection_1.tif\n    │     └── sal_projection_2.tif\n\n\n\n\n\n\nTip\n\n\n\n\nEnsure that projection layers have the same resolution, geographic extent, coordinate reference systems (CRS - WGS84), and variable names as the environmental layers used during model fitting. This consistency is crucial for accurate forecasting and projections.\nIf your projections involve different time periods, ensure the raster files are clearly organized and ordered to reflect these periods accurately."
  },
  {
    "objectID": "pages/documentation/working_with_input_data.html#study-area-polygon-optional",
    "href": "pages/documentation/working_with_input_data.html#study-area-polygon-optional",
    "title": "Preparing your data",
    "section": "Study area polygon (optional)",
    "text": "Study area polygon (optional)\nYou can define a study area polygon to limit the geographic scope of your analysis. This will crop the environmental layers and filter out occurrence points that fall outside the study area. By default, GLOSSA uses the extent of your environmental rasters to define the study area, fitting the model only with occurrences that have valid values for all predictor variables. It also projects only onto cells covered by all environmental variables.\nHowever, if your rasters cover a larger region than your area of interest or if you have occurrence points outside the region you’d like to filter, you can upload a custom polygon. This allows you to specify the geographic region of interest, and GLOSSA will automatically crop the environmental layers and restrict the analysis to within the polygon boundaries.\nThe supported formats of this file are:\n\nGPKG (GeoPackage)\nKML\nGeoJSON\n\n\nExample use case:\nYou might have environmental data for an entire ocean but only want to model species distributions within the Mediterranean Sea. Uploading a Mediterranean Sea polygon will crop the data accordingly.\n\n\n\n\n\n\nTip\n\n\n\n\nIf the resolution of your polygon is too coarse, you can apply a buffer to expand or refine it. This buffer option can be tuned in the next section of the documentation."
  },
  {
    "objectID": "pages/documentation/working_with_input_data.html#conclusion",
    "href": "pages/documentation/working_with_input_data.html#conclusion",
    "title": "Preparing your data",
    "section": "Conclusion",
    "text": "Conclusion\nBy properly formatting your data, you’ll ensure that GLOSSA runs smoothly and provides accurate results. Once your data is ready, move on to the next step: Running a new analysis."
  },
  {
    "objectID": "pages/documentation/run_new_analysis.html",
    "href": "pages/documentation/run_new_analysis.html",
    "title": "Running a new analysis",
    "section": "",
    "text": "In this section, we’ll guide you through setting up and running your first species distribution analysis in GLOSSA. Whether you’re new to modeling or experienced, this walkthrough will help you configure the analysis step-by-step."
  },
  {
    "objectID": "pages/documentation/run_new_analysis.html#step-1-launch-glossa",
    "href": "pages/documentation/run_new_analysis.html#step-1-launch-glossa",
    "title": "Running a new analysis",
    "section": "Step 1: Launch GLOSSA",
    "text": "Step 1: Launch GLOSSA\nAfter installing GLOSSA, open R or RStudio, load the package, and launch the app:\nlibrary(glossa)\nrun_glossa()\nThis will open the GLOSSA interface in your default web browser, where you’ll start your analysis by clicking the “New analysis” button.\n\n\n\nHow to move to the New analysis tab from the Home tab of GLOSSA"
  },
  {
    "objectID": "pages/documentation/run_new_analysis.html#step-2-upload-your-data",
    "href": "pages/documentation/run_new_analysis.html#step-2-upload-your-data",
    "title": "Running a new analysis",
    "section": "Step 2: Upload your data",
    "text": "Step 2: Upload your data\nIn the New analysis tab, you’ll upload your input data. You’ll need the following:\n\nOccurrences:\n\nUpload your tab-separated file containing species occurrence data with decimalLongitude, decimalLatitude, pa (optional), and timestamp (optional). You can upload multiple files, and each will be analyzed in independent models within the same run. For example, in the figure below, two files (sp1.txt and sp2.csv) for different species were uploaded. The plot on the right shows a preview of the occurrences.\n\nEnvironmental data:\n\nUpload your environmental raster layers as a ZIP file, following the structure described in the Preparing Your Data section. Make sure all rasters have the same resolution and extent. In the figure, we uploaded fit_layers.zip, which contains the environmental data.\n\nProjection layers (optional):\n\nIf you have future scenarios (e.g., climate change projections), upload your projection layers. Using the fitted model, GLOSSA wil predict using this rasters. You can upload multiple ZIP files with different projection layers, as shown in the figure where proj_layers_1.zip and proj_layers_2.zip were uploaded.\n\nStudy area (optional):\n\nTo limit your analysis to a specific region, upload a study area polygon in GPKG, KML, or GeoJSON format. The previsualization plot will help you check the spatial coverage of the polygon.\n\n\n\n\n\nNew analysis tab"
  },
  {
    "objectID": "pages/documentation/run_new_analysis.html#step-3-configure-your-analysis",
    "href": "pages/documentation/run_new_analysis.html#step-3-configure-your-analysis",
    "title": "Running a new analysis",
    "section": "Step 3: Configure your analysis",
    "text": "Step 3: Configure your analysis\nOnce the data is uploaded, you’ll configure your analysis by adjusting the following settings:\n\nChoose a model type:\n\n\nNative range: Includes both environmental variables and spatial coordinates to predict the species’ native distribution.\nSuitable habitat: Uses only environmental variables to predict suitable habitats, ignoring spatial coordinates.\n\n\n\n\n\nModel fitting: This option fits the selected model, enabling you to compute functional responses and variable importance. It also makes predictions based on an averaged environmental scenario, which is created by calculating the mean values of each environmental variable across all provided timestamps.\nModel projection: If enabled, the fitted model will be used to make projections on the uploaded projection layers.\n\n\nOther analysis options:\n\n\nFunctional responses: This option generates response curves, which show the relationship between species occurrence and each environmental variable using partial dependence plots. These plots show the marginal effect of each variable on the predicted outcome of the BART model. Response curves are calculated only for the suitable habitat model without variable standardization, so they can be interpreted on the original scale.\nVariable importance: When selected, variable importance is computed using a permutation-based approach. This measures the change in prediction error after shuffling the values of each variable, using the F-score as a metric.\nCross-validation: Enables k-fold cross-validation (k = 5) to evaluate your model’s predictive performance. The cross-validation is conducted randomly, without accounting for spatial or temporal blocks or autocorrelation\n\n\nAdvanced options:\n\n\nOccurrence thinning - Rounding precision: Apply spatial thinning to your occurrence data using a precision-based approach. You can specify the decimal precision for latitude and longitude, and GLOSSA will remove duplicate records based on the rounded coordinates.\nLayers processing - Standardized covariates: This option z-score standardizes all environmental variables (subtract the mean and divide by the standard deviation). Mean and sd are computed based on the fitting layers and is also applied to projection layers.\nPolygon processing - Enlarge polygon: If your study area polygon has a low resolution or you want to extend its coverage, you can apply a buffer to enlarge it. The buffer value is specified in degrees, and you can preview the result by clicking the “play” button in the previsualization plot.\nModel: Currently, only the BART model is available for fitting\nSet a seed: Set a seed for reproducibility."
  },
  {
    "objectID": "pages/documentation/run_new_analysis.html#step-4-run-the-analysis",
    "href": "pages/documentation/run_new_analysis.html#step-4-run-the-analysis",
    "title": "Running a new analysis",
    "section": "Step 4: Run the analysis",
    "text": "Step 4: Run the analysis\nAfter configuring your settings, click Run Job to start the analysis. A confirmation dialog will appear, prompting you to double-check your input and settings. It’s important to ensure everything is configured correctly since some analyses may take a long time to complete, and you wouldn’t want to realize a mistake afterward.\n\n\n\n\n\nOnce confirmed, GLOSSA will begin the analysis by following these steps:\n\nData processing: GLOSSA processes your input data, removing duplicate coordinates, excluding records outside the study area, and filtering points with missing covariate values. If spatial thinning or standardization is enabled, these steps will also be applied. If a study area polygon is provided, the rasters will be cropped accordingly. For presence-only data, pseudo-absences will be generated.\nModel fitting: GLOSSA fits the BART model using presence/(pseudo-)absence data as the response variable, with environmental variables (for the suitable habitat model) and also geographic coordinates (for the native range model) as predictors. After fitting the model, GLOSSA calculates an optimal cutoff for predicting presence/absence using Youden’s index, also known as the True Skill Statistic (TSS).\nModel validation and additional analyses: GLOSSA computes validation metrics such as AUC, a confusion matrix, and the distribution of fitted values. If requested, GLOSSA also calculates functional responses, variable importance, and performs cross-validation to evaluate model performance.\nProjections: The fitted models are used to make projections based on the uploaded projection layers. Since GLOSSA operates within a Bayesian framework, it computes a predictive posterior distribution for each grid cell.\nResults: Once the analysis is complete, you’ll be automatically redirected to the Reports tab, where you can explore and interpret your results.\n\n\n\n\n\n\n\nNote\n\n\n\nThis process may take time depending on the size and resolution of your data, the number of projections, and additional options you have enabled (e.g., functional responses, cross-validation).\n\n\n\n\n\nSummary of the GLOSSA analysis workflow."
  },
  {
    "objectID": "pages/documentation/run_new_analysis.html#conclusion",
    "href": "pages/documentation/run_new_analysis.html#conclusion",
    "title": "Running a new analysis",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve successfully run your first analysis in GLOSSA! Now it’s time to explore the outputs and dive into the results. Head over to the next section to learn how to interpret your findings: Explore the results."
  },
  {
    "objectID": "pages/documentation/index.html",
    "href": "pages/documentation/index.html",
    "title": "GLOSSA documentation guide",
    "section": "",
    "text": "Welcome to the GLOSSA documentation! Here, you’ll find everything you need to understand, use, and get the most out of GLOSSA for species distribution modeling. Whether you’re new to modeling or already experienced, we’ve got guides, examples, and detailed explanations to help you.\nIf you’d like to contribute, check out our contribution guidelines. For questions, feel free to reach out via the contact form on the iMARES website."
  },
  {
    "objectID": "pages/documentation/index.html#new-to-glossa",
    "href": "pages/documentation/index.html#new-to-glossa",
    "title": "GLOSSA documentation guide",
    "section": "New to GLOSSA?",
    "text": "New to GLOSSA?\nStart with our how-to guides, which provide simple, step-by-step instructions for common tasks:\n\nWhat is GLOSSA?\nInstalling and setting up GLOSSA\nPreparing your data\nRunning a new analysis\nExplore the results\nExporting results and maps"
  },
  {
    "objectID": "pages/documentation/index.html#tutorials-and-examples",
    "href": "pages/documentation/index.html#tutorials-and-examples",
    "title": "GLOSSA documentation guide",
    "section": "Tutorials and examples",
    "text": "Tutorials and examples\nExplore real-world examples that guide you through complete analyses, from start to finish. You’ll also find specific tutorials on how to source and prepare your data for modeling:\n\nDownload and prepare occurrence data\nDownload and prepare environmental data\nGlobal species distribution model\nUsing a polygon to define the study area\nRunning GLOSSA on a regional scale"
  },
  {
    "objectID": "pages/documentation/index.html#explanations-and-reference-docs",
    "href": "pages/documentation/index.html#explanations-and-reference-docs",
    "title": "GLOSSA documentation guide",
    "section": "Explanations and reference docs",
    "text": "Explanations and reference docs\nWant to know what’s happening behind the scenes? Here’s where you can dive deeper into the concepts, like how GLOSSA’s BART model works, how we generate pseudo-absences or detailed technical information about data formats and specific settings:\n\nHow does GLOSSA work?\nWhat is Bayesian Additive Regression Trees (BART)?\nHandling presence-only data with pseudo-absences"
  },
  {
    "objectID": "pages/documentation/index.html#common-issues-and-troubleshooting",
    "href": "pages/documentation/index.html#common-issues-and-troubleshooting",
    "title": "GLOSSA documentation guide",
    "section": "Common issues and troubleshooting",
    "text": "Common issues and troubleshooting\nFind solutions to common issues, such as installation problems, data formatting errors, and model performance concerns:\n\nTroubleshooting\nFAQs"
  },
  {
    "objectID": "pages/documentation/index.html#contributing-to-glossa",
    "href": "pages/documentation/index.html#contributing-to-glossa",
    "title": "GLOSSA documentation guide",
    "section": "Contributing to GLOSSA",
    "text": "Contributing to GLOSSA\nInterested in contributing to GLOSSA? Learn how to get involved and what kinds of contributions we’re looking for:\n\nContribution guidelines"
  },
  {
    "objectID": "pages/documentation/index.html#glossa-resources",
    "href": "pages/documentation/index.html#glossa-resources",
    "title": "GLOSSA documentation guide",
    "section": "GLOSSA resources",
    "text": "GLOSSA resources\nHere are some useful links to access GLOSSA resources:\n\nCRAN repository\nGitHub repository\niMARES website\nIssues and feature requests"
  },
  {
    "objectID": "pages/documentation/explore_results.html",
    "href": "pages/documentation/explore_results.html",
    "title": "Explore the results",
    "section": "",
    "text": "After running your analysis in GLOSSA, the Reports tab presents several outputs and visualizations to help you interpret your species distribution model. This section walks you through each component of the results panel and explains how to explore and understand your model’s predictions, performance, and other key metrics."
  },
  {
    "objectID": "pages/documentation/explore_results.html#select-species-or-occurrence-file",
    "href": "pages/documentation/explore_results.html#select-species-or-occurrence-file",
    "title": "Explore the results",
    "section": "Select species or occurrence file",
    "text": "Select species or occurrence file\nIf you’ve uploaded multiple species occurrence files, you can select which species’ results to view using the dropdown menu in the top-left corner. This will update all the plots, so they represent the predictions, validation metrics, and other outputs for the selected species.\nFor most of the visualizations, you can customize the view using the three dots icon . This allows you to change how the data is displayed, select between the native range and the suitable habitat model, select different projection layers, or explore various aspects of the model results in more detail."
  },
  {
    "objectID": "pages/documentation/explore_results.html#key-metrics-in-the-first-row",
    "href": "pages/documentation/explore_results.html#key-metrics-in-the-first-row",
    "title": "Explore the results",
    "section": "Key metrics in the first row",
    "text": "Key metrics in the first row\nAt the top of the results panel, you’ll find key metrics summarizing the model’s predictions. First, there’s the species selector, which we discussed above. Next, you’ll see sparklines representing potential suitability for the selected projection scenario in the GLOSSA Predictions box.\n\nPotential suitable area (km2): This metric represents the total area (in square kilometers) predicted to be suitable for the species, calculated based on predicted presence/absence using the computed optimal cutoff.\nMean suitable probability: This value represents the average of all grid cells in the study area. For each grid cell, as we have a posterior predictive distribution, we used the mean of each grid cell to calculate the mean of the whole raster.\n\nThese two metrics help you inspect trends in suitable habitat over time. The big value represent the value of the last year, the sparkline shows year-by-year changes, and the displayed percentage indicates the change between the first 5% and the last 5% of the projection period. For instance, if your projection covers 100 years, the percentage shows the difference between the first five years and the last five years.\n\nPresences/Absences: This box tells you how many presence and absence (or pseudo-absence) records were used to fit the model, providing valuable information about the sample size.\n\nIn the example below, the habitat suitability has a decreasing trend, with around a 10% reduction from the beginning of the projection period. The model was fitted using a total of 800 points.\n\n\n\n\n\nThese metrics give you a quick overview of your model’s predictions and the data used for fitting."
  },
  {
    "objectID": "pages/documentation/explore_results.html#explore-input-data",
    "href": "pages/documentation/explore_results.html#explore-input-data",
    "title": "Explore the results",
    "section": "Explore input data",
    "text": "Explore input data\nOn the two panels on the right we can explore the environmental variables used to fit the model and the occurrences that were kept and discarded (duplicates, thinning, missing values, etc.). These two plots are very useful because you can explore patterns in the environmental variables that could be related to patterns in species occurrences and predictions comparing with the plot on the left that we will discuss next. The environmental variables are represented in the scale they were used to fit the model, so if you choose to standardize you’ll see the standardized covariates. And the plot of the occurrences allows you to know which records has been filtered by GLOSSA."
  },
  {
    "objectID": "pages/documentation/explore_results.html#glossa-projections",
    "href": "pages/documentation/explore_results.html#glossa-projections",
    "title": "Explore the results",
    "section": "GLOSSA projections",
    "text": "GLOSSA projections\nThis section provides a map showing the predicted species distribution across the study area. The heat map uses color gradients to represent the probability of species presence, where warmer colors indicate higher probabilities, and cooler colors indicate lower probabilities or unsuitable habitats. You can adjust the view to show different projection layers, timestamps, and metrics from the model’s posterior distribution (e.g., mean probability, median, or quantiles).\nUsing the three-dot icon , you can open the sidebar to customize the display:\n\nChoose between predictions based on fitting layers or projection layers.\nToggle between viewing the native range or the suitable habitat predictions.\nSelect which value from the posterior distribution to display (e.g., mean probability, median, etc.).\nOverlay points used to fit the model to visualize how occurrence data aligns with the predictions.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChanging the projection scenario in this panel will also update the sparklines at the top of the results panel, reflecting the trend in habitat suitability for that scenario."
  },
  {
    "objectID": "pages/documentation/explore_results.html#additional-results",
    "href": "pages/documentation/explore_results.html#additional-results",
    "title": "Explore the results",
    "section": "Additional results",
    "text": "Additional results\nIn the next row of the results panel, you can view the additional analyses that you requested in GLOSSA:\n\nFunctional responses: Shows the functional responses for each environmental variable, showing how changes in each variable affect the probability of species presence. These curves are partial dependence plots that provide insights into the environmental factors driving species distributions.\nVariable importance: The variable importance plot shows how much each environmental variable influences the model’s predictions. GLOSSA uses a permutation-based approach to measure the effect of shuffling each variable’s values on the model’s prediction accuracy. Variables with high importance scores play a larger role in predicting species presence, helping you identify the key drivers of habitat suitability.\nCross-Validation: If you enabled cross-validation, this panel shows the performance metrics from k-fold cross-validation. The radar plot displays metrics such as precision, sensitivity, specificity, false discovery rate, F-score, accuracy, and TSS (True Skill Statistic)."
  },
  {
    "objectID": "pages/documentation/explore_results.html#model-summary",
    "href": "pages/documentation/explore_results.html#model-summary",
    "title": "Explore the results",
    "section": "Model Summary",
    "text": "Model Summary\nThe model summary panel provides key validation metrics for the model fitting:\n\nROC Curve: This plot evaluates the model’s ability to distinguish between species presence and absence across different probability thresholds, showing the AUC (Area Under the Curve) as an overall measure of accuracy.\nConfusion Matrix Plot: This plot breaks down true positives, false positives, true negatives, and false negatives, and shows their predicted probability with the computed optimal cutoff. classification performance.\nDistribution of Fitted Values: This histogram shows the distribution of predicted probabilities across the study area."
  },
  {
    "objectID": "pages/documentation/explore_results.html#exporting-results",
    "href": "pages/documentation/explore_results.html#exporting-results",
    "title": "Explore the results",
    "section": "Exporting Results",
    "text": "Exporting Results\nOnce you’re satisfied with your results, you can use the download icon  to export any plot or visualization. Export formats include PNG, JPG, SVG, and PGF. You can also adjust the height and width of the exported plot."
  },
  {
    "objectID": "pages/documentation/explore_results.html#conclusion",
    "href": "pages/documentation/explore_results.html#conclusion",
    "title": "Explore the results",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve now learned how to navigate the GLOSSA report panel to explore key metrics, validate model performance, and gain insights into the factors driving species distributions. The next step is to export your results, save the projection rasters, the data used for model fitting, and other analysis outputs: Exporting results and maps."
  },
  {
    "objectID": "how_to_cite.html",
    "href": "how_to_cite.html",
    "title": "How to cite GLOSSA",
    "section": "",
    "text": "How to cite GLOSSA\nThank you for using GLOSSA. Proper citation helps to further develop the tool and highlight its impact on the research community.\n\nMain citation for GLOSSA\nTo cite GLOSSA in publications, please use the following reference:\n\nMestre-Tomás J., Fuster-Alonso, A., Bellido, J. M., & Coll, M. (2024). glossa: User-Friendly ‘shiny’ App for Bayesian Species Distribution Models. R package version 1.0.0, https://cran.r-project.org/package=glossa\n\nCorresponding BibTeX entry:\n@Manual{,\n  title = {glossa: User-Friendly 'shiny' App for Bayesian Species Distribution Models},\n  author = {Jorge Mestre-Tomás and Alba Fuster-Alonso and José M. Bellido and Marta Coll},\n  year = {2024},\n  note = {R package version 1.0.0},\n  url = {https://cran.r-project.org/package=glossa},\n}\n\n\nAdditional citations\n\nBayesian Additive Regression Trees (BART)\nIf you use the BART model in your analyses, please include this reference:\n\nChipman, H. A., George, E. I., & McCulloch, R. E. (2010). BART: Bayesian Additive Regression Trees. The Annals of Applied Statistics, 4(1), 266–298. doi: https://doi.org/10.1214/09-AOAS285\n\n\n\nGlobal-scale BART modeling\nFor global-scale analyses using BART, please include the following citation:\n\nFuster-Alonso, A., Mestre-Tomás, J., Baez, J. C., Pennino, M. G., Barber, X., Bellido, J. M., … & Coll, M. (2024). Machine learning applied to global scale species distribution models (SDMs). PREPRINT (Version 1) available at Research Square. doi: https://doi.org/10.21203/rs.3.rs-4411399/v1."
  },
  {
    "objectID": "contact_us.html",
    "href": "contact_us.html",
    "title": "Contact us",
    "section": "",
    "text": "We value your feedback! Whether you have questions about GLOSSA, need technical support, or are interested in collaboration, please feel free to reach out through any of the options below.\n\n\nFor issues, feature requests, questions, or general feedback, please use the Issues tab on our GitHub repository:\n\nGLOSSA GitHub issues\n\n\n\n\nStay connected and follow our latest news and updates on social media:\n\n@iMARES_group\n\n\n\n\nFor special support requests, or collaborations, please contact us via the contact form at the end of the iMARES website:\n\niMARES\n\n\n\nThank you for your support! If you want to know more about the group go and visit our website: iMARES"
  },
  {
    "objectID": "contact_us.html#github",
    "href": "contact_us.html#github",
    "title": "Contact us",
    "section": "",
    "text": "For issues, feature requests, questions, or general feedback, please use the Issues tab on our GitHub repository:\n\nGLOSSA GitHub issues"
  },
  {
    "objectID": "contact_us.html#x-twitter",
    "href": "contact_us.html#x-twitter",
    "title": "Contact us",
    "section": "",
    "text": "Stay connected and follow our latest news and updates on social media:\n\n@iMARES_group"
  },
  {
    "objectID": "contact_us.html#imares-website",
    "href": "contact_us.html#imares-website",
    "title": "Contact us",
    "section": "",
    "text": "For special support requests, or collaborations, please contact us via the contact form at the end of the iMARES website:\n\niMARES\n\n\n\nThank you for your support! If you want to know more about the group go and visit our website: iMARES"
  },
  {
    "objectID": "get_started.html",
    "href": "get_started.html",
    "title": "Get started",
    "section": "",
    "text": "GLOSSA is a user-friendly R shiny app designed to help you model marine species distributions using species occurrences and environmental data. Below is a quick guide to help you get started.\n\n\nBefore you begin, ensure you have R version 4.0.0 or higher installed on your system:\n\n\n\n\n\n\nDownload R\n\n\n\n\n\n\nTo install GLOSSA, simply run the following command in your R console:\ninstall.packages(\"glossa\")\nAlternatively, you can install the latest development version from GitHub to get the latest updates and fixes:\nif (!require(\"devtools\")) \n  install.packages(\"devtools\")\n\ndevtools::install_github(\"iMARES-group/glossa\")\nAfter installation, you can launch the GLOSSA app by running this command:\nlibrary(glossa)\nrun_glossa()"
  },
  {
    "objectID": "get_started.html#welcome-to-glossa",
    "href": "get_started.html#welcome-to-glossa",
    "title": "Get started",
    "section": "",
    "text": "GLOSSA is a user-friendly R shiny app designed to help you model marine species distributions using species occurrences and environmental data. Below is a quick guide to help you get started.\n\n\nBefore you begin, ensure you have R version 4.0.0 or higher installed on your system:\n\n\n\n\n\n\nDownload R\n\n\n\n\n\n\nTo install GLOSSA, simply run the following command in your R console:\ninstall.packages(\"glossa\")\nAlternatively, you can install the latest development version from GitHub to get the latest updates and fixes:\nif (!require(\"devtools\")) \n  install.packages(\"devtools\")\n\ndevtools::install_github(\"iMARES-group/glossa\")\nAfter installation, you can launch the GLOSSA app by running this command:\nlibrary(glossa)\nrun_glossa()"
  },
  {
    "objectID": "get_started.html#glossa-workflow-overview",
    "href": "get_started.html#glossa-workflow-overview",
    "title": "Get started",
    "section": "GLOSSA workflow overview",
    "text": "GLOSSA workflow overview\nGLOSSA is designed to be intuitive, with a simple step-by-step workflow:\n\nData input:\n\n\nUpload species occurrence data, either presence-absence or presence-only data (in which case, pseudo-absences will be generated).\nUpload environmental data, for example, climate variables in raster format.\nOptionally, provide projection layers, for example, to project future scenarios.\nOptionnally, provide a study area polygon to define the study area.\n\n\nData processing:\n\n\nClean coordinates: GLOSSA automatically removes duplicates and points outside the study area.\nProcess layers: Environmental layers are cropped and Z-score standardized if needed.\nGenerate pseudo-absences: For presence-only data, pseudo-absences are randomly generated.\n\n\nModel fitting and prediction:\n\n\nGLOSSA fits the BART (Bayesian Additive Regression Trees) model to predict species distributions. Two models can be fitted—one based on environmental data and another including spatial smoothing.\nYou can explore model outputs, including probability maps, performance metrics, and variable importance.\n\n\nResults and visualization:\n\n\nExplore interactive maps and charts showing predictions and analysis results.\nExport results for further use or reporting.\n\n\n\n\nGLOSSA workflow overview."
  },
  {
    "objectID": "get_started.html#running-your-first-analysis",
    "href": "get_started.html#running-your-first-analysis",
    "title": "Get started",
    "section": "Running your first analysis",
    "text": "Running your first analysis\nHere’s a quick guide on how to use the app. First, launch the app by running the run_glossa() function:\nlibrary(glossa)\nrun_glossa()\nThis will open the app in your web browser directly in the Home tab. Here, you can start a new analysis, watch a demo, or read tutorials.\n\n\n\nScreenshot of the “Home” tab, the landing page.\n\n\nClicking on the question mark icon  on the top right corner will bring up a brief explanation of what each tab and button does.\n\n\n\n\n\n\nSidebar overview\nOn the sidebar, you’ll find two main sections: Modelling and Resources.\n\nResources: This section provides essential information to help you run the app. It’s a great place to refresh your knowledge, especially if it’s been a while since you last used GLOSSA. Note that the full documentation is hosted outside the app in this website.\nModelling: This section includes the New Analysis, Reports, and Exports tabs. Typically, you’ll go through these tabs in sequence:\n\nNew Analysis: Set up your analysis, upload your data, and select analysis options.\nReports: Explore the results and visualizations generated by the analysis.\nExports: Download the results for further use."
  },
  {
    "objectID": "get_started.html#running-your-first-analysis-1",
    "href": "get_started.html#running-your-first-analysis-1",
    "title": "Get started",
    "section": "Running your first analysis",
    "text": "Running your first analysis\nTo run your first analysis, go to the New Analysis tab. This tab looks like this:\n\n\n\n\n\nHere, you need to upload the required input files and select your analysis options:\n\nData upload: The first panel is where you upload your data and configure the analysis settings.\nPrevisualization: The second panel provides an interactive map to preview your input data.\nPredictor variables: Choose predictor variables for each uploaded species.\nUploaded files: A table indicates if your input files are formatted correctly.\n\n\nRequired files for analysis\nGLOSSA can function with just occurrence data and environmental variables, but additional options are available. Let’s briefly go through the necessary files:\n\nOccurrences: Upload a tab-separated CSV file with four columns indicating the occurrence location, whether it is a presence or absence, and the time it was recorded. The columns must be named exactly as follows: decimalLongitude, decimalLatitude, pa, and timestamp. If the pa column is missing, GLOSSA will assume all rows are presences. If timestamp is missing, GLOSSA will assume all observations occurred at the same time. GLOSSA also supports presence-only data but will generate randomly distributed balanced pseudo-absences to fit the model.\n&gt; head(sp1)\n  decimalLongitude decimalLatitude timestamp pa\n1          5.42909        43.20937         1  1\n2           -43.05           49.03         1  0\n3         -2.52369        47.29234         2  1\n4           34.054         -26.913         2  1\n5           -41.63            46.3         2  0\n6           -174.5            27.5         3  1\nEnvironmental data: Upload environmental data as raster files (e.g., .tif or .nc format) in a ZIP file with a specific structure. The ZIP file should contain a subdirectory for each environmental variable, with files sorted by time period. For example, if you have two variables (x1 and x2) and your observations are from two different years, your ZIP file should look like this:\nfit_layers.zip\n    ├───x1\n    │       x1_1.tif\n    │       x1_2.tif\n    └───x2\n            x2_1.tif\n            x2_2.tif\nEnsure that all layers have the same resolution, the same number of layers, and that they match the number of years in your occurrence files. If you want to use the same layer for all observations, include just one file in the subdirectory and set all timestamp values to 1 or remove the timestamp column.\nProjection layers (Optional): If you want to make predictions, upload your projection data here. This file has the same format as the environmental data file, and the subdirectories must match those used for fitting the model. You can upload multiple ZIP files if you want to predict multiple scenarios (e.g., different temperature increase scenarios).\nStudy area (Optional): If your rasters cover a larger area than your study region, you can provide a polygon to delimit your study area (formats: GPKG, KML or GeoJSON). This will remove points outside the polygon and mask the environmental variables accordingly.\n\nOnce all files are uploaded, if they pass the checks and are properly formatted, the table in the bottom right panel will show a checkmark for each file. If something is incorrect, refer to the documentation for a quick solution.\n\n\nAnalysis options\nIn this section, you need to select the desired output. GLOSSA fits two kinds of models:\n\nNative range: The model includes environmental variables and uses longitude and latitude coordinates as a spatial smoother.\nSuitable habitat: The model only includes the environmental variables.\n\nYou can choose to fit the model under the Model fitting option. This option will fit the model, compute variable importance, and generate a prediction map representing the mean environmental conditions across all provided layers. If you’ve uploaded projection layers and want to make predictions using the fitted model, select the Model projection option.\nWhen fitting the model, if multiple years or time periods are uploaded, GLOSSA will extract the value of the corresponding environmental layer for each occurrence based on the specific time stamp.\nAdditionally, you can check the Functional responses checkbox if you want to compute the response curves (i.e., the relationship between the occurrence of a species and each environmental variable). You can also enable Cross-validation, which will perform a K-fold cross-validation with (k = 5).\n\n\nAdvanced options\nBy selecting the Advanced options button, a sidebar will appear with extra options for refining your analysis:\n\nOccurrences thinning: Specify the number of decimal places to round coordinates, allowing you to apply spatial thinning to your occurrence data using a precision-based method. GLOSSA currently implements this method via the GeoThinneR R package, which is the most time- and memory-efficient option for large datasets. If you need to perform spatial thinning based on distance or a grid, you can do so before uploading your data to GLOSSA using GeoThinneR or other methods.\nStandardize covariates: GLOSSA uses a scaling method that subtracts the mean and divides by the standard deviation for standardization. The mean and standard deviation are calculated from the fitting layers, and the same values are used to standardize the projection layers, ensuring consistency across variables.\nEnlarge polygon: If your polygon has low resolution, you can apply a buffer in degrees to expand it. This is useful if you have points near the coast that fall outside the polygon due to poor resolution. You can preview the buffer using the “play” icon before running the analysis to find the optimal value.\nModel: Choose the model to apply. Currently, GLOSSA only supports the BART model, so no additional selection is needed here.\nSet a seed: Specify a seed for reproducibility of your results.\n\n\n\n\n\n\n\n\nPredictor variables\nIf you uploaded multiple species, you can select different predictor variables for each species in the bottom left panel.\n\n\n\n\n\n\n\nUploaded files\nIn the table in the bottom right corner, ensure that all files are checked and that you’ve selected your analysis options.\n\n\n\n\n\nOnce everything is set, you’re ready to run the analysis. Click the Run Job button, confirm in the dialog, and wait for the analysis to complete."
  },
  {
    "objectID": "get_started.html#analysis-results",
    "href": "get_started.html#analysis-results",
    "title": "Get started",
    "section": "Analysis results",
    "text": "Analysis results\nOnce the analysis is complete, you will be redirected to the Reports tab, where you can explore all the results and export visualizations .\n\n\n\n\n\nIn the top left corner of the tab, you can select the species for which you want to view the results. The first row displays key metrics:\n\nPotential suitable area: Calculated in square kilometers, based on the predicted presence-absence grid cells.\nMean suitable probability: The average probability of suitable habitat across the entire prediction area.\nPresences/Absences: The number of presence and absence points used to fit the model after all processing and cleaning.\n\nIf multiple projection layers are provided (for example, a time series), a sparkline plot will display the values for each time period, and the text value shown will represent the last one in the time series.\n\nGLOSSA predictions\nThe first plot, titled GLOSSA predictions, shows the presence probability predictions within the study area. As we are working in the Bayesian framework, each grid cell has associated one predictive posterior distribution, therefore we can obtain a more comprenhensive undertanding of the predictions by exploring metrics like the mean, median or quantiles. Using the three-dot icon , you can open the sidebar to customize the display:\n\nChoose between predictions on the fitting layers or the projection layers.\nToggle between viewing the native range or the suitable habitat.\nSelect which value from the posterior distribution to display (e.g., mean probability, median, etc.).\n\n\n\n\n\n\n\n\nEnvironmental variables and Presence validation\nThe plot on the right shows the environmental variables used to fit the model, allowing you to quickly compare them with the probability projections. Below this, another plot displays the occurrence points that were retained or filtered out during the analysis.\n\n\n\n\n\n\n\nFunctional responses and Variable importance\nIn the last row, you can view:\n\nFunctional response: These illustrate the relationship between each environmental variable and the predicted suitable habitat (response curves).\nVariable importance: Displays the importance of each variable for both the native range and the suitable habitat.\n\n\n\n\n\n\n\n\nCross-validation\nIn the cross-validation panel, you’ll find performance metrics such as:\n\nAIC (Akaike Information Criterion)\nRMSE (Root Mean Square Error)\n5-Fold Cross-Validation Results"
  },
  {
    "objectID": "get_started.html#exports",
    "href": "get_started.html#exports",
    "title": "Get started",
    "section": "Exports",
    "text": "Exports\nIn the Exports tab, you can export the results of your analysis. This includes all projection maps, the data used to fit the model, variable importance metrics, cross-validation results, etc. You can export almost everything, enabling you to explore the results further or create your own visualizations for your work!"
  },
  {
    "objectID": "get_started.html#next-steps",
    "href": "get_started.html#next-steps",
    "title": "Get started",
    "section": "Next Steps",
    "text": "Next Steps\nThis Quick start guide for GLOSSA should help you get up and running. For more detailed instructions, check out the Tutorial tab, where you’ll find tutorials on how to prepare your data for GLOSSA or even worked examples that guide you through using GLOSSA from end-to-end.\nAdditionally, the Documentation tab offers a comprehensive guide to every aspect of the GLOSSA app. Here, you’ll find not only in-depth information on how GLOSSA works but also tips and tricks to help you get the most out of the app.\nIf you have further questions, visit the FAQs tab, or if you still need help, you can reach us through the Contact Us tab.\nThank you for using GLOSSA, and enjoy your species distribution modeling!"
  },
  {
    "objectID": "pages/documentation/export_outputs.html",
    "href": "pages/documentation/export_outputs.html",
    "title": "Exporting results and maps",
    "section": "",
    "text": "Our team is hard at work to provide you with comprehensive guides and resources.\nThank you for your patience. Stay tuned for updates!"
  },
  {
    "objectID": "pages/documentation/installation_setup.html",
    "href": "pages/documentation/installation_setup.html",
    "title": "Installing and setting up GLOSSA",
    "section": "",
    "text": "This guide will walk you through installing GLOSSA on your machine, setting up your environment, and preparing to run your first analysis. By the end, you’ll be ready to use GLOSSA for species distribution modeling."
  },
  {
    "objectID": "pages/documentation/installation_setup.html#system-requirements",
    "href": "pages/documentation/installation_setup.html#system-requirements",
    "title": "Installing and setting up GLOSSA",
    "section": "1. System requirements",
    "text": "1. System requirements\nBefore installing GLOSSA, ensure that your system meets the following requirements:\n\nR version: GLOSSA requires R version 4.0.0 or higher. If you don’t have R installed, you can download it from CRAN.\nOperating system: GLOSSA is compatible with Windows, macOS, and Linux.\nRStudio (optional but recommended): We recommend using RStudio as it provides useful tools for working with R. Download RStudio here."
  },
  {
    "objectID": "pages/documentation/installation_setup.html#installing-glossa",
    "href": "pages/documentation/installation_setup.html#installing-glossa",
    "title": "Installing and setting up GLOSSA",
    "section": "2. Installing GLOSSA",
    "text": "2. Installing GLOSSA\n\nInstall from CRAN (stable version)\nTo install the latest stable version of GLOSSA from CRAN, open R or RStudio and run the following command:\ninstall.packages(\"glossa\")\n\n\nInstall the development version from GitHub (latest features)\nFor the latest features or bug fixes, you can install the development version from GitHub. First, install devtools:\ninstall.packages(\"devtools\")\nThen, install GLOSSA from GitHub:\ndevtools::install_github(\"iMARES-group/glossa\")"
  },
  {
    "objectID": "pages/documentation/installation_setup.html#launching-glossa",
    "href": "pages/documentation/installation_setup.html#launching-glossa",
    "title": "Installing and setting up GLOSSA",
    "section": "3. Launching GLOSSA",
    "text": "3. Launching GLOSSA\nOnce GLOSSA is installed, you can launch it in R or RStudio by running:\nlibrary(glossa)\nrun_glossa()\nThis command starts the GLOSSA shiny web app, which will open in your default browser."
  },
  {
    "objectID": "pages/documentation/installation_setup.html#verifying-your-installation",
    "href": "pages/documentation/installation_setup.html#verifying-your-installation",
    "title": "Installing and setting up GLOSSA",
    "section": "4. Verifying Your Installation",
    "text": "4. Verifying Your Installation\nAfter installation, you can verify that GLOSSA is correctly installed by checking the package version:\npackageVersion(\"glossa\")\nIf GLOSSA has been installed successfully, this will return the installed version number. The app should open in your web browser and be ready to use."
  },
  {
    "objectID": "pages/documentation/installation_setup.html#conclusion",
    "href": "pages/documentation/installation_setup.html#conclusion",
    "title": "Installing and setting up GLOSSA",
    "section": "Conclusion",
    "text": "Conclusion\nYou’re now ready to start working with GLOSSA! After installation, you can move on to preparing your data and running your first analysis. Head over to the Preparing your data section to learn how to format your species occurrence data and environmental layers for GLOSSA.\nIf you encounter any issues during installation, visit our Common issues and troubleshooting page for solutions."
  },
  {
    "objectID": "pages/documentation/what_is_glossa.html",
    "href": "pages/documentation/what_is_glossa.html",
    "title": "What is GLOSSA?",
    "section": "",
    "text": "GLOSSA is a simple, interactive R Shiny app designed to help you model species distributions without needing to code. Using Bayesian Additive Regression Trees (BART), GLOSSA predicts species habitats based on presence and environmental data, allowing you to visualize both current and future distributions."
  },
  {
    "objectID": "pages/documentation/what_is_glossa.html#how-glossa-works",
    "href": "pages/documentation/what_is_glossa.html#how-glossa-works",
    "title": "What is GLOSSA?",
    "section": "How GLOSSA works?",
    "text": "How GLOSSA works?\nGLOSSA’s workflow is designed to be intuitive, with four main steps:\n\n1. Data upload\n\nSpecies occurrence: Upload your species data with either presence/absence or presence-only records. GLOSSA can work with multiple species in one session, generating pseudo-absences when needed.\nEnvironmental data: Upload environmental rasters (e.g., from sources like Bio-ORACLE) matched to the occurrence points by location and time.\nProjections: Include layers representing future time periods or climate scenarios to generate predictions for each.\nStudy area: Upload a polygon to define your study region, or let GLOSSA use the extent of your environmental data.\n\n\n\n2. Data processing\n\nCoordinate cleaning: GLOSSA automatically removes duplicate records, filters out invalid points, and applies spatial thinning if needed.\nLayer processing: Environmental rasters are cropped, masked, and can be standardized.\nGenerate pseudo-absences: For presence-only data, GLOSSA creates randomly distributed pseudo-absences across the study area.\n\n\n\n3. Model fitting and prediction\n\nFit BART model: GLOSSA fits a BART model, generating predictions for suitable habitats and native ranges, and computes an optimal threshold for presence/absence classification.\nModel output: The output includes metrics like AUC, ROC curve, confusion matrix, functional response curves, and variable importance.\nProjections: Projections can be made to different areas, time periods, and climate scenarios.\n\n\n\n4. Visualization and export\nAfter the analysis, GLOSSA provides interactive maps and charts to explore model results, allowing you to export data and visuals for further use."
  },
  {
    "objectID": "pages/documentation/what_is_glossa.html#why-use-glossa",
    "href": "pages/documentation/what_is_glossa.html#why-use-glossa",
    "title": "What is GLOSSA?",
    "section": "Why use GLOSSA?",
    "text": "Why use GLOSSA?\nGLOSSA simplifies species distribution modeling by:\n\nMaking modeling accessible with a no-code interface.\nOffering robust machine learning predictions with BART.\nSupporting independent multi-species and scenario-based analysis in a single run.\nProviding detailed visualizations and export options for further analysis or publication."
  },
  {
    "objectID": "pages/documentation/what_is_glossa.html#ready-to-get-started",
    "href": "pages/documentation/what_is_glossa.html#ready-to-get-started",
    "title": "What is GLOSSA?",
    "section": "Ready to get started?",
    "text": "Ready to get started?\nLearn how to install and set up GLOSSA: Installation guide."
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html",
    "title": "Example 2",
    "section": "",
    "text": "This vignette provides a detailed example of fitting a single-species distribution model within a user-defined region. We will walk through the steps of obtaining occurrence data, gathering environmental data, and defining the study area using a polygon. The example focuses on the distribution of the loggerhead sea turtle (Caretta caretta) in the Mediterranean Sea. Occurrence data will be retrieved from GBIF for the years 2000 to 2019, and environmental data from Bio-ORACLE v3.0 (Assis et al., 2024). We will use GLOSSA to fit a species distribution model and project it under different future climate scenarios.\nTo get started, we load the glossa package, as well as terra (Hijmans, 2024) and sf (Pebesma, 2018) to work with spatial rasters and vector data. We also load rgbif (Chamberlain, 2017) and biooracler (Fernandez Bejarano and Salazar, 2024) for downloading species occurrences and environmental data, respectively. Additionally, the dplyr (Wickham et al., 2023) package will be used for data manipulation.\n\nlibrary(glossa)\nlibrary(terra)\nlibrary(sf)\nlibrary(rgbif)\nlibrary(biooracler)\nlibrary(dplyr)"
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#introduction",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#introduction",
    "title": "Example 2",
    "section": "",
    "text": "This vignette provides a detailed example of fitting a single-species distribution model within a user-defined region. We will walk through the steps of obtaining occurrence data, gathering environmental data, and defining the study area using a polygon. The example focuses on the distribution of the loggerhead sea turtle (Caretta caretta) in the Mediterranean Sea. Occurrence data will be retrieved from GBIF for the years 2000 to 2019, and environmental data from Bio-ORACLE v3.0 (Assis et al., 2024). We will use GLOSSA to fit a species distribution model and project it under different future climate scenarios.\nTo get started, we load the glossa package, as well as terra (Hijmans, 2024) and sf (Pebesma, 2018) to work with spatial rasters and vector data. We also load rgbif (Chamberlain, 2017) and biooracler (Fernandez Bejarano and Salazar, 2024) for downloading species occurrences and environmental data, respectively. Additionally, the dplyr (Wickham et al., 2023) package will be used for data manipulation.\n\nlibrary(glossa)\nlibrary(terra)\nlibrary(sf)\nlibrary(rgbif)\nlibrary(biooracler)\nlibrary(dplyr)"
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#data-preparation",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#data-preparation",
    "title": "Example 2",
    "section": "Data preparation",
    "text": "Data preparation\n\nDownload occurrence data\nWe will download occurrence data for Caretta caretta in the Mediterranean Sea using the GBIF API. The data will be filtered by date and geographic boundaries to fit the Mediterranean region and align with the temporal scale of the environmental data from Bio-ORACLE. To use the GBIF API, you need to have a registered account (user, pwd, and email). First, we retrieve the taxon key for C. caretta, and then use occ_download() from the rgbif package to download the occurrences. To restrict the download to the Mediterranean Sea, we use the pred_within() argument and define a polygon of interest (\"POLYGON ((-10 28, 38 28, 38 51, -10 51, -10 28))\"). Since Bio-ORACLE provides environmental layers in decadal steps, we limit the occurrence data to points between 2000 and 2019.\n\n# Function to retrieve taxon key from scientific name\nget_taxon_key &lt;- function(name) {\n  result &lt;- rgbif::occ_search(scientificName = name, hasCoordinate = TRUE)\n  if (!is.null(result$data)) {\n    return(as.character(result$data$taxonKey[1]))\n  } else {\n    warning(paste(\"No taxon key found for\", name))\n    return(NA)\n  }\n}\n\n# Define GBIF credentials and species information\nuser &lt;- \"&lt;gbif username&gt;\"\npwd &lt;- \"&lt;password&gt;\"\nemail &lt;- \"&lt;gbif mail&gt;\"\ntaxon_key &lt;- get_taxon_key(\"Caretta caretta\")\n\n# Download GBIF occurrence data\nrequest_id &lt;- as.character(rgbif::occ_download(\n  rgbif::pred(\"taxonKey\", taxon_key),\n  rgbif::pred(\"hasCoordinate\", TRUE),\n  rgbif::pred_within(\"POLYGON ((-10 28, 38 28, 38 51, -10 51, -10 28))\"),\n  rgbif::pred(\"occurrenceStatus\", \"PRESENT\"),\n  rgbif::pred_gte(\"YEAR\", 2000),\n  rgbif::pred_lte(\"YEAR\", 2019),\n  format = \"SIMPLE_CSV\",\n  user = user,\n  pwd = pwd,\n  email = email\n))\n\nresponse &lt;- rgbif::occ_download_wait(request_id)\n\nif (response$status == \"SUCCEEDED\"){\n  temp &lt;- tempfile(fileext = \".csv\")\n  download.file(response$downloadLink, temp, mode = \"wb\")\n  caretta &lt;- read.csv(unz(temp, paste0(response$key, \".csv\")),\n                        header = TRUE, sep = \"\\t\", dec = \".\")\n}\n\nOnce we have the dataset, we prepare it to fit the GLOSSA format. For this, we extract the required record locations (decimalLongitude and decimalLatitude) and assign a timestamp for each record. As Bio-ORACLE provides environmental layers for two decades (2000s and 2010s), we assign points from 2000 to 2009 to the first time period and points from 2010 to 2019 to the second time period. We then create a presence/absence column (pa), setting all downloaded records as presences. Finally, we save the file as a tab-separated CSV and retrieve the DOI of the downloaded data from GBIF (GBIF.org, 2024).\n\n# Prepare the data for modeling\n# Separate timestamp in two decades to fit Bio-ORACLE format (2000s and 2010s)\ncaretta &lt;- data.frame(\n  decimalLongitude = caretta$decimalLongitude,\n  decimalLatitude = caretta$decimalLatitude,\n  timestamp = ifelse(caretta$year %in% 2000:2009, 1, 2),\n  pa = 1\n)\ncaretta &lt;- caretta[complete.cases(caretta),]\n\n# Save the data\ndir.create(\"data\")\nwrite.table(caretta, file = \"data/Caretta_caretta_occ.csv\", sep = \"\\t\",\n            dec = \".\", quote = FALSE, row.names = FALSE)\n\n# Get citation for the downloaded data\ncitation &lt;- rgbif::gbif_citation(request_id)$download\ncitation\n# GBIF Occurrence Download https://doi.org/10.15468/dl.es7562\n# Accessed from R via rgbif (https://github.com/ropensci/rgbif) on 2024-08-27\n\n\n\nDownload environmental data\nBio-ORACLE provides environmental layers at a spatial resolution of 0.05 degrees and in decadal steps. We will download environmental variables such as ocean surface temperature (thetao in \\(^{\\circ}\\)C), primary productivity (phyc in \\(\\text{mmol} \\cdot \\text{m}^{-3}\\)), and salinity (so). Bathymetry (bat) will be obtained from the ETOPO 2022 Global Relief Model by NOAA (https://www.ncei.noaa.gov/products/etopo-global-relief-model).\n\n# Define temporal and spatial constraints\ntime = c(\"2000-01-01T00:00:00Z\", \"2010-01-01T00:00:00Z\")\nlatitude = c(28, 51)\nlongitude = c(-10, 38)\nconstraints = list(time, longitude, latitude)\nnames(constraints) = c(\"time\", \"longitude\", \"latitude\")\n\n# Download environmental layers from Bio-Oracle\nthetao_hist &lt;- download_layers(\"thetao_baseline_2000_2019_depthsurf\", \n                               \"thetao_mean\", constraints)\nphyc_hist &lt;- download_layers(\"phyc_baseline_2000_2020_depthsurf\", \n                             \"phyc_mean\", constraints)\nso_hist &lt;- download_layers(\"so_baseline_2000_2019_depthsurf\", \n                           \"so_mean\", constraints)\n\n# Download bathymetry data\ndownload.file(\"https://www.ngdc.noaa.gov/thredds/fileServer/global/ETOPO2022/60s/60s_bed_elev_netcdf/ETOPO_2022_v1_60s_N90W180_bed.nc\",\n              destfile = \"data/ETOPO_2022_v1_60s_N90W180_bed.nc\")\n\nWe will prepare the downloaded environmental data and save it in the directory structure required by GLOSSA.\n\ndir.create(\"data/fit_layers\")\n\n# Prepare environmental variables for modeling\nenv_var &lt;- list(thetao_hist, so_hist, phyc_hist)\nnames(env_var) &lt;- c(\"thetao\", \"so\", \"phyc\")\n\nfor (i in seq_along(env_var)) {\n  var_dir &lt;- paste0(\"data/fit_layers/\", names(env_var)[i])\n  dir.create(var_dir)\n  \n  terra::writeRaster(\n    env_var[[i]][[1]], \n    filename = paste0(var_dir, \"/\", names(env_var)[i], \"_1.tif\")\n  )\n  terra::writeRaster(\n    env_var[[i]][[2]],\n    filename = paste0(var_dir, \"/\", names(env_var)[i], \"_2.tif\")\n  )\n  \n  # Clean up auxiliary files generated by terra - Optional\n  aux_files &lt;- list.files(var_dir, pattern = \"\\\\.aux\\\\.json$\", full.names = TRUE)\n  file.remove(aux_files)\n}\n\n# Prepare bathymetry data\nbat &lt;- terra::rast(\"data/ETOPO_2022_v1_60s_N90W180_bed.nc\")\nbat &lt;- -1*bat # Change sign from elvation to bathymetry\n\n# Aggregate to match resolution with Bio-ORACLE layers\nbat &lt;- terra::aggregate(bat, fact = 3, fun = \"mean\")\nr &lt;- terra::rast(terra::ext(env_var[[1]]), res = terra::res(env_var[[1]]))\nbat &lt;- terra::resample(bat, r)\n\ndir.create(\"data/fit_layers/bat\")\nfor (i in 1:2){\n  terra::writeRaster(bat, filename = paste0(\"data/fit_layers/bat/bat_\", i, \".tif\"))\n}\n\n# Zip all prepared layers\nzip(zipfile = \"data/fit_layers.zip\", files = \"data/fit_layers\")\n\n\n\nClimate projections: SSP1 2.6 and SSP5 8.5\nWe will now download climate projections for SSP1 2.6 (sustainable development scenario where global CO2 emissions are strongly reduced but less rapidly) and SSP5 8.5 (a high emissions scenario) scenarios, and prepare the data for model projection.\n\nSSP1 2.6\n\n# SSP1 2.6 projections constraints\ntime = c(\"2020-01-01T00:00:00Z\", \"2090-01-01T00:00:00Z\")\nlatitude = c(28, 51)\nlongitude = c(-10, 38)\nconstraints = list(time, longitude, latitude)\nnames(constraints) = c(\"time\", \"longitude\", \"latitude\")\n\nenv_var_ssp126 &lt;- list(\n  thetao_ssp126 = download_layers(\"thetao_ssp126_2020_2100_depthsurf\", \n                                  \"thetao_mean\", constraints),\n  so_ssp126 = download_layers(\"so_ssp126_2020_2100_depthsurf\", \n                              \"so_mean\", constraints),\n  phyc_ssp126 = download_layers(\"phyc_ssp126_2020_2100_depthsurf\", \n                                \"phyc_mean\", constraints)\n)\nnames(env_var_ssp126) &lt;- c(\"thetao\", \"so\", \"phyc\")\n\ndir.create(\"data/proj_ssp126\")\nfor (i in seq_along(env_var_ssp126)) {\n  var_dir &lt;- paste0(\"data/proj_ssp126/\", names(env_var_ssp126)[i])\n  dir.create(var_dir, showWarnings = FALSE)\n  \n  terra::writeRaster(\n    env_var_ssp126[[i]][[1]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp126)[i], \"_1.tif\")\n  )\n  terra::writeRaster(\n    env_var_ssp126[[i]][[4]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp126)[i], \"_2.tif\")\n  )\n  terra::writeRaster(\n    env_var_ssp126[[i]][[8]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp126)[i], \"_3.tif\")\n  )\n  \n  # Clean up auxiliary files\n  aux_files &lt;- list.files(var_dir, pattern = \"\\\\.aux\\\\.json$\", full.names = TRUE)\n  file.remove(aux_files)\n}\n\n\n# Prepare bathymetry data for SSP1 2.6\ndir.create(\"data/proj_ssp126/bat\", showWarnings = FALSE)\nfor (i in 1:3) {\n  terra::writeRaster(bat, filename = paste0(\"data/proj_ssp126/bat/bat_\", i, \".tif\"))\n}\n\n# Zip the data\nzip(zipfile = \"data/proj_ssp126.zip\", files = \"data/proj_ssp126\")\n\n\n\nSSP5 8.5\n\n# SSP5 8.5 projections constraints\ntime = c(\"2020-01-01T00:00:00Z\", \"2090-01-01T00:00:00Z\")\nlatitude = c(28, 51)\nlongitude = c(-10, 38)\nconstraints = list(time, longitude, latitude)\nnames(constraints) = c(\"time\", \"longitude\", \"latitude\")\n\nenv_var_ssp585 &lt;- list(\n  thetao_ssp585 = download_layers(\"thetao_ssp585_2020_2100_depthsurf\", \n                                  \"thetao_mean\", constraints),\n  so_ssp585 = download_layers(\"so_ssp585_2020_2100_depthsurf\", \n                              \"so_mean\", constraints),\n  phyc_ssp585 = download_layers(\"phyc_ssp585_2020_2100_depthsurf\", \n                                \"phyc_mean\", constraints)\n)\nnames(env_var_ssp585) &lt;- c(\"thetao\", \"so\", \"phyc\")\n\n\ndir.create(\"data/proj_ssp585\")\nfor (i in seq_along(env_var_ssp585)) {\n  var_dir &lt;- paste0(\"data/proj_ssp585/\", names(env_var_ssp585)[i])\n  dir.create(var_dir, showWarnings = FALSE)\n  \n  terra::writeRaster(\n    env_var_ssp585[[i]][[1]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp585)[i], \"_1.tif\")\n  )\n  terra::writeRaster(\n    env_var_ssp585[[i]][[4]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp585)[i], \"_2.tif\")\n  )\n  terra::writeRaster(\n    env_var_ssp585[[i]][[8]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp585)[i], \"_3.tif\")\n  )\n  \n  # Clean up auxiliary files\n  aux_files &lt;- list.files(var_dir, pattern = \"\\\\.aux\\\\.json$\", full.names = TRUE)\n  file.remove(aux_files)\n}\n\n# Prepare bathymetry data for SSP5 8.5\ndir.create(\"data/proj_ssp585/bat\", showWarnings = FALSE)\nfor (i in 1:3) {\n  terra::writeRaster(bat, filename = paste0(\"data/proj_ssp585/bat/bat_\", i, \".tif\"))\n}\n\n# Zip the data\nzip(zipfile = \"data/proj_ssp585.zip\", files = \"data/proj_ssp585\")\n\n\n\n\nStudy area polygon\nTo define the study area, we downloaded a polygon representing the Mediterranean Sea, which restricts the analysis to this region as the environmental layers extend beyond it. The shapefile was obtained from the Marine Regions repository. The specific download link for the Mediterranean Sea polygon can be found here.\n\n\n\nPolygon defining the study area for the Mediterranean Sea. This polygon restricts the analysis to the Mediterranean region, considering the extent of the environmental layers. Image obtained from the Marine Regions website.\n\n\n\ntmpdir &lt;- tempdir()\nzip_contents &lt;- utils::unzip(\"data/iho.zip\", unzip = getOption(\"unzip\"), exdir = tmpdir)\nmed_sea &lt;- list.files(tmpdir, pattern = \"\\\\.shp$\", full.names = TRUE) %&gt;% \n  sf::st_read() %&gt;% \n  sf::st_geometry() %&gt;% \n  sf::st_union() %&gt;%\n  sf::st_make_valid()\nmed_sea &lt;- sf::st_geometry(med_sea[[2]]) %&gt;% \n  sf::st_make_valid()\nsf::st_crs(med_sea) &lt;- \"epsg:4326\"\nsf::st_write(med_sea, \"data/mediterranean_sea.gpkg\")"
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#glossa-modeling",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#glossa-modeling",
    "title": "Example 2",
    "section": "GLOSSA modeling",
    "text": "GLOSSA modeling\nWith the data prepared and formatted for the GLOSSA framework, we use the glossa::run_glossa() function to launch the GLOSSA Shiny app for species distribution modeling and projection under different climate scenarios.\n\nrun_glossa()\n\nUpload the occurrence file for C. caretta and the environmental layers for model fitting and projection scenarios. For habitat suitability analysis, select the Model fitting and Model projection options from the Suitable habitat model. Enable Variable importance in the Others section to check the decrease of the F-score.\nIn the advanced options, set the following parameters: thinning precision to 2, standardize environmental data, and enlarge the polygon by 0.01 degrees to account for the lower resolution of the polygon and ensure the boundaries match properly -previously, many points near the coast were lost due to an insufficient buffer size-. Select the BART (Chipman, et al., 2010; Dorie, 2024) model and set the seed to 5648 for reproducibility.\n\n\n\nAnalysis options set in GLOSSA for modeling the distribution of Caretta caretta."
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#results",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#results",
    "title": "Example 2",
    "section": "Results",
    "text": "Results\nOnce the analysis is completed, we observe occurrence records that were excluded due to being outside the study area or too close to other points (orange points in Figure 3), as we applied a thinning precision of 2. We are left with 5572 presence points and an equal number of pseudo-absences.\n\n\n\nValidation of presence points for Caretta caretta. Points are filtered based on their proximity and location relative to the study area, resulting in 5572 presence records (blue points).\n\n\nThe model summary indicates a decent predictive capacity with an AUC of 0.9 and an F-score of 0.8 The distribution of fitted values can also be explored. Note that the model was fitted using randomly generated pseudo-absences rather than real absences.\n\n\n\nSummary of the fitted model for Caretta caretta. From left to right: the ROC curve, a plot representing the confusion matrix with the associated probabilities, and the distribution on the fitted values.\n\n\nWe can also determine which predictors play a more significant role in predicting the outcome by examining the variable importance plot. This plot shows the decrease in model F-score using permutation feature importance, which measures the increase in prediction error after shuffling the predictor values. We see that the most important variable is the sea surface temperature followed by the bathymetry. Sea turtles live their entire lives in the ocean, but they migrate to nest on coastal land, with their sex being determined by the temperature during incubation (Mancino et al., 2022).\n\n\n\nVariable importance plot using the F-score as the performance metric\n\n\nFor the climate projections, there is a predicted decrease of 16.4% in suitable habitat area (\\(\\text{km}^2\\)) in the SSP1-2.6 scenario and about 78.2% in the SSP5-8.5 scenario from the 2020s to the 2090s. The western Mediterranean and the Adriatic Sea are projected to have relatively better habitat suitability.\n\n\n\nProjected changes in suitable habitat for Caretta caretta under different climate scenarios."
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#conclusion",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#conclusion",
    "title": "Example 2",
    "section": "Conclusion",
    "text": "Conclusion\nThis example shows how GLOSSA can be used to model the distribution of Caretta caretta in the Mediterranean Sea. The steps included downloading occurrence data from GBIF, obtaining environmental layers from Bio-ORACLE, and preparing the data for model fitting and projections under various climate change scenarios."
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#computation-time",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#computation-time",
    "title": "Example 2",
    "section": "Computation time",
    "text": "Computation time\nThe analysis was performed on a single Windows 11 machine equipped with 64 GB of RAM and an Intel(R) Core(TM) i7-1165G7 processor. This processor features 4 cores and 8 threads, with a base clock speed of 2.80 GHz. The following table summarizes the computation times for various stages of the GLOSSA analysis. This provides an overview of the computational resources required for each step in the analysis.\n\nTable 1. Computation times for different stages of the GLOSSA analysis for the loggerhead sea turtle (Caretta caretta) in the Mediterranean Sea.\n\n\n\n\nTask\nExecution Time\n\n\n\n\nLoading input data\n33.65 secs\n\n\nProcessing P/A coordinates\n0.21 secs\n\n\nProcessing covariate layers\n9.80 secs\n\n\nBuilding model matrix\n12.01 secs\n\n\nFitting native range models\n0.232 mins\n\n\nVariable importance (native range)\n8.31 mins\n\n\nP/A cutoff (native range)\n0.191 mins\n\n\nProjections on fit layers (native range)\n9.84 mins\n\n\nNative range projections\n9.47 mins\n\n\nNative range\n19.74 mins\n\n\nFitting suitable habitat models\n0.232 mins\n\n\nVariable importance (suitable habitat)\n8.31 mins\n\n\nP/A cutoff (suitable habitat)\n0.191 mins\n\n\nProjections on fit layers (suitable habitat)\n9.84 mins\n\n\nSuitable habitat projections\n9.47 mins\n\n\nHabitat suitability\n0.006 mins\n\n\nSuitable habitat\n19.74 mins\n\n\nComputing functional responses\n13.35 mins\n\n\nCross-validation\n4.19 mins\n\n\nModel summary\n0.18 mins\n\n\nTotal GLOSSA analysis\n38.39 mins"
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#references",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#references",
    "title": "Example 2",
    "section": "References",
    "text": "References\n\nAssis, J., Fernández Bejarano, S. J., Salazar, V. W., Schepers, L., Gouvea, L., Fragkopoulou, E., … & De Clerck, O. (2024). Bio‐ORACLE v3. 0. Pushing marine data layers to the CMIP6 Earth System Models of climate change research. Global Ecology and Biogeography, 33(4), e13813.\nChamberlain, S. (2017). rgbif: Interface to the Global Biodiversity Information Facility API. R package version 0.9.8. https://CRAN.R-project.org/package=rgbif\nChipman, H. A., George, E. I., & McCulloch, R. E. (2010). BART: Bayesian additive regression trees.\nDorie V (2024). dbarts: Discrete Bayesian Additive Regression Trees Sampler. R package version 0.9-28, https://CRAN.R-project.org/package=dbarts.\nFernandez-Bejarano, S. J. & Salazar, V. W. (2024). biooraclee: R package to access Bio-Oracle data via ERDDAP. R package version 0.0.0.9, https://github.com/bio-oracle/biooracler\nGBIF.org (26 August 2024) GBIF Occurrence Download https://doi.org/10.15468/dl.es7562\nHijmans, R. (2024). terra: Spatial Data Analysis. R package version 1.7-81, https://rspatial.github.io/terra/, https://rspatial.org/.\nMancino, C., Canestrelli, D., & Maiorano, L. (2022). Going west: Range expansion for loggerhead sea turtles in the Mediterranean Sea under climate change. Global Ecology and Conservation, 38, e02264.\nPebesma E (2018). Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal, 10(1), 439–446. doi:10.32614/RJ-2018-009.\nWickham H, François R, Henry L, Müller K & Vaughan D (2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.4, https://github.com/tidyverse/dplyr, https://dplyr.tidyverse.org."
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html",
    "title": "Example 3",
    "section": "",
    "text": "This example shows how to use GLOSSA to predict the suitable habitat of Siganus luridus in the Greek Seas and identify potential areas at risk of invasion. We achieve this by comparing the species’ native range with its projected suitable habitat. Occurrence data (2000-2020) is obtained from the GreekMarineICAS geodataset, created under the ALAS: Aliens in the Aegean – A Sea Under Siege project (https://alas.edu.gr/). Environmental data is obtained from the EU Copernicus Marine Environment Monitoring Service (https://marine.copernicus.eu/).\nWe start by loading the required R packages.\n\nlibrary(glossa)\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)"
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#introduction",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#introduction",
    "title": "Example 3",
    "section": "",
    "text": "This example shows how to use GLOSSA to predict the suitable habitat of Siganus luridus in the Greek Seas and identify potential areas at risk of invasion. We achieve this by comparing the species’ native range with its projected suitable habitat. Occurrence data (2000-2020) is obtained from the GreekMarineICAS geodataset, created under the ALAS: Aliens in the Aegean – A Sea Under Siege project (https://alas.edu.gr/). Environmental data is obtained from the EU Copernicus Marine Environment Monitoring Service (https://marine.copernicus.eu/).\nWe start by loading the required R packages.\n\nlibrary(glossa)\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)"
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#data-preparation",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#data-preparation",
    "title": "Example 3",
    "section": "Data preparation",
    "text": "Data preparation\n\nDownload occurrence data\nWe download occurrence data for Siganus luridus from the MedOBIS node, specifically from the GreekMarineICAS geodataset, part of the ALAS project. The dataset is available here (Sini et al, 2024).\nAfter downloading and unzipping the data, we filter it to retain records of Siganus luridus from 2000 to 2020, aligning with the environmental data’s timeframe. Since all records are presences, we set the pa column to 1. We then format the data, saving it as a tab-separated file with decimalLongitude, decimalLatitude, timestamp, and pa columns.\n\n# Unzip and read the data\ntmpdir &lt;- tempdir()\nzip_contents &lt;- utils::unzip(\"data/dwca-greekmarineicas_geodataset-v2.0.zip\", \n                             unzip = getOption(\"unzip\"), exdir = tmpdir)\nluridus &lt;- list.files(tmpdir, pattern = \"\\\\.txt$\", full.names = TRUE)\nluridus &lt;- read.csv2(luridus, header = TRUE, sep = \"\\t\")\n\n# Filter data for *Siganus luridus*\nluridus &lt;- luridus[luridus$scientificName == \"Siganus luridus\", ]\n\n# Select and rename columns of interest\nluridus &lt;- luridus[, c(\"decimalLongitude\", \"decimalLatitude\", \n                       \"eventDate\", \"occurrenceStatus\")]\nluridus$eventDate &lt;- as.numeric(sapply(strsplit(luridus$eventDate, \"/|-\"), \n                                       function(x) x[[1]]))\n\n# Filter data by event date to match COPERNICUS layers (2000-2020)\nluridus &lt;- luridus[luridus$eventDate &gt;= 2000 & luridus$eventDate &lt;= 2020, ]\n\n# Convert occurrence status to binary presence/absence\ntable(luridus$occurrenceStatus)\nluridus$occurrenceStatus &lt;- 1\ncolnames(luridus) &lt;- c(\"decimalLongitude\", \"decimalLatitude\", \"timestamp\", \"pa\")\n\n# Remove incomplete occurrences\nluridus &lt;- luridus[complete.cases(luridus), ]\n\n# Save cleaned data to file\nwrite.table(luridus, file = \"data/Siganus_luridus_occ.csv\", sep = \"\\t\", \n            dec = \".\", quote = FALSE)\n\n\n\nDownload environmental data\nWe download environmental data from the Copernicus Marine Service using their Python API. The data includes sea water temperature (thetao) and salinity (so) from the Mediterranean Sea Physics Reanalysis, and primary production (nppv) and dissolved oxygen (o2) from the Mediterranean Sea Biogeochemistry Reanalysis product. The data is downloaded yearly (2000-2020) at a grid resolution of 1/24 degrees for a depth range of 2 to 40 meters, which corresponds to the species’ preferred habitat, as described in FishBase (Gothel, 1992).\nimport copernicusmarine\n\n# Download temperature data version \"202211\"\ncopernicusmarine.subset(\n  dataset_id=\"cmems_mod_med_phy-tem_my_4.2km_P1Y-m\",\n  variables=[\"thetao\"],\n  minimum_longitude=19,\n  maximum_longitude=30,\n  minimum_latitude=34,\n  maximum_latitude=41.1,\n  start_datetime=\"2000-01-01T00:00:00\",\n  end_datetime=\"2020-12-31T23:59:00\",\n  minimum_depth=2,\n  maximum_depth=40,\n)\n\n# Download salinity data version \"202211\"\ncopernicusmarine.subset(\n  dataset_id=\"cmems_mod_med_phy-sal_my_4.2km_P1Y-m\",\n  variables=[\"so\"],\n  minimum_longitude=19,\n  maximum_longitude=30,\n  minimum_latitude=34,\n  maximum_latitude=41.1,\n  start_datetime=\"2000-01-01T00:00:00\",\n  end_datetime=\"2020-12-31T23:59:00\",\n  minimum_depth=2,\n  maximum_depth=40,\n)\n\n# Download biogeochemical data version \"202211\"\ncopernicusmarine.subset(\n  dataset_id=\"cmems_mod_med_bgc-bio_my_4.2km_P1Y-m\",\n  variables=[\"nppv\", \"o2\"],\n  minimum_longitude=19,\n  maximum_longitude=30,\n  minimum_latitude=34,\n  maximum_latitude=41.1,\n  start_datetime=\"2000-01-01T00:00:00\",\n  end_datetime=\"2020-12-31T23:59:00\",\n  minimum_depth=2,\n  maximum_depth=40,\n)\nOnce the data is downloaded, we compute the mean values of the environmental variables within the specified depth range. Apart from the dynamic variables, we also include the bathymetry as a static variable. We obtained the bathymetry from the ETOPO Global Relief Model, which can be downloaded from here. We downloaded the bedrock elevation netCDF version ETOPO 2022 with a 60 arc-second resolution.\n\n# Load biogeochemical variables\nbiogeochem_variables &lt;- terra::rast(\"data/cmems_mod_med_bgc-bio_my_4.2km_P1Y-m_nppv-o2_19.00E-30.00E_34.02N-41.06N_3.17-37.85m_2000-01-01-2020-01-01.nc\")\n\n# Extract and process layer names\nlayer_names &lt;- names(biogeochem_variables)\nlayer_names &lt;- strsplit(layer_names, \"_\")\nlayer_names &lt;- do.call(rbind, layer_names)\nlayer_names &lt;- as.data.frame(layer_names)\ncolnames(layer_names) &lt;- c(\"variable\", \"depth\", \"year\")\n\n# Compute mean values for each year across depths\nenv_data_year &lt;- list(\"nppv\" = list(), \"o2\" = list(), \"thetao\" = list(), \"so\" = list()) \nfor (variable in unique(layer_names$variable)){\n  for (year in unique(layer_names$year)){\n    indices &lt;- which(layer_names$variable == variable & layer_names$year == year)\n    print(paste(variable, year))\n    mean_water_column &lt;- terra::mean(biogeochem_variables[[indices]], na.rm = TRUE)\n    env_data_year[[variable]][[year]] &lt;- mean_water_column\n  }\n}\n\n# Load physical variables\nphysical_variables &lt;- c(\n  terra::rast(\"data/cmems_mod_med_phy-tem_my_4.2km_P1Y-m_thetao_19.00E-30.00E_34.02N-41.06N_3.17-37.85m_2000-01-01-2020-01-01.nc\"),\n  terra::rast(\"data/cmems_mod_med_phy-sal_my_4.2km_P1Y-m_so_19.00E-30.00E_34.02N-41.06N_3.17-37.85m_2000-01-01-2020-01-01.nc\")\n)\n\n# Process physical variables\nlayer_names &lt;- names(physical_variables)\nlayer_names &lt;- strsplit(layer_names, \"_\")\nlayer_names &lt;- do.call(rbind, layer_names)\nlayer_names &lt;- as.data.frame(layer_names)\ncolnames(layer_names) &lt;- c(\"variable\", \"depth\", \"year\")\n\n# Mean between different depths\nfor (variable in unique(layer_names$variable)){\n  for (year in unique(layer_names$year)){\n    indices &lt;- which(layer_names$variable == variable & layer_names$year == year)\n    print(paste(variable, year))\n    mean_water_column &lt;- terra::mean(physical_variables[[indices]], na.rm = TRUE)\n    env_data_year[[variable]][[year]] &lt;- mean_water_column\n  }\n}\n\n# Load and process bathymetry data\nbat &lt;- terra::rast(\"data/ETOPO_2022_v1_60s_N90W180_bed.nc\")\nbat &lt;- -1*bat\nbat &lt;- terra::aggregate(bat, fact = 5, fun = \"mean\")\nr &lt;- terra::rast(terra::ext(env_data_year[[1]][[1]]), \n                 res = terra::res(env_data_year[[1]][[1]]))\nbat &lt;- terra::resample(bat, r)\nfor (i in seq_len(length(env_data_year[[1]]))){\n  env_data_year[[\"bat\"]][[i]] &lt;- bat\n}\n\n# Save processed layers to files\ndir.create(\"data/fit_layers\")\ndir.create(\"data/fit_layers/bat\")\ndir.create(\"data/fit_layers/thetao\")\ndir.create(\"data/fit_layers/so\")\ndir.create(\"data/fit_layers/nppv\")\ndir.create(\"data/fit_layers/o2\")\nfor (i in seq_len(length(env_data_year[[1]]))){\n  for (j in names(env_data_year)){\n    terra::writeRaster(\n      env_data_year[[j]][[i]], \n      filename = paste0(\"data/fit_layers/\", j ,\"/\", j, \"_\", i, \".tif\")\n    )\n  }\n}\n\n# Compress the layers into a zip file\nzip(zipfile = \"data/fit_layers.zip\", files = \"data/fit_layers\")\n\nAdditionally, we created a proj_layers.zip file containing the layers for 2020, allowing us to predict the native range and suitable habitat under “present” conditions.\nproj_layers.zip\n    ├───bat\n    │       bat_21.tif\n    ├───nppv\n    │       nppv_21.tif\n    ├───o2\n    │       o2_21.tif\n    ├───so\n    │       so_21.tif\n    └───thetao\n            thetao_21.tif"
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#glossa-modeling",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#glossa-modeling",
    "title": "Example 3",
    "section": "GLOSSA modeling",
    "text": "GLOSSA modeling\nWith the data formatted and ready, we run the GLOSSA Shiny app.\n\nrun_glossa()\n\nWe upload the occurrence file for S. luridus and the environmental layers for model fitting and projection (in this case, the year 2020). To compute the native range and suitable habitat, we select the Model fitting and Model projection options for both the Native range and Suitable habitat models. Additionally, we enable Functional responses, Variable importance and Cross-validation in the Others section.\nIn the advanced options, we standardize the environmental data, choose the BART model (Chipman, et al., 2010; Dorie, 2024), and set the seed to 1984 for reproducibility.\n\n\n\nAnalysis options set in GLOSSA for modeling the distribution of Siganus luridus."
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#results",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#results",
    "title": "Example 3",
    "section": "Results",
    "text": "Results\nThe model was fitted with 256 presences and 256 pseudo-absences. Many presence points were excluded due to their proximity to the coast, where the environmental layers from COPERNICUS lack data. Future analyses could benefit from using environmental data sources with better coastal resolution or imputing values. For this example, this exclusion demonstrates how GLOSSA handles these situations.\n\n\n\nDiscarded records of Siganus luridus during occurrence processing.\n\n\nDespite the data limitations, the model summary indicates good performance, with clear classification between presences and pseudo-absences for both models.\n\n\n\nSummary of the fitted models for Siganus luridus.\n\n\nSimilarly, the 5-fold cross-validation results show a high F-score for both the native range and suitable habitat models, suggesting strong predictive performance. However, keep in mind that this k-fold cross-validation is random and does not use temporal or spatial blocks, which could be relevant for more robust assessments.\n\n\n\nK-fold cross-validation for the native range and suitable habitat models.\n\n\nThe figure below shows the native range and suitable habitat predictions for 2020. The native range represents the species’ current distribution, considering spatial smoothing with latitude and longitude included as predictors in the model. In contrast, the suitable habitat map highlights areas with favorable environmental conditions for the species, which may not yet be occupied.\n\n\n\nPredicted native range and suitable habitat of Siganus luridus in 2020.\n\n\nFor example, the Thracian Sea and northeast Aegean islands were identified as high-risk areas for potential invasion, even though the species is not currently established there according to the predicted native range in 2020.\n\n\n\nDifference between mean suitable habitat and native range."
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#conclusion",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#conclusion",
    "title": "Example 3",
    "section": "Conclusion",
    "text": "Conclusion\nUsing GLOSSA and environmental data, we successfully modeled the potential suitable habitat for Siganus luridus and identified areas at risk of invasion. This information is critical for understanding and managing the impacts of invasive species on marine ecosystems."
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#computation-time",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#computation-time",
    "title": "Example 3",
    "section": "Computation time",
    "text": "Computation time\nThe table below summarizes the computation times for the various steps of the GLOSSA analysis, providing an overview of the computational resources required at each step. The analysis was performed on a single Windows 11 machine equipped with 64 GB of RAM and an Intel(R) Core(TM) i7-1165G7 processor. This processor features 4 cores and 8 threads, with a base clock speed of 2.80 GHz.\n\nTable 1. Computation times for different steps of the GLOSSA analysis for Siganus luridus in the Greek Seas.\n\n\n\n\nTask\nExecution Time\n\n\n\n\nLoading input data\n1.92 secs\n\n\nProcessing P/A coordinates\n0.004 secs\n\n\nProcessing covariate layers\n1.23 secs\n\n\nBuilding model matrix\n1.26 secs\n\n\nFitting native range models\n0.018 mins\n\n\nVariable importance (native range)\n0.60 mins\n\n\nP/A cutoff (native range)\n0.009 mins\n\n\nProjections on fit layers (native range)\n0.997 mins\n\n\nNative range projections\n0.42 mins\n\n\nNative range\n1.45 mins\n\n\nFitting suitable habitat models\n0.016 mins\n\n\nVariable importance (suitable habitat)\n0.41 mins\n\n\nP/A cutoff (suitable habitat)\n0.008 mins\n\n\nProjections on fit layers (suitable habitat)\n0.79 mins\n\n\nSuitable habitat projections\n0.38 mins\n\n\nHabitat suitability\n0.002 mins\n\n\nSuitable habitat\n1.20 mins\n\n\nComputing functional responses\n0.69 mins\n\n\nCross-validation\n0.48 mins\n\n\nModel summary\n0.016 mins\n\n\nTotal GLOSSA analysis\n3.91 mins"
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#references",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#references",
    "title": "Example 3",
    "section": "References",
    "text": "References\n\nChipman, H. A., George, E. I., & McCulloch, R. E. (2010). BART: Bayesian additive regression trees.\nDorie V (2024). dbarts: Discrete Bayesian Additive Regression Trees Sampler. R package version 0.9-28, https://CRAN.R-project.org/package=dbarts.\nGothel, H. (1992). Fauna marina del Mediterráneo. Ediciones Omega, SA, Barcelona, 319.\nSini M, Ragkousis M, Koukourouvli N, Katsanevakis S & Zenetos A (2024). Marine impactful cryptogenic and alien species in the Greek Seas: A georeferenced dataset (1893-2020). Version 2.0. Hellenic Center for Marine Research. Occurrence dataset. https://doi.org/10.25607/t2smha"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Explore research articles, reports, and publications that use GLOSSA for species distribution modeling in marine and ecological research.\n\n\n\n\n\nFuster-Alonso, A., Mestre-Tomás, J., Baez, J. C., Pennino, M. G., Barber, X., Bellido, J. M., … & Coll, M. (2024). Machine learning applied to global scale species distribution models (SDMs). PREPRINT (Version 1) available at Research Square. doi: https://doi.org/10.21203/rs.3.rs-4411399/v1\n\n\n\n\n\nHave you used GLOSSA in your research? We would love to showcase your publication! Please contact us with the details of your work."
  },
  {
    "objectID": "publications.html#full-list-of-publications",
    "href": "publications.html#full-list-of-publications",
    "title": "Publications",
    "section": "",
    "text": "Fuster-Alonso, A., Mestre-Tomás, J., Baez, J. C., Pennino, M. G., Barber, X., Bellido, J. M., … & Coll, M. (2024). Machine learning applied to global scale species distribution models (SDMs). PREPRINT (Version 1) available at Research Square. doi: https://doi.org/10.21203/rs.3.rs-4411399/v1"
  },
  {
    "objectID": "publications.html#submit-your-work",
    "href": "publications.html#submit-your-work",
    "title": "Publications",
    "section": "",
    "text": "Have you used GLOSSA in your research? We would love to showcase your publication! Please contact us with the details of your work."
  }
]