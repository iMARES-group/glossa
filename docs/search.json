[
  {
    "objectID": "under_construction.html",
    "href": "under_construction.html",
    "title": "Under construction",
    "section": "",
    "text": "Our team is hard at work to provide you with comprehensive guides and resources.\nThank you for your patience. Stay tuned for updates!"
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html",
    "title": "Example 1",
    "section": "",
    "text": "This example demonstrates how to use the GLOSSA framework to model the suitable habitat of Thunnus albacares (yellowfin tuna) on a global scale under different climate scenarios. We will use occurrence data from 1850 to 2014, downloaded from OBIS (Accessed on 26/08/2024), and historical and future environmental projections from ISIMIP (https://data.isimip.org/). The goal is to predict how habitat suitability for Thunnus albacares may change under the SSP1-2.6 (sustainable development) and SSP5-8.5 (high emissions) climate scenarios.\nFirst, we will load the glossa package, as well as terra and sf to work with spatial rasters and vector data. We also load robis for downloading species occurrences. Additionally, the dplyr package will be used for data manipulation.\n\nlibrary(glossa)\nlibrary(robis)\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Global species distribution model"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#introduction",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#introduction",
    "title": "Example 1",
    "section": "",
    "text": "This example demonstrates how to use the GLOSSA framework to model the suitable habitat of Thunnus albacares (yellowfin tuna) on a global scale under different climate scenarios. We will use occurrence data from 1850 to 2014, downloaded from OBIS (Accessed on 26/08/2024), and historical and future environmental projections from ISIMIP (https://data.isimip.org/). The goal is to predict how habitat suitability for Thunnus albacares may change under the SSP1-2.6 (sustainable development) and SSP5-8.5 (high emissions) climate scenarios.\nFirst, we will load the glossa package, as well as terra and sf to work with spatial rasters and vector data. We also load robis for downloading species occurrences. Additionally, the dplyr package will be used for data manipulation.\n\nlibrary(glossa)\nlibrary(robis)\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Global species distribution model"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#data-preparation",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#data-preparation",
    "title": "Example 1",
    "section": "Data preparation",
    "text": "Data preparation\n\nDownload occurrence data\nWe will download occurrence data for Thunnus albacares from the OBIS database (https://obis.org/taxon/127027) using the robis package. After that, we’ll select the necessary columns for GLOSSA (decimalLongitude, decimalLatitude, timestamp, and pa), and since all records indicate presences, we’ll replace them with a value of 1 (where 1 represents presence and 0 absence, as required by GLOSSA). We will remove records with missing values and filter the data to include only records from 1850 to 2014, aligning with the temporal coverage of our environmental layers.\n\n# Download data from OBIS\nalbacares &lt;- robis::occurrence(scientificname = \"Thunnus albacares\")\n\n# Format data to fit GLOSSA\n# Select and rename columns of interest\nalbacares &lt;- albacares[, c(\"decimalLongitude\", \"decimalLatitude\", \n                           \"date_year\", \"occurrenceStatus\")]\ncolnames(albacares) &lt;- c(\"decimalLongitude\", \"decimalLatitude\", \"timestamp\", \"pa\")\n\n# Convert presence data to 1 (for presences)\ntable(albacares$pa)\n# In this case, we only have presences\nalbacares &lt;- albacares[albacares$pa %in% c(\"present\", \"Present\", \"Presente\", \"P\"), ] \nalbacares$pa &lt;- 1\n\n# Remove incomplete records (with NA values)\nalbacares &lt;- albacares[complete.cases(albacares), ]\n\n# Filter study period to match environmental variables (1850-2014)\nalbacares &lt;- albacares[albacares$timestamp &gt;= 1850 & albacares$timestamp &lt;= 2014, ]\n\n# Save to file\nwrite.table(as.data.frame(albacares), file = \"data/Thunnus_albacares_occ.csv\", \n            sep = \"\\t\", dec = \".\", quote = FALSE)\n\n\n\nDownload environmental data\nNext, we will download environmental data from ISIMIP, using the GFDL-ESM4 Earth System Model from the ISIMIP3b climate dataset. The variables we’ll download include:\n\nSea surface temperature (tos)\nSea surface water salinity (so)\nPhytoplankton content vertically integrated over all oceans levels (phyc)\n\nAdditionally, we will download bathymetry data from the ETOPO 2022 Global Relief Model by NOAA to include in our analysis. We downloaded the bedrock elevation netCDF version ETOPO 2022 with a 60 arc-second resolution.\n\n# Load environmental data layers\nenv_data &lt;- list(\n  tos = terra::rast(\"data/gfdl-esm4_r1i1p1f1_historical_tos_60arcmin_global_monthly_1850_2014.nc\"),\n  so = terra::rast(\"data/gfdl-esm4_r1i1p1f1_historical_so-surf_60arcmin_global_monthly_1850_2014.nc\"),\n  phyc = terra::rast(\"data/gfdl-esm4_r1i1p1f1_historical_phyc-vint_60arcmin_global_monthly_1850_2014.nc\")\n)\n\n# Process data to calculate annual means for each variable\nenv_data_year &lt;- list()\nn &lt;- 1\nfor (i in seq(from = 1056, to = (1980 - 12), by = 12)){\n  for (j in names(env_data)){\n    env_data_year[[j]][[n]] &lt;- terra::mean(env_data[[j]][[i:(i+11)]])\n  }\n  n &lt;- n + 1\n}\n\n# Prepare bathymetry data and resample to match environmental data resolution\nbat &lt;- terra::rast(\"data/ETOPO_2022_v1_60s_N90W180_bed.nc\")\nbat &lt;- -1*bat\nbat &lt;- terra::aggregate(bat, fact = 60, fun = \"mean\")\nr &lt;- terra::rast(terra::ext(env_data_year[[1]][[1]]), \n                 res = terra::res(env_data_year[[1]][[1]]))\nbat &lt;- terra::resample(bat, r)\nfor (i in seq_len(length(env_data_year[[1]]))){\n  env_data_year[[\"bat\"]][[i]] &lt;- bat\n}\n\n# Save processed layers to files\ndir.create(\"data/fit_layers\")\ndir.create(\"data/fit_layers/bat\")\ndir.create(\"data/fit_layers/tos\")\ndir.create(\"data/fit_layers/so\")\ndir.create(\"data/fit_layers/phyc\")\nfor (i in seq_len(length(env_data_year[[1]]))){\n  for (j in names(env_data_year)){\n    terra::writeRaster(\n      env_data_year[[j]][[i]], \n      filename = paste0(\"data/fit_layers/\", j ,\"/\", j, \"_\", i, \".tif\")\n    )\n  }\n}\n\n# Zip the output files\nzip(zipfile = \"data/fit_layers.zip\", files = \"data/fit_layers\")\n\n\n\nClimate projections: SSP1-2.6 and SSP5-8.5\nWe will now download climate projections for the SSP1-2.6 and SSP5-8.5 scenarios, which represent two different future climate pathways. The SSP1-2.6 scenario assumes sustainable development and slower reductions in CO2 emissions, while the SSP5-8.5 scenario assumes a high-emissions future.\n\nSSP1-2.6\nThe following environmental variables will be downloaded for the SSP1-2.6 scenario:\n\ntos: Sea surface temperature\nso: Sea surface water salinity\nphyc: Phytoplankton content\n\n\n# Load SSP1-2.6 climate projection data\nenv_data &lt;- list(\n  tos = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp126_tos_60arcmin_global_monthly_2015_2100.nc\"),\n  so = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp126_so-surf_60arcmin_global_monthly_2015_2100.nc\"),\n  phyc = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp126_phyc-vint_60arcmin_global_monthly_2015_2100.nc\")\n)\n\n# Extract data for 2025, 2050, and 2100\nenv_data_year &lt;- list()\nn &lt;- 1\nfor (i in c(121, 421, 1021)){\n  for (j in names(env_data)){\n    env_data_year[[j]][[n]] &lt;- terra::mean(env_data[[j]][[i:(i+11)]])\n  }\n  n &lt;- n + 1\n}\n\n# Add bathymetry data to the layers\nfor (i in seq_len(length(env_data_year[[1]]))){\n  env_data_year[[\"bat\"]][[i]] &lt;- bat\n}\n\n# Save projection data to files\ndir.create(\"data/proj_ssp126\")\ndir.create(\"data/proj_ssp126/bat\")\ndir.create(\"data/proj_ssp126/tos\")\ndir.create(\"data/proj_ssp126/so\")\ndir.create(\"data/proj_ssp126/phyc\")\nfor (i in seq_len(length(env_data_year[[1]]))){\n  for (j in names(env_data_year)){\n    terra::writeRaster(\n      env_data_year[[j]][[i]], \n      filename = paste0(\"data/proj_ssp126/\", j ,\"/\", j, \"_\", i, \".tif\")\n    )\n  }\n}\n\n# Zip the output files\nzip(zipfile = \"data/proj_ssp126.zip\", files = \"data/proj_ssp126\")\n\n\n\nSSP5-8.5\nSimilarly, we will download environmental data for the SSP5-8.5 scenario.\n\ntos: Sea surface temperature\nso: Sea surface water salinity\nphyc: Phytoplankton content\n\n\n# Load SSP5-8.5 climate projection data\nenv_data &lt;- list(\n  tos = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp585_tos_60arcmin_global_monthly_2015_2100.nc\"),\n  so = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp585_so-surf_60arcmin_global_monthly_2015_2100.nc\"),\n  phyc = terra::rast(\"data/gfdl-esm4_r1i1p1f1_ssp585_phyc-vint_60arcmin_global_monthly_2015_2100.nc\")\n)\n\n# Extract data for 2025, 2050, and 2100\nenv_data_year &lt;- list()\nn &lt;- 1\nfor (i in c(121, 421, 1021)){\n  for (j in names(env_data)){\n    env_data_year[[j]][[n]] &lt;- terra::mean(env_data[[j]][[i:(i+11)]])\n  }\n  n &lt;- n + 1\n}\n\n# Add bathymetry data to the layers\nfor (i in seq_len(length(env_data_year[[1]]))){\n  env_data_year[[\"bat\"]][[i]] &lt;- bat\n}\n\n# Save projection data to files\ndir.create(\"data/proj_ssp585\")\ndir.create(\"data/proj_ssp585/bat\")\ndir.create(\"data/proj_ssp585/tos\")\ndir.create(\"data/proj_ssp585/so\")\ndir.create(\"data/proj_ssp585/phyc\")\nfor (i in seq_len(length(env_data_year[[1]]))){\n  for (j in names(env_data_year)){\n    terra::writeRaster(\n      env_data_year[[j]][[i]], \n      filename = paste0(\"data/proj_ssp585/\", j ,\"/\", j, \"_\", i, \".tif\")\n    )\n  }\n}\n\n# Zip the output files\nzip(zipfile = \"data/proj_ssp585.zip\", files = \"data/proj_ssp585\")",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Global species distribution model"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#glossa-modeling",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#glossa-modeling",
    "title": "Example 1",
    "section": "GLOSSA modeling",
    "text": "GLOSSA modeling\nNow that we have our environmental data ready, we can proceed with fitting the habitat suitability model using GLOSSA.\n\nrun_glossa()\n\nWe upload the occurrence file for T. albacares along with the environmental layers for model fitting and the two projection scenarios. Since our goal is to compute suitable habitat, we select the Model fitting and Model projection options from the Suitable habitat model. Additionally, we enable Functional responses in the Others section to compute the response curves.\nIn the advanced options, we adjust the settings as follows: we select a thinning precision of 2 and standardize the environmental data. We choose the BART model (Chipman, et al., 2010; Dorie, 2024) and set the seed to 4572 for reproducibility.\n\n\n\nAnalysis options set in GLOSSA for modeling the distribution of Thunnus albacares.",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Global species distribution model"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#results",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#results",
    "title": "Example 1",
    "section": "Results",
    "text": "Results\nIn the Reports section, we can observe the suitable habitat projections for both climate scenarios (SSP1-2.6 and SSP5-8.5). The model was built using a total of 91508 presence records. In the model summary section, we can see the ROC curve with a high AUC value, indicating good model performance. The middle plot shows the optimal cutoff point, and the rightmost plot displays the distribution of the fitted values.\n\n\n\nModel summary.\n\n\nAfter selecting the desired climate scenario in the GLOSSA predictions box, we can view the evolution of the potential suitable area over time for the projected layers (2025, 2050, and 2100) in the top section. For both climate scenarios, a reduction in the potential suitable habitat area is evident, with a more pronounced decline under the high-emission scenario (20% reduction) compared to the low-emission scenario (8.3% reduction).\n\n\n\nDecrease in potential suitable habitat.\n\n\nThe GLOSSA predictions tab allows us to explore projections for the uploaded layers across different years and climate scenarios. The results reveal a decline in occurrence probability from 2025 to 2100 for both SSP1-2.6 (low-emission) and SSP5-8.5 (high-emission) scenarios. Under the SSP5-8.5 scenario, the decrease in occurrence probability is not only more pronounced, but the species’ distribution has also shifted away from equatorial and tropical regions towards subtropical zones, highlighting the impact of higher emissions on species distributions.\n\n\n\nThunnus albacares GLOSSA results. a) Mean of posterior distribution of occurrence probability for suitable habitat model in both climate scenarios. b) Variable importance. c) Response curve for sea surface temperature.\n\n\nThe Variable importance plot shows which predictor variables are playing a more important role in predicting the species occurrences. Sea surface temperature appears as the variable that most improves predictive performance, followed by salinity and bathymetry, which also contribute meaningfully to the model’s accuracy. In contrast, primary productivity has a lower importance score, suggesting it provides less predictive power compared to the other variables in this model.\nFinally, we have computed the response curve for each predictor variable. The response curve for sea surface temperature indicates that the probability of species presence peaks at around 25°C, illustrating the species’ preference for moderate water temperatures. As temperatures deviate from this optimal range, the probability of occurrence declines sharply, underscoring the species’ vulnerability to temperature changes.",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Global species distribution model"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#conclusion",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#conclusion",
    "title": "Example 1",
    "section": "Conclusion",
    "text": "Conclusion\nThis example shows how GLOSSA can be used to model the global distribution of Thunnus albacares and assess potential shifts in its habitat under different climate scenarios.",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Global species distribution model"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#computation-time",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#computation-time",
    "title": "Example 1",
    "section": "Computation time",
    "text": "Computation time\nThe analysis was performed on a single Windows 11 machine equipped with 64 GB of RAM and an Intel(R) Core(TM) i7-1165G7 processor. This processor features 4 cores and 8 threads, with a base clock speed of 2.80 GHz. The following table summarizes the computation times for various stages of the GLOSSA analysis. This provides an overview of the computational resources required for each step in the analysis.\n\nTable 1. Computation times for different stages of the GLOSSA analysis for the global scale example.\n\n\n\n\n\n\n\n\nTask\nExecution Time\n\n\n\n\nLoading input data\n6.38 secs\n\n\nProcessing P/A coordinates\n0.69 secs\n\n\nProcessing covariate layers\n3.93 secs\n\n\nBuilding model matrix\n121.76 secs\n\n\nFitting native range models\n5.26 mins\n\n\nVariable importance (native range)\n165.21 mins\n\n\nP/A cutoff (native range)\n4.23 mins\n\n\nProjections on fit layers (native range)\n165.90 mins\n\n\nNative range projections\n4.15 mins\n\n\nNative range\n0.004 mins\n\n\nFitting suitable habitat models\n5.26 mins\n\n\nVariable importance (suitable habitat)\n165.21 mins\n\n\nP/A cutoff (suitable habitat)\n4.23 mins\n\n\nProjections on fit layers (suitable habitat)\n165.90 mins\n\n\nSuitable habitat projections\n4.15 mins\n\n\nHabitat suitability\n0.004 mins\n\n\nSuitable habitat\n179.54 mins\n\n\nComputing functional responses\n273.79 mins\n\n\nCross-validation\n89.25 mins\n\n\nModel summary\n3.93 mins\n\n\nTotal GLOSSA analysis\n548.72 mins",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Global species distribution model"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Thunnus_albacares_example1.html#references",
    "href": "pages/tutorials_examples/Thunnus_albacares_example1.html#references",
    "title": "Example 1",
    "section": "References",
    "text": "References\n\nChipman, H. A., George, E. I., & McCulloch, R. E. (2010). BART: Bayesian additive regression trees.\nDorie V (2024). dbarts: Discrete Bayesian Additive Regression Trees Sampler. R package version 0.9-28, https://CRAN.R-project.org/package=dbarts.\nISIMIP Project, 2024. Climate projections data. Available at https://data.isimip.org/.\nOBIS (2024) Ocean Biodiversity Information System. Intergovernmental Oceanographic Commission of UNESCO. https://obis.org.\nNOAA National Centers for Environmental Information, 2022. ETOPO 2022 Global Relief Model. Available at https://www.ncei.noaa.gov/products/etopo-global-relief-model.",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Global species distribution model"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/index.html",
    "href": "pages/tutorials_examples/index.html",
    "title": "Tutorials and examples",
    "section": "",
    "text": "Example 1\n\n\nWorldwide suitable habitat of Thunnus albacares\n\n\n13 min\n\n\nThis example demonstrates how to use the GLOSSA framework to model the suitable habitat of Thunnus albacares (yellowfin tuna) on a global scale under different climate…\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2\n\n\nDistribution of the loggerhead sea turtle in the Mediterranean Sea\n\n\n17 min\n\n\nThis vignette provides a detailed example of fitting a single-species distribution model within a user-defined region.\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3\n\n\nPotential risk areas of Siganus luridus in the Greek Seas\n\n\n12 min\n\n\nThis example shows how to use GLOSSA to predict the suitable habitat of Siganus luridus in the Greek Seas and identify potential areas at risk of invasion.\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Documentation",
      "Tutorials and examples"
    ]
  },
  {
    "objectID": "pages/documentation/working_with_input_data.html",
    "href": "pages/documentation/working_with_input_data.html",
    "title": "Preparing your data",
    "section": "",
    "text": "To ensure smooth and accurate analyses in GLOSSA, your data must be formatted correctly. This guide will walk you through preparing the necessary files: species occurrence data, environmental data, and optional projection layers and study area polygons.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Preparing your data"
    ]
  },
  {
    "objectID": "pages/documentation/working_with_input_data.html#species-occurrence-data",
    "href": "pages/documentation/working_with_input_data.html#species-occurrence-data",
    "title": "Preparing your data",
    "section": "Species occurrence data",
    "text": "Species occurrence data\nSpecies occurrence data must be provided in a tab-separated file (TSV, tab-separated CSV, etc.). This file should include the following columns:\n\ndecimalLongitude: Longitude of the occurrence point in decimal degrees.\ndecimalLatitude: Latitude of the occurrence point in decimal degrees.\npa: Presence (1) or absence (0) of the species. If this column is missing, GLOSSA will assume all rows represent presences (1, presence-only) and will generate pseudo-absences before the modeling step.\ntimestamp: The time when the occurrence was recorded. This column is optional. If used, GLOSSA will match each occurrence to the environmental data from that specific time period. If omitted, GLOSSA will assume all occurrences occurred at the same time.\n\n\nExample:\ndecimalLongitude decimalLatitude pa   timestamp\n5.42909          43.20937        1    1\n-43.05000        49.03000        0    1\n-2.52369         47.29234        1    2\n34.05400         -26.91300       1    3\n\n\n\n\n\n\nTip\n\n\n\n\nEnsure that all occurrence points fall within the study area defined by your environmental data to avoid losing data points due to missing covariate values.\nDouble-check for formatting errors or missing columns before uploading to avoid processing issues.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Preparing your data"
    ]
  },
  {
    "objectID": "pages/documentation/working_with_input_data.html#environmental-data",
    "href": "pages/documentation/working_with_input_data.html#environmental-data",
    "title": "Preparing your data",
    "section": "Environmental data",
    "text": "Environmental data\nEnvironmental data is provided as raster layers in formats like .tif or .nc (NetCDF). These layers are used as predictors for species distributions and should represent variables like temperature, salinity, or other relevant data. All environmental layers must be uploaded as a ZIP file, with each variable organized into separate subdirectories. Each subdirectory should contain raster files corresponding to the relevant time periods.\nGLOSSA supports the use of categorical variables in your models. These variables can represent categorical data such as ice cover, habitat classes, or other discrete data. Categorical variables are automatically transformed using one-hot encoding by GLOSSA and the dbarts package, converting them into new binary columns for modeling. To ensure a good fit in the GLOSSA workflow, categorical layers must be properly formatted before uploading.\n\n\n\n\n\n\nIncluding categorical variables in GLOSSA\n\n\n\n\n\nGLOSSA now supports including categorical variables in your models, but they must be formatted correctly to ensure a smooth integration.\n\nAs with continuous variables, categorical layers must be provided as raster files (e.g., .tif, .nc, .asc) where the values are integer IDs that map to specific categories. For example, if representing ice cover, you might define 0 as “no ice” and 1 as “ice-covered.”\nRasters must be defined as factors, which typically requires metadata files (e.g., .xml) to enable proper mapping between the integer values and their corresponding categories. If categorical variables are not uploaded as factors, GLOSSA will treat them as continuous variables.\nWhen using categorical layers across time steps or for projections, ensure the categories remain consistent. The set of categories must match or be a subset of those observed in the training data. Avoid attempting to predict on unobserved categories.\n\nTo include categorical variables in GLOSSA, you need to provide categorical factor rasters. You can create these using functions like terra::as.factor() to convert integer rasters into factor rasters with defined levels and labels:\nlibrary(terra)\nraster &lt;- rast(\"ice_cover.tif\")\nraster &lt;- as.factor(raster)\nlevels(raster) &lt;- data.frame(id = c(0, 1), label = c(\"no\", \"yes\"))\nwriteRaster(raster, \"ice_cover_factor.tif\")\nWhen saving your file, the raster metadata is often stored in a separate file (e.g., .xml, .aux.xml). Ensure these metadata files are placed alongside the corresponding raster file in the same directory of the ZIP file before uploading.\nenvironmental_data.zip\n    ├── continuous_variable\n    │     └── cont_var.tif\n    ├── categorical_variable\n    │     ├── cat_var.tif\n    │     └── cat_var.tif.aux.xml\n\n\n\nHere are some guidelines for preparing your files:\n\nConsistent resolution and extent: Ensure that all raster layers have the same resolution and geographic extent. GLOSSA will check for mismatches and notify you with warnings or errors:\n\nIf the geographic extents differ, GLOSSA will extend the smaller layers with missing values to match the largest raster.\nIf the resolutions differ, GLOSSA will stop and return an error, as all layers need to be at the same resolution.\n\nTemporal alignment: If your occurrence data contains multiple time periods (timestamp column in the occurrence data), the rasters must align with those timestamps. GLOSSA expects the raster files to be ordered alphabetically by time. For example, if your occurrence data includes years 1 and 3, you should have raster files for each environmental variable for years 1, 2, and 3 (even if you don’t have occurrence data for year 2). In this case, you can provide a blank or duplicate raster for year 2, but the file must exist to ensure proper indexing for year 3.\n\n\nExample ZIP structure:\nenvironmental_data.zip\n    ├── temperature\n    │     ├── temp_1.tif\n    │     ├── temp_2.tif\n    │     └── temp_3.tif\n    ├── salinity\n    │     ├── sal_1.tif\n    │     ├── sal_2.tif\n    │     └── sal_3.tif\n\n\n\n\n\n\nTip\n\n\n\n\nLarge raster files may slow down the processing. For testing purposes, consider using lower-resolution rasters or aggregating the cells using the terra::aggregate() function in R\nUse clear and consistent file names, especially when handling multiple time periods or variables.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Preparing your data"
    ]
  },
  {
    "objectID": "pages/documentation/working_with_input_data.html#projection-layers-optional-but-likely-your-primary-interest",
    "href": "pages/documentation/working_with_input_data.html#projection-layers-optional-but-likely-your-primary-interest",
    "title": "Preparing your data",
    "section": "Projection layers (optional, but likely your primary interest)",
    "text": "Projection layers (optional, but likely your primary interest)\nProjection layers allow you to forecast species distributions under different environmental conditions, such as future climate scenarios. These layers should follow the exact format as the environmental data, with identical subdirectory names and matching variable names.\nSome guidelines for preparing your files:\n\nFile order: Ensure that the files within each subdirectory are ordered consistently across variables. The first file of each variable (e.g., temperature, salinity) will be treated as corresponding to the same time period, and GLOSSA will stack and project them together as part of the same time series. That is, in the following example, GLOSSA will make one projection for the temp_projection_1.tif and sal_projection_1.tif scenario, and a different projection for the conditions of temp_projection_2.tif and sal_projection_2.tif.\nMultiple scenarios: If you’re working with multiple independent scenarios (e.g., two different climate models), upload each scenario in a separate ZIP file. This way, they won’t be included in the same time series, allowing you to compare scenarios separately during plotting and exporting.\n\n\nExample projection ZIP structure:\nprojection_layers.zip\n    ├── temperature\n    │     ├── temp_projection_1.tif\n    │     └── temp_projection_2.tif\n    ├── salinity\n    │     ├── sal_projection_1.tif\n    │     └── sal_projection_2.tif\n\n\n\n\n\n\nTip\n\n\n\n\nEnsure that projection layers have the same resolution, geographic extent, coordinate reference systems (CRS - WGS84), and variable names as the environmental layers used during model fitting. This consistency is crucial for accurate forecasting and projections.\nIf your projections involve different time periods, ensure the raster files are clearly organized and ordered to reflect these periods accurately.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Preparing your data"
    ]
  },
  {
    "objectID": "pages/documentation/working_with_input_data.html#study-area-polygon-optional",
    "href": "pages/documentation/working_with_input_data.html#study-area-polygon-optional",
    "title": "Preparing your data",
    "section": "Study area polygon (optional)",
    "text": "Study area polygon (optional)\nYou can define a study area polygon to limit the geographic scope of your analysis. This will crop the environmental layers and filter out occurrence points that fall outside the study area. By default, GLOSSA uses the extent of your environmental rasters to define the study area, fitting the model only with occurrences that have valid values for all predictor variables. It also projects only onto cells covered by all environmental variables.\nHowever, if your rasters cover a larger region than your area of interest or if you have occurrence points outside the region you’d like to filter, you can upload a custom polygon. This allows you to specify the geographic region of interest, and GLOSSA will automatically crop the environmental layers and restrict the analysis to within the polygon boundaries.\nThe supported formats of this file are:\n\nGPKG (GeoPackage)\nKML\nGeoJSON\n\n\nExample use case:\nYou might have environmental data for an entire ocean but only want to model species distributions within the Mediterranean Sea. Uploading a Mediterranean Sea polygon will crop the data accordingly.\n\n\n\n\n\n\nTip\n\n\n\n\nIf the resolution of your polygon is too coarse, you can apply a buffer to expand or refine it. This buffer option can be tuned in the next section of the documentation.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Preparing your data"
    ]
  },
  {
    "objectID": "pages/documentation/working_with_input_data.html#conclusion",
    "href": "pages/documentation/working_with_input_data.html#conclusion",
    "title": "Preparing your data",
    "section": "Conclusion",
    "text": "Conclusion\nBy properly formatting your data, you’ll ensure that GLOSSA runs smoothly and provides accurate results. Once your data is ready, move on to the next step: Running a new analysis.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Preparing your data"
    ]
  },
  {
    "objectID": "pages/documentation/run_new_analysis.html",
    "href": "pages/documentation/run_new_analysis.html",
    "title": "Running a new analysis",
    "section": "",
    "text": "In this section, we’ll guide you through setting up and running your first species distribution analysis in GLOSSA. Whether you’re new to modeling or experienced, this walkthrough will help you configure the analysis step-by-step.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Running a new analysis"
    ]
  },
  {
    "objectID": "pages/documentation/run_new_analysis.html#step-1-launch-glossa",
    "href": "pages/documentation/run_new_analysis.html#step-1-launch-glossa",
    "title": "Running a new analysis",
    "section": "Step 1: Launch GLOSSA",
    "text": "Step 1: Launch GLOSSA\nAfter installing GLOSSA, open R or RStudio, load the package, and launch the app:\nlibrary(glossa)\nrun_glossa()\nThis will open the GLOSSA interface in your default web browser, where you’ll start your analysis by clicking the “New analysis” button.\n\n\n\nHow to move to the New analysis tab from the Home tab of GLOSSA",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Running a new analysis"
    ]
  },
  {
    "objectID": "pages/documentation/run_new_analysis.html#step-2-upload-your-data",
    "href": "pages/documentation/run_new_analysis.html#step-2-upload-your-data",
    "title": "Running a new analysis",
    "section": "Step 2: Upload your data",
    "text": "Step 2: Upload your data\nIn the New analysis tab, you’ll upload your input data. You’ll need the following:\n\nOccurrences:\n\nUpload your tab-separated file containing species occurrence data with decimalLongitude, decimalLatitude, pa (optional), and timestamp (optional). You can upload multiple files, and each will be analyzed in independent models within the same run. For example, in the figure below, two files (sp1.txt and sp2.csv) for different species were uploaded. The plot on the right shows a preview of the occurrences.\n\nEnvironmental data:\n\nUpload your environmental raster layers as a ZIP file, following the structure described in the Preparing Your Data section. Make sure all rasters have the same resolution and extent. In the figure, we uploaded fit_layers.zip, which contains the environmental data.\n\nProjection layers (optional):\n\nIf you have future scenarios (e.g., climate change projections), upload your projection layers. Using the fitted model, GLOSSA wil predict using this rasters. You can upload multiple ZIP files with different projection layers, as shown in the figure where proj_layers_1.zip and proj_layers_2.zip were uploaded.\n\nStudy area (optional):\n\nTo limit your analysis to a specific region, upload a study area polygon in GPKG, KML, or GeoJSON format. The previsualization plot will help you check the spatial coverage of the polygon.\n\n\n\n\n\nNew analysis tab",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Running a new analysis"
    ]
  },
  {
    "objectID": "pages/documentation/run_new_analysis.html#step-3-configure-your-analysis",
    "href": "pages/documentation/run_new_analysis.html#step-3-configure-your-analysis",
    "title": "Running a new analysis",
    "section": "Step 3: Configure your analysis",
    "text": "Step 3: Configure your analysis\nOnce the data is uploaded, you’ll configure your analysis by adjusting the following settings:\n\nChoose a model type:\n\n\nNative range: Includes both environmental variables and spatial coordinates to predict the species’ native distribution.\nSuitable habitat: Uses only environmental variables to predict suitable habitats, ignoring spatial coordinates.\n\n\n\n\n\nModel fitting: This option fits the selected model, enabling you to compute functional responses and variable importance. It also makes predictions based on an averaged environmental scenario, which is created by calculating the mean values of each environmental variable across all provided timestamps.\nModel projection: If enabled, the fitted model will be used to make projections on the uploaded projection layers.\n\n\nOther analysis options:\n\n\nFunctional responses: This option generates response curves, which show the relationship between species occurrence and each environmental variable using partial dependence plots. These plots show the marginal effect of each variable on the predicted outcome of the BART model. Response curves are calculated only for the suitable habitat model without variable standardization, so they can be interpreted on the original scale.\nVariable importance: When selected, variable importance is computed using a permutation-based approach. This measures the change in prediction error after shuffling the values of each variable, using the F-score as a metric.\nCross-validation: Enables k-fold cross-validation (k = 5) to evaluate your model’s predictive performance. The cross-validation is conducted randomly, without accounting for spatial or temporal blocks or autocorrelation\n\n\nAdvanced options:\n\n\nOccurrence thinning - Rounding precision: Apply spatial thinning to your occurrence data using a precision-based approach. You can specify the decimal precision for latitude and longitude, and GLOSSA will remove duplicate records based on the rounded coordinates.\nLayers processing - Standardized covariates: This option z-score standardizes all environmental variables (subtract the mean and divide by the standard deviation). Mean and sd are computed based on the fitting layers and is also applied to projection layers.\nPolygon processing - Enlarge polygon: If your study area polygon has a low resolution or you want to extend its coverage, you can apply a buffer to enlarge it. The buffer value is specified in degrees, and you can preview the result by clicking the “play” button in the previsualization plot.\nModel: Currently, only the BART model is available for fitting\nSet a seed: Set a seed for reproducibility.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Running a new analysis"
    ]
  },
  {
    "objectID": "pages/documentation/run_new_analysis.html#step-4-run-the-analysis",
    "href": "pages/documentation/run_new_analysis.html#step-4-run-the-analysis",
    "title": "Running a new analysis",
    "section": "Step 4: Run the analysis",
    "text": "Step 4: Run the analysis\nAfter configuring your settings, click Run Job to start the analysis. A confirmation dialog will appear, prompting you to double-check your input and settings. It’s important to ensure everything is configured correctly since some analyses may take a long time to complete, and you wouldn’t want to realize a mistake afterward.\n\n\n\n\n\nOnce confirmed, GLOSSA will begin the analysis by following these steps:\n\nData processing: GLOSSA processes your input data, removing duplicate coordinates, excluding records outside the study area, and filtering points with missing covariate values. If spatial thinning or standardization is enabled, these steps will also be applied. If a study area polygon is provided, the rasters will be cropped accordingly. For presence-only data, pseudo-absences will be generated.\nModel fitting: GLOSSA fits the BART model using presence/(pseudo-)absence data as the response variable, with environmental variables (for the suitable habitat model) and also geographic coordinates (for the native range model) as predictors. After fitting the model, GLOSSA calculates an optimal cutoff for predicting presence/absence using Youden’s index, also known as the True Skill Statistic (TSS).\nModel validation and additional analyses: GLOSSA computes validation metrics such as AUC, a confusion matrix, and the distribution of fitted values. If requested, GLOSSA also calculates functional responses, variable importance, and performs cross-validation to evaluate model performance.\nProjections: The fitted models are used to make projections based on the uploaded projection layers. Since GLOSSA operates within a Bayesian framework, it computes a predictive posterior distribution for each grid cell.\nResults: Once the analysis is complete, you’ll be automatically redirected to the Reports tab, where you can explore and interpret your results.\n\n\n\n\n\n\n\nNote\n\n\n\nThis process may take time depending on the size and resolution of your data, the number of projections, and additional options you have enabled (e.g., functional responses, cross-validation).\n\n\n\n\n\nSummary of the GLOSSA analysis workflow.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Running a new analysis"
    ]
  },
  {
    "objectID": "pages/documentation/run_new_analysis.html#conclusion",
    "href": "pages/documentation/run_new_analysis.html#conclusion",
    "title": "Running a new analysis",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve successfully run your first analysis in GLOSSA! Now it’s time to explore the outputs and dive into the results. Head over to the next section to learn how to interpret your findings: Explore the results.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Running a new analysis"
    ]
  },
  {
    "objectID": "pages/documentation/installation_setup.html",
    "href": "pages/documentation/installation_setup.html",
    "title": "Installing and setting up GLOSSA",
    "section": "",
    "text": "This guide will walk you through installing GLOSSA on your machine, setting up your environment, and preparing to run your first analysis. By the end, you’ll be ready to use GLOSSA for species distribution modeling.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Installing and setting up GLOSSA"
    ]
  },
  {
    "objectID": "pages/documentation/installation_setup.html#system-requirements",
    "href": "pages/documentation/installation_setup.html#system-requirements",
    "title": "Installing and setting up GLOSSA",
    "section": "1. System requirements",
    "text": "1. System requirements\nBefore installing GLOSSA, ensure that your system meets the following requirements:\n\nR version: GLOSSA requires R version 4.0.0 or higher. If you don’t have R installed, you can download it from CRAN.\nRTools: Check you have RTools installed for building R packages from source. You can download RTools from here.\nOperating system: GLOSSA is compatible with Windows, macOS, and Linux.\nRStudio (optional but recommended): We recommend using RStudio as it provides useful tools for working with R. Download RStudio here.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Installing and setting up GLOSSA"
    ]
  },
  {
    "objectID": "pages/documentation/installation_setup.html#installing-glossa",
    "href": "pages/documentation/installation_setup.html#installing-glossa",
    "title": "Installing and setting up GLOSSA",
    "section": "2. Installing GLOSSA",
    "text": "2. Installing GLOSSA\n\nInstall from CRAN (stable version)\nTo install the latest stable version of GLOSSA from CRAN, open R or RStudio and run the following command:\ninstall.packages(\"glossa\")\n\n\nInstall the development version from GitHub (latest features)\nFor the latest features or bug fixes, you can install the development version from GitHub. First, install devtools:\ninstall.packages(\"devtools\")\nThen, install GLOSSA from GitHub:\ndevtools::install_github(\"iMARES-group/glossa\")",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Installing and setting up GLOSSA"
    ]
  },
  {
    "objectID": "pages/documentation/installation_setup.html#launching-glossa",
    "href": "pages/documentation/installation_setup.html#launching-glossa",
    "title": "Installing and setting up GLOSSA",
    "section": "3. Launching GLOSSA",
    "text": "3. Launching GLOSSA\nOnce GLOSSA is installed, you can launch it in R or RStudio by running:\nlibrary(glossa)\nrun_glossa()\nThis command starts the GLOSSA shiny web app, which will open in your default browser.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Installing and setting up GLOSSA"
    ]
  },
  {
    "objectID": "pages/documentation/installation_setup.html#verifying-your-installation",
    "href": "pages/documentation/installation_setup.html#verifying-your-installation",
    "title": "Installing and setting up GLOSSA",
    "section": "4. Verifying Your Installation",
    "text": "4. Verifying Your Installation\nAfter installation, you can verify that GLOSSA is correctly installed by checking the package version:\npackageVersion(\"glossa\")\nIf GLOSSA has been installed successfully, this will return the installed version number. The app should open in your web browser and be ready to use.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Installing and setting up GLOSSA"
    ]
  },
  {
    "objectID": "pages/documentation/installation_setup.html#conclusion",
    "href": "pages/documentation/installation_setup.html#conclusion",
    "title": "Installing and setting up GLOSSA",
    "section": "Conclusion",
    "text": "Conclusion\nYou’re now ready to start working with GLOSSA! After installation, you can move on to preparing your data and running your first analysis. Head over to the Preparing your data section to learn how to format your species occurrence data and environmental layers for GLOSSA.\nIf you encounter any issues during installation, visit our Common issues and troubleshooting page for solutions.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Installing and setting up GLOSSA"
    ]
  },
  {
    "objectID": "pages/documentation/export_outputs.html",
    "href": "pages/documentation/export_outputs.html",
    "title": "Exporting results",
    "section": "",
    "text": "In this section, we’ll learn how to export the generated results from the GLOSSA analysis, understand the structure of the downloaded files, and show tips for working with these files.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Exporting results"
    ]
  },
  {
    "objectID": "pages/documentation/export_outputs.html#the-export-form",
    "href": "pages/documentation/export_outputs.html#the-export-form",
    "title": "Exporting results",
    "section": "The export form",
    "text": "The export form\nWhen opening the Exports tab, you’ll find a form to select which results to export. This form provides access to all results displayed in the Reports tab, as well as the data used to fit the models.\n\n\n\n\n\nIn the first field, select the species or occurrence files you want to export—multiple species can be chosen. In the second field, choose which prediction results to export: either the predictions based on the average environmental scenario used to fit the model or predictions from the different projection layers. Next, decide whether to export the results for the native range (NR) model, the suitable habitat (SH) model, or both.\nSince GLOSSA operates within a Bayesian framework, each grid cell has an associated predictive posterior distribution. In the Fields field, you can choose which summary statistics to save:\n\nmean: mean value\nmedian: quantile 0.5\nsd: standard deviation\nq0.025: quantile 0.025\nq0.975: quantile 0.975\ndiff: difference between quantile 0.975 and 0.025\npotential_presences: predicted presences (1) and absences (0) using the estimated optimal cutoff.\n\nFor these raster files, you can select the file type in the next field (TIFF, NetCDF, or ASCII).\nAdditionally, you can export other data, such as:\n\nThe processed data used to fit the model\nThe confusion matrix of predicted vs. observed occurrences\nVariable importance scores\nFunctional response plots\nCross-validation metrics\nThe value of the estimated probability cutoff for presence/absence.\n\nIf you want to export everything, use the Select All button.\nOnce you have selected what you want to export, click on the Save GLOSSA Results button. A ZIP file containing all selected results will be downloaded to your local machine.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Exporting results"
    ]
  },
  {
    "objectID": "pages/documentation/export_outputs.html#exported-zip-file",
    "href": "pages/documentation/export_outputs.html#exported-zip-file",
    "title": "Exporting results",
    "section": "Exported ZIP file",
    "text": "Exported ZIP file\nWhen you export results from GLOSSA, a ZIP file is created that organizes the outputs in a structured way, making it easier to explore specific files and load them into R for further analysis.\n\nZIP file overview\nThe ZIP file is organized by species, with a folder for each species or occurrence file you analyzed (e.g., sp1, sp2). Within each species folder, there are several subfolders and files containing specific results:\n\nsp_model_data.csv: A tab-separated file with the dataset used to fit the model after processing the data (decimalLongitude and decimalLatitude: longitude and latitude from the occurrence file., timestamp: timestamp values, pa: response variable indicating presence or absence, Xs: one column per uploaded predictor variable, grid_long and grid_lat: longitude and latitude from the raster created for the native range model).\nsp_presence_probability_cutoff.csv: Estimated optimal probability cutoff for presence/absence prediction for both the native range and suitable habitat models.\nconfusion_matrix: Contains tab-separated files (NR and SH) with confusion matrices for each model, comparing observed occurrences with predicted values and probabilities.\ncross_validation: Includes tab-separated files (NR and SH) with cross-validation results for \\(k = 10\\) folds, one row per fold.\nfunctional_responses: Contains one tab-separated file per predictor variable in the suitable habitat model, with partial dependence plot values. Each file includes a column with talues of the predictor variable used to construct the plot and the probability values (mean and quantiles 0.025 and 0.975 columns) that represent how predictor variables influence the model, that is, the changes relative to the overall central tendency.\nnative_range and suitable_habitat: These folders contain raster files with predictions for each model type (native range and suitable habitat), organized as follows:\n\nfit_layers: Results from the fitting dataset, organized by summary statistics:\n\ndiff: Difference between the 97.5th and 2.5th quantiles.\nmean: Mean prediction value.\nmedian: Median prediction (quantile 0.5).\npotential_presences: Binary presence-absence predictions based on the optimal cutoff.\nq0.025 and q0.975: Lower and upper quantiles of the posterior distribution.\nsd: Standard deviation.\n\nprojections: Contains projections for each environmental scenario (e.g., project_layers_1, project_layers_2), with the same summary statistics as above.\n\nvariable_importance: Contains a tab-separated file for each model (NR and SH) listing the variable importance score for each predictor variable. Each row represents one of the 10 permutations performed.\n\n\n\nWorking with the exported Files in R\nAll the exported files are in tab-separated format except for raster files, which are saved in the format you selected. You can load the CSV files into R with the read.csv() and read.table() functions, and raster files with the rast() function from the terra package. For example:\n# Load functional responses\nfunctional_responses &lt;- read.table(\"path/to/sp1/functional_responses/sp1_functional_response_x1.csv\", header = TRUE, sep = \"\\t\", dec = \".\")\nplot(functional_responses$value, functional_responses$mean)\n\n# Load projection file\nlibrary(terra)\nprojection &lt;- rast(\"path/to/sp1/suitable_habitat/projections/project_layers_1/mean/sp1_suitable_habitat_projections_1_mean.tif\")\nplot(projection)\n\n\n\n\n\n\nNote\n\n\n\nIf you’re exporting data for multiple species or comparing models, automating workflows in R can streamline your analysis.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Exporting results"
    ]
  },
  {
    "objectID": "pages/documentation/export_outputs.html#conclusion",
    "href": "pages/documentation/export_outputs.html#conclusion",
    "title": "Exporting results",
    "section": "Conclusion",
    "text": "Conclusion\nCongratulations! You’ve now learned how to run a complete analysis with GLOSSA, from data preparation to output export. Now it’s time for you to try it with your own data! To explore more tutorials or real case examples, visit Tutorials and Examples.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Exporting results"
    ]
  },
  {
    "objectID": "news.html",
    "href": "news.html",
    "title": "Changelog",
    "section": "",
    "text": "All notable changes to this project will be documented in this file.\n\n\n\n\n\n\n\n\nAdded support for tuning the number of trees in the sum-of-trees formulation of BART and the end-node shrinkage prior parameter to control overfitting (12/06/2025).\nResolution harmonization is now implemented for environmental layers by aggregating to the coarsest resolution, complementing the existing extent and CRS harmonization (10/06/2025).\n\n\n\n\n\nResolved a UI issue introduced in version 1.1.0 that prevented users from selecting thinning options (28/05/2025).\n\n\n\n\n\n\n\n\nAdded support for spatial block and temporal block cross-validation. Spatial block size can be determined manually or using residual or predictor autocorrelation (27/05/2025).\nContinuous Boyce Index (CBI) and Area Under the ROC Curve (AUC) are now computed for each model based on fitted values and in cross-validation (27/05/2025).\nNow three different pseudo-absence generation can be used: random background, target-group, and delimited by a buffer around presences (21/05/2025).\nA new configuration file including the input files and settings is now automatically created when exporting the results for better reproducibility (15/05/2025).\nUsers can now upload a file of raster timestamps to correctly align irregular or custom time series with occurrence data. This prevents mismatches between occurrence timestamps and raster layers when temporal coverage is not sequential or has a different starting point (11/04/2025).\nCategorical variables can now be included as predictor variables using raster layers with factors (25/11/2024).\n\n\n\n\n\nAdded confirmation dialog to close the app to avoid accidental clicks on the “Close App” button (27/05/2025).\nImproved error handling so analysis does not stop if any of the species drops an error (15/05/2025).\nFixed reactivity and selection reset issues when updating plot inputs in the report tab (e.g. functional response plot) (02/04/2025).\nFixed the broken link in the Documentation button on the GLOSSA home page (31/03/2025).\nFixed an issue where users could upload .tif files but not .tiff files, which are used interchangeably (31/03/2025).\nAdded support for handling zipped files from macOS by ignoring hidden system files (31/03/2025).\nFixed an issue where accessing factor level labels assumed the column was named “label”. Now uses the second column (31/03/2025).\nWarning messages appear when NA values are present in factor levels. Those levels are ignored in the analysis (31/03/2025).\nAllow a numerical tolerance of 1e-7 when evaluating resolution equality between provided raster layers (31/03/2025).\nUpdated functions to support raster files with metadata (e.g., .xml) for declaring categorical variables or factors (25/11/2024).\nEnhanced handling of mixed continuous and categorical datasets during layer reading and processing (25/11/2024).\nScaling now applies only to continuous variables, with categorical variables excluded and appended untransformed (25/11/2024).\nImproved functional response calculations to handle categorical predictors, filtering out levels with no observations (25/11/2024).\nAdded functionality to compute the mode across time layers for categorical variables, preserving factor levels, when computing the average environmental scenario (25/11/2024).\nChanged exported file names to avoid Windows file path length errors when unzipping. Also, changed extension from .csv to .tsv (26/05/2025). The following naming adjustments were made: suitable_habitat -&gt; sh, native_range -&gt; nr, cross_validation -&gt; cross_val, functional_responses -&gt; func_res, variable_importance -&gt; var_imp, confusion_matrix -&gt; mod_diag, fit_layers -&gt; fit, projections -&gt; proj.\n\n\n\n\n\n\nInitial CRAN submission."
  },
  {
    "objectID": "news.html#glossa-1.2.0",
    "href": "news.html#glossa-1.2.0",
    "title": "Changelog",
    "section": "",
    "text": "Added support for tuning the number of trees in the sum-of-trees formulation of BART and the end-node shrinkage prior parameter to control overfitting (12/06/2025).\nResolution harmonization is now implemented for environmental layers by aggregating to the coarsest resolution, complementing the existing extent and CRS harmonization (10/06/2025).\n\n\n\n\n\nResolved a UI issue introduced in version 1.1.0 that prevented users from selecting thinning options (28/05/2025)."
  },
  {
    "objectID": "news.html#glossa-1.1.0",
    "href": "news.html#glossa-1.1.0",
    "title": "Changelog",
    "section": "",
    "text": "Added support for spatial block and temporal block cross-validation. Spatial block size can be determined manually or using residual or predictor autocorrelation (27/05/2025).\nContinuous Boyce Index (CBI) and Area Under the ROC Curve (AUC) are now computed for each model based on fitted values and in cross-validation (27/05/2025).\nNow three different pseudo-absence generation can be used: random background, target-group, and delimited by a buffer around presences (21/05/2025).\nA new configuration file including the input files and settings is now automatically created when exporting the results for better reproducibility (15/05/2025).\nUsers can now upload a file of raster timestamps to correctly align irregular or custom time series with occurrence data. This prevents mismatches between occurrence timestamps and raster layers when temporal coverage is not sequential or has a different starting point (11/04/2025).\nCategorical variables can now be included as predictor variables using raster layers with factors (25/11/2024).\n\n\n\n\n\nAdded confirmation dialog to close the app to avoid accidental clicks on the “Close App” button (27/05/2025).\nImproved error handling so analysis does not stop if any of the species drops an error (15/05/2025).\nFixed reactivity and selection reset issues when updating plot inputs in the report tab (e.g. functional response plot) (02/04/2025).\nFixed the broken link in the Documentation button on the GLOSSA home page (31/03/2025).\nFixed an issue where users could upload .tif files but not .tiff files, which are used interchangeably (31/03/2025).\nAdded support for handling zipped files from macOS by ignoring hidden system files (31/03/2025).\nFixed an issue where accessing factor level labels assumed the column was named “label”. Now uses the second column (31/03/2025).\nWarning messages appear when NA values are present in factor levels. Those levels are ignored in the analysis (31/03/2025).\nAllow a numerical tolerance of 1e-7 when evaluating resolution equality between provided raster layers (31/03/2025).\nUpdated functions to support raster files with metadata (e.g., .xml) for declaring categorical variables or factors (25/11/2024).\nEnhanced handling of mixed continuous and categorical datasets during layer reading and processing (25/11/2024).\nScaling now applies only to continuous variables, with categorical variables excluded and appended untransformed (25/11/2024).\nImproved functional response calculations to handle categorical predictors, filtering out levels with no observations (25/11/2024).\nAdded functionality to compute the mode across time layers for categorical variables, preserving factor levels, when computing the average environmental scenario (25/11/2024).\nChanged exported file names to avoid Windows file path length errors when unzipping. Also, changed extension from .csv to .tsv (26/05/2025). The following naming adjustments were made: suitable_habitat -&gt; sh, native_range -&gt; nr, cross_validation -&gt; cross_val, functional_responses -&gt; func_res, variable_importance -&gt; var_imp, confusion_matrix -&gt; mod_diag, fit_layers -&gt; fit, projections -&gt; proj."
  },
  {
    "objectID": "news.html#glossa-1.0.0",
    "href": "news.html#glossa-1.0.0",
    "title": "Changelog",
    "section": "",
    "text": "Initial CRAN submission."
  },
  {
    "objectID": "how_to_cite.html",
    "href": "how_to_cite.html",
    "title": "How to cite GLOSSA",
    "section": "",
    "text": "How to cite GLOSSA\nThank you for using GLOSSA! The GLOSSA paper is currently in progress. Meanwhile, to cite the software in publications, you can use the preprint:\n\nMain citation for GLOSSA\nTo cite GLOSSA in publications, please use the following reference:\n\nMestre-Tomás, J., Fuster-Alonso, A., Bellido, J. M., and Coll, M. (2025). GLOSSA: a user-friendly R Shiny application for Bayesian machine learning analysis of marine species distribution. arXiv preprint arXiv:2505.05862. DOI: https://doi.org/10.48550/arXiv.2505.05862\n\n\n\nAdditional citations\nGLOSSA is built on the work of many researchers and developers. To acknowledge their work, we provide citation guidelines for key contributions and inspirational works.\n\nBayesian Additive Regression Trees (BART)\nIf you use the BART model in your analyses, please include this reference:\n\nChipman, H. A., George, E. I., & McCulloch, R. E. (2010). BART: Bayesian Additive Regression Trees. The Annals of Applied Statistics, 4(1), 266–298. DOI: https://doi.org/10.1214/09-AOAS285\n\n\n\nGlobal-scale BART modeling\nFor global-scale analyses using BART, please check the following paper:\n\nFuster-Alonso, A., Mestre-Tomás, J., Baez, J. C., Pennino, M. G., Barber, X., Bellido, J. M., … & Coll, M. (2024). Machine learning applied to global scale species distribution models (SDMs). PREPRINT (Version 1) available at Research Square. DOI: https://doi.org/10.21203/rs.3.rs-4411399/v1."
  },
  {
    "objectID": "contact_us.html",
    "href": "contact_us.html",
    "title": "Contact us",
    "section": "",
    "text": "Contact us\nWe value your feedback and are here to help you with any questions, support needs or collaboration requests. Please use the following means to get in touch with us:\n\nGitHub\nFor issues, feature requests, questions, or general feedback, please use the Issues tab on our GitHub repository:\n\nGLOSSA GitHub issues\n\n\n\nContact form\nFor special support or collaboration requests, please contact us via the contact form at the end of the iMARES website:\n\niMARES\n\n\n\nOutreach channels\nFollow our research group on Bluesky and X (Twitter) for the latest news and updates:\n\nBluesky: @iMARES_group\nX (Twitter): @iMARES_group\n\nThank you for your support! If you want to know more about the group go and visit our website: iMARES"
  },
  {
    "objectID": "get_started.html",
    "href": "get_started.html",
    "title": "Get started",
    "section": "",
    "text": "GLOSSA is a user-friendly R shiny app designed to help you model marine species distributions using species occurrences and environmental data. Below is a quick guide to help you get started.\n\n\nBefore you begin, ensure you have R version 4.0.0 or higher installed on your system:\n\n\n\n\n\n\nDownload R\n\n\n\n\n\n\nTo install GLOSSA, simply run the following command in your R console:\ninstall.packages(\"glossa\")\nAlternatively, you can install the latest development version from GitHub to get the latest updates and fixes:\nif (!require(\"devtools\")) \n  install.packages(\"devtools\")\n\ndevtools::install_github(\"iMARES-group/glossa\")\nAfter installation, you can launch the GLOSSA app by running this command:\nlibrary(glossa)\nrun_glossa()\nYou can find more information on the Installation page."
  },
  {
    "objectID": "get_started.html#welcome-to-glossa",
    "href": "get_started.html#welcome-to-glossa",
    "title": "Get started",
    "section": "",
    "text": "GLOSSA is a user-friendly R shiny app designed to help you model marine species distributions using species occurrences and environmental data. Below is a quick guide to help you get started.\n\n\nBefore you begin, ensure you have R version 4.0.0 or higher installed on your system:\n\n\n\n\n\n\nDownload R\n\n\n\n\n\n\nTo install GLOSSA, simply run the following command in your R console:\ninstall.packages(\"glossa\")\nAlternatively, you can install the latest development version from GitHub to get the latest updates and fixes:\nif (!require(\"devtools\")) \n  install.packages(\"devtools\")\n\ndevtools::install_github(\"iMARES-group/glossa\")\nAfter installation, you can launch the GLOSSA app by running this command:\nlibrary(glossa)\nrun_glossa()\nYou can find more information on the Installation page."
  },
  {
    "objectID": "get_started.html#glossa-workflow-overview",
    "href": "get_started.html#glossa-workflow-overview",
    "title": "Get started",
    "section": "GLOSSA workflow overview",
    "text": "GLOSSA workflow overview\nGLOSSA is designed to be intuitive, with a simple step-by-step workflow:\n\nData input:\n\n\nUpload species occurrence data, either presence-absence or presence-only data (in which case, pseudo-absences will be generated).\nUpload environmental data, for example, climate variables in raster format.\nOptionally, provide projection layers, for example, to project future scenarios.\nOptionnally, provide a study area polygon to define the study area.\n\n\nData processing:\n\n\nClean coordinates: GLOSSA automatically removes duplicates and points outside the study area.\nProcess layers: Environmental layers are cropped and Z-score standardized if needed.\nGenerate pseudo-absences: For presence-only data, pseudo-absences are randomly generated.\n\n\nModel fitting and prediction:\n\n\nGLOSSA fits the BART (Bayesian Additive Regression Trees) model to predict species distributions. Two models can be fitted—one based on environmental data and another including spatial smoothing.\nYou can explore model outputs, including probability maps, performance metrics, and variable importance.\n\n\nResults and visualization:\n\n\nExplore interactive maps and charts showing predictions and analysis results.\nExport results for further use or reporting.\n\n\n\n\nGLOSSA workflow overview."
  },
  {
    "objectID": "get_started.html#running-your-first-analysis",
    "href": "get_started.html#running-your-first-analysis",
    "title": "Get started",
    "section": "Running your first analysis",
    "text": "Running your first analysis\nHere’s a quick guide on how to use the app. First, launch the app by running the run_glossa() function:\nlibrary(glossa)\nrun_glossa()\nThis will open the app in your web browser directly in the Home tab. Here, you can start a new analysis, watch a demo, or read tutorials.\n\n\n\nScreenshot of the “Home” tab, the landing page.\n\n\nClicking on the question mark icon  on the top right corner will bring up a brief explanation of what each tab and button does.\n\n\n\n\n\n\nSidebar overview\nOn the sidebar, you’ll find two main sections: Modelling and Resources.\n\nResources: This section provides essential information to help you run the app. It’s a great place to refresh your knowledge, especially if it’s been a while since you last used GLOSSA. Note that the full documentation is hosted outside the app in this website.\nModelling: This section includes the New Analysis, Reports, and Exports tabs. Typically, you’ll go through these tabs in sequence:\n\nNew Analysis: Set up your analysis, upload your data, and select analysis options.\nReports: Explore the results and visualizations generated by the analysis.\nExports: Download the results for further use."
  },
  {
    "objectID": "get_started.html#running-your-first-analysis-1",
    "href": "get_started.html#running-your-first-analysis-1",
    "title": "Get started",
    "section": "Running your first analysis",
    "text": "Running your first analysis\n\n\n\n\n\n\nBefore starting, we will download a toy dataset to use as a demo for running our first analysis with GLOSSA. The dataset can be downloaded from Zenodo: https://doi.org/10.5281/zenodo.14223866.\nThis repository includes occurrences for two species: one globally distributed (sp1.txt) and another endemic to the continental shelf of Australia (sp2.csv). The dataset also contains four predictor variables: two continuous and two categorical (fit_layers.zip). Additionally, it includes files representing two projected climate scenarios (project_layers_1.zip and project_layers_2.zip). Finally, the dataset provides two polygon files to define the extent of the study area: a global polygon object (world.gpkg) and a polygon delimiting the Australian region (oceania.gpkg).\n\n\n\nTo run your first analysis, go to the New Analysis tab. This tab looks like this:\n\n\n\n\n\nHere, you need to upload the required input files and select your analysis options:\n\nData upload: The first panel is where you upload your data and configure the analysis settings.\nPrevisualization: The second panel provides an interactive map to preview your input data.\nPredictor variables: Choose predictor variables for each uploaded species.\nUploaded files: A table indicates if your input files are formatted correctly.\n\n\nRequired files for analysis\nGLOSSA can function with just occurrence data and environmental variables, but additional options are available. Let’s briefly go through the necessary files:\n\nOccurrences: Upload a tab-separated CSV file with four columns indicating the occurrence location, whether it is a presence or absence, and the time it was recorded. The columns must be named exactly as follows: decimalLongitude, decimalLatitude, pa, and timestamp. If the pa column is missing, GLOSSA will assume all rows are presences. If timestamp is missing, GLOSSA will assume all observations occurred at the same time. GLOSSA also supports presence-only data but will generate randomly distributed balanced pseudo-absences to fit the model.\n&gt; head(sp1)\n  decimalLongitude decimalLatitude timestamp pa\n1          5.42909        43.20937         1  1\n2           -43.05           49.03         1  0\n3         -2.52369        47.29234         2  1\n4           34.054         -26.913         2  1\n5           -41.63            46.3         2  0\n6           -174.5            27.5         3  1\nEnvironmental data: Upload environmental data as raster files (e.g., .tif or .nc format) in a ZIP file with a specific structure. The ZIP file should contain a subdirectory for each environmental variable, with files sorted by time period. For example, if you have two variables (x1 and x2) and your observations are from two different years, your ZIP file should look like this:\nfit_layers.zip\n    ├───x1\n    │       x1_1.tif\n    │       x1_2.tif\n    └───x2\n            x2_1.tif\n            x2_2.tif\nEnsure that all layers have the same resolution, the same number of layers, and that they match the number of years in your occurrence files. If you want to use the same layer for all observations, include just one file in the subdirectory and set all timestamp values to 1 or remove the timestamp column.\nProjection layers (Optional): If you want to make predictions, upload your projection data here. This file has the same format as the environmental data file, and the subdirectories must match those used for fitting the model. You can upload multiple ZIP files if you want to predict multiple scenarios (e.g., different temperature increase scenarios).\nStudy area (Optional): If your rasters cover a larger area than your study region, you can provide a polygon to delimit your study area (formats: GPKG, KML or GeoJSON). This will remove points outside the polygon and mask the environmental variables accordingly.\n\nOnce all files are uploaded, if they pass the checks and are properly formatted, the table in the bottom right panel will show a checkmark for each file. If something is incorrect, refer to the documentation for a quick solution.\n\n\n\n\n\n\nIf you have downloaded the toy dataset, you can try uploading the following files: the two species occurrence files (sp1.txt and sp2.csv), the environmental variables for fitting the model (fit_layers.zip), the two climate scenarios for making projections (project_layers_1.zip and project_layers_2.zip), and the polygon file for the Australian region (oceania.gpkg) to delimit the study area and speed up computations for this initial attempt.\n\n\n\n\n\nAnalysis options\nIn this section, you need to select the desired output. GLOSSA fits two kinds of models:\n\nNative range: The model includes environmental variables and uses longitude and latitude coordinates as a spatial smoother.\nSuitable habitat: The model only includes the environmental variables.\n\nYou can choose to fit the model under the Model fitting option. This option will fit the model, compute variable importance, and generate a prediction map representing the mean environmental conditions across all provided layers. If you’ve uploaded projection layers and want to make predictions using the fitted model, select the Model projection option.\nWhen fitting the model, if multiple years or time periods are uploaded, GLOSSA will extract the value of the corresponding environmental layer for each occurrence based on the specific time stamp.\nAdditionally, you can check the Functional responses checkbox if you want to compute the response curves (i.e., the relationship between the occurrence of a species and each environmental variable). You can also enable Cross-validation, which will perform a K-fold cross-validation with (k = 10).\n\n\nAdvanced options\nBy selecting the Advanced options button, a sidebar will appear with extra options for refining your analysis:\n\nOccurrences thinning: Specify the number of decimal places to round coordinates, allowing you to apply spatial thinning to your occurrence data using a precision-based method. GLOSSA currently implements this method via the GeoThinneR R package, which is the most time- and memory-efficient option for large datasets. If you need to perform spatial thinning based on distance or a grid, you can do so before uploading your data to GLOSSA using GeoThinneR or other methods.\nStandardize covariates: GLOSSA uses a scaling method that subtracts the mean and divides by the standard deviation for standardization. The mean and standard deviation are calculated from the fitting layers, and the same values are used to standardize the projection layers, ensuring consistency across variables.\nEnlarge polygon: If your polygon has low resolution, you can apply a buffer in degrees to expand it. This is useful if you have points near the coast that fall outside the polygon due to poor resolution. You can preview the buffer using the “play” icon before running the analysis to find the optimal value.\nModel: Choose the model to apply. Currently, GLOSSA only supports the BART model, so no additional selection is needed here.\nSet a seed: Specify a seed for reproducibility of your results.\n\n\n\n\n\n\n\n\nPredictor variables\nIf you uploaded multiple species, you can select different predictor variables for each species in the bottom left panel.\n\n\n\n\n\n\n\n\n\n\n\nTry selecting different analysis options, tuning parameters, and choosing different combinations of predictor variables in different GLOSSA runs to explore how these changes affect the results of your analysis. Additionally, consider changing the seed value. Since the provided toy dataset contains presence-only data, different seeds will generate different random pseudo-absences, which can impact model performance.\nThis exercise is intended to help you get comfortable with the GLOSSA environment and its analysis options. By trying different parameters, predictor variables, and seeds, you can explore how these changes influence the results and gain confidence in using GLOSSA effectively.\n\n\n\n\n\nUploaded files\nIn the table in the bottom right corner, ensure that all files are checked and that you’ve selected your analysis options.\n\n\n\n\n\nOnce everything is set, you’re ready to run the analysis. Click the Run Job button, confirm in the dialog, and wait for the analysis to complete."
  },
  {
    "objectID": "get_started.html#analysis-results",
    "href": "get_started.html#analysis-results",
    "title": "Get started",
    "section": "Analysis results",
    "text": "Analysis results\nOnce the analysis is complete, you will be redirected to the Reports tab, where you can explore all the results and export visualizations .\n\n\n\n\n\nIn the top left corner of the tab, you can select the species for which you want to view the results. The first row displays key metrics:\n\nPotential suitable area: Calculated in square kilometers, based on the predicted presence-absence grid cells.\nMean suitable probability: The average probability of suitable habitat across the entire prediction area.\nPresences/Absences: The number of presence and absence points used to fit the model after all processing and cleaning.\n\nIf multiple projection layers are provided (for example, a time series), a sparkline plot will display the values for each time period, and the text value shown will represent the last one in the time series.\n\nGLOSSA predictions\nThe first plot, titled GLOSSA predictions, shows the presence probability predictions within the study area. As we are working in the Bayesian framework, each grid cell has associated one predictive posterior distribution, therefore we can obtain a more comprenhensive undertanding of the predictions by exploring metrics like the mean, median or quantiles. Using the three-dot icon , you can open the sidebar to customize the display:\n\nChoose between predictions on the fitting layers or the projection layers.\nToggle between viewing the native range or the suitable habitat.\nSelect which value from the posterior distribution to display (e.g., mean probability, median, etc.).\n\n\n\n\n\n\n\n\nEnvironmental variables and Presence validation\nThe plot on the right shows the environmental variables used to fit the model, allowing you to quickly compare them with the probability projections. Below this, another plot displays the occurrence points that were retained or filtered out during the analysis.\n\n\n\n\n\n\n\nFunctional responses and Variable importance\nIn the last row, you can view:\n\nFunctional response: These illustrate the relationship between each environmental variable and the predicted suitable habitat (response curves).\nVariable importance: Displays the importance of each variable for both the native range and the suitable habitat.\n\n\n\n\n\n\n\n\nCross-validation\nIn the cross-validation panel, you’ll find performance metrics such as:\n\nAIC (Akaike Information Criterion)\nRMSE (Root Mean Square Error)\n5-Fold Cross-Validation Results"
  },
  {
    "objectID": "get_started.html#exports",
    "href": "get_started.html#exports",
    "title": "Get started",
    "section": "Exports",
    "text": "Exports\nIn the Exports tab, you can export the results of your analysis. This includes all projection maps, the data used to fit the model, variable importance metrics, cross-validation results, etc. You can export almost everything, enabling you to explore the results further or create your own visualizations for your work!"
  },
  {
    "objectID": "get_started.html#next-steps",
    "href": "get_started.html#next-steps",
    "title": "Get started",
    "section": "Next Steps",
    "text": "Next Steps\nThis Quick start guide for GLOSSA should help you get up and running. For more detailed instructions, check out the Tutorial tab, where you’ll find tutorials on how to prepare your data for GLOSSA or even worked examples that guide you through using GLOSSA from end-to-end.\nAdditionally, the Documentation tab offers a comprehensive guide to every aspect of the GLOSSA app. Here, you’ll find not only in-depth information on how GLOSSA works but also tips and tricks to help you get the most out of the app.\nIf you have further questions, visit the FAQs tab, or if you still need help, you can reach us through the Contact Us tab.\nThank you for using GLOSSA, and enjoy your species distribution modeling!"
  },
  {
    "objectID": "pages/documentation/explore_results.html",
    "href": "pages/documentation/explore_results.html",
    "title": "Explore the results",
    "section": "",
    "text": "After running your analysis in GLOSSA, the Reports tab presents several outputs and visualizations to help you interpret your species distribution model. This section walks you through each component of the results panel and explains how to explore and understand your model’s predictions, performance, and other key metrics.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Explore the results"
    ]
  },
  {
    "objectID": "pages/documentation/explore_results.html#select-species-or-occurrence-file",
    "href": "pages/documentation/explore_results.html#select-species-or-occurrence-file",
    "title": "Explore the results",
    "section": "Select species or occurrence file",
    "text": "Select species or occurrence file\nIf you’ve uploaded multiple species occurrence files, you can select which species’ results to view using the dropdown menu in the top-left corner. This will update all the plots, so they represent the predictions, validation metrics, and other outputs for the selected species.\nFor most of the visualizations, you can customize the view using the three dots icon . This allows you to change how the data is displayed, select between the native range and the suitable habitat model, select different projection layers, or explore various aspects of the model results in more detail.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Explore the results"
    ]
  },
  {
    "objectID": "pages/documentation/explore_results.html#key-metrics-in-the-first-row",
    "href": "pages/documentation/explore_results.html#key-metrics-in-the-first-row",
    "title": "Explore the results",
    "section": "Key metrics in the first row",
    "text": "Key metrics in the first row\nAt the top of the results panel, you’ll find key metrics summarizing the model’s predictions. First, there’s the species selector, which we discussed above. Next, you’ll see sparklines representing potential suitability for the selected projection scenario in the GLOSSA Predictions box.\n\nPotential suitable area (km2): This metric represents the total area (in square kilometers) predicted to be suitable for the species, calculated based on predicted presence/absence using the computed optimal cutoff.\nMean suitable probability: This value represents the average of all grid cells in the study area. For each grid cell, as we have a posterior predictive distribution, we used the mean of each grid cell to calculate the mean of the whole raster.\n\nThese two metrics help you inspect trends in suitable habitat over time. The big value represent the value of the last year, the sparkline shows year-by-year changes, and the displayed percentage indicates the change between the first 5% and the last 5% of the projection period. For instance, if your projection covers 100 years, the percentage shows the difference between the first five years and the last five years.\n\nPresences/Absences: This box tells you how many presence and absence (or pseudo-absence) records were used to fit the model, providing valuable information about the sample size.\n\nIn the example below, the habitat suitability has a decreasing trend, with around a 10% reduction from the beginning of the projection period. The model was fitted using a total of 800 points.\n\n\n\n\n\nThese metrics give you a quick overview of your model’s predictions and the data used for fitting.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Explore the results"
    ]
  },
  {
    "objectID": "pages/documentation/explore_results.html#explore-input-data",
    "href": "pages/documentation/explore_results.html#explore-input-data",
    "title": "Explore the results",
    "section": "Explore input data",
    "text": "Explore input data\nOn the two panels on the right we can explore the environmental variables used to fit the model and the occurrences that were kept and discarded (duplicates, thinning, missing values, etc.). These two plots are very useful because you can explore patterns in the environmental variables that could be related to patterns in species occurrences and predictions comparing with the plot on the left that we will discuss next. The environmental variables are represented in the scale they were used to fit the model, so if you choose to standardize you’ll see the standardized covariates. And the plot of the occurrences allows you to know which records has been filtered by GLOSSA.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Explore the results"
    ]
  },
  {
    "objectID": "pages/documentation/explore_results.html#glossa-projections",
    "href": "pages/documentation/explore_results.html#glossa-projections",
    "title": "Explore the results",
    "section": "GLOSSA projections",
    "text": "GLOSSA projections\nThis section provides a map showing the predicted species distribution across the study area. The heat map uses color gradients to represent the probability of species presence, where warmer colors indicate higher probabilities, and cooler colors indicate lower probabilities or unsuitable habitats. You can adjust the view to show different projection layers, timestamps, and metrics from the model’s posterior distribution (e.g., mean probability, median, or quantiles).\nUsing the three-dot icon , you can open the sidebar to customize the display:\n\nChoose between predictions based on fitting layers or projection layers.\nToggle between viewing the native range or the suitable habitat predictions.\nSelect which value from the posterior distribution to display (e.g., mean probability, median, etc.).\nOverlay points used to fit the model to visualize how occurrence data aligns with the predictions.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nChanging the projection scenario in this panel will also update the sparklines at the top of the results panel, reflecting the trend in habitat suitability for that scenario.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Explore the results"
    ]
  },
  {
    "objectID": "pages/documentation/explore_results.html#additional-results",
    "href": "pages/documentation/explore_results.html#additional-results",
    "title": "Explore the results",
    "section": "Additional results",
    "text": "Additional results\nIn the next row of the results panel, you can view the additional analyses that you requested in GLOSSA:\n\nFunctional responses: Shows the functional responses for each environmental variable, showing how changes in each variable affect the probability of species presence. These curves are partial dependence plots that provide insights into the environmental factors driving species distributions.\nVariable importance: The variable importance plot shows how much each environmental variable influences the model’s predictions. GLOSSA uses a permutation-based approach to measure the effect of shuffling each variable’s values on the model’s prediction accuracy. Variables with high importance scores play a larger role in predicting species presence, helping you identify the key drivers of habitat suitability.\nCross-Validation: If you enabled cross-validation, this panel shows the performance metrics from k-fold cross-validation. The radar plot displays metrics such as precision, sensitivity, specificity, false discovery rate, F-score, accuracy, and TSS (True Skill Statistic).",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Explore the results"
    ]
  },
  {
    "objectID": "pages/documentation/explore_results.html#model-summary",
    "href": "pages/documentation/explore_results.html#model-summary",
    "title": "Explore the results",
    "section": "Model summary",
    "text": "Model summary\nThe model summary panel provides key validation metrics for the model fitting:\n\nROC curve: This plot evaluates the model’s ability to distinguish between species presence and absence across different probability thresholds, showing the AUC (Area Under the Curve) as an overall measure of accuracy.\nConfusion matrix plot: This plot breaks down true positives, false positives, true negatives, and false negatives, and shows their predicted probability with the computed optimal cutoff. classification performance.\nDistribution of fitted values: This histogram shows the distribution of predicted probabilities across the study area.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Explore the results"
    ]
  },
  {
    "objectID": "pages/documentation/explore_results.html#exporting-plots",
    "href": "pages/documentation/explore_results.html#exporting-plots",
    "title": "Explore the results",
    "section": "Exporting plots",
    "text": "Exporting plots\nOnce you’re satisfied with your results, you can use the download icon  to export any plot or visualization. Export formats include PNG, JPG, SVG, and PGF. You can also adjust the height and width of the exported plot.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Explore the results"
    ]
  },
  {
    "objectID": "pages/documentation/explore_results.html#conclusion",
    "href": "pages/documentation/explore_results.html#conclusion",
    "title": "Explore the results",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve now learned how to navigate the GLOSSA report panel to explore key metrics, validate model performance, and gain insights into the factors driving species distributions. The next step is to export your results, save the projection rasters, the data used for model fitting, and other analysis outputs: Exporting results.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?",
      "Explore the results"
    ]
  },
  {
    "objectID": "pages/documentation/index.html",
    "href": "pages/documentation/index.html",
    "title": "GLOSSA documentation guide",
    "section": "",
    "text": "Welcome to the GLOSSA documentation! Here, you’ll find everything you need to understand, use, and get the most out of GLOSSA for species distribution modeling. Whether you’re new to modeling or already experienced, we’ve got guides, examples, and detailed explanations to help you.\nIf you’d like to contribute, check out our contribution guidelines. For questions, feel free to reach out via the contact form on the iMARES website.",
    "crumbs": [
      "Documentation",
      "GLOSSA documentation guide"
    ]
  },
  {
    "objectID": "pages/documentation/index.html#new-to-glossa",
    "href": "pages/documentation/index.html#new-to-glossa",
    "title": "GLOSSA documentation guide",
    "section": "New to GLOSSA?",
    "text": "New to GLOSSA?\nStart with our how-to guides, which provide simple, step-by-step instructions for common tasks:\n\nWhat is GLOSSA?\nInstalling and setting up GLOSSA\nPreparing your data\nRunning a new analysis\nExplore the results\nExporting results",
    "crumbs": [
      "Documentation",
      "GLOSSA documentation guide"
    ]
  },
  {
    "objectID": "pages/documentation/index.html#tutorials-and-examples",
    "href": "pages/documentation/index.html#tutorials-and-examples",
    "title": "GLOSSA documentation guide",
    "section": "Tutorials and examples",
    "text": "Tutorials and examples\nExplore real-world examples that guide you through complete analyses, from start to finish. You’ll also find specific tutorials on how to source and prepare your data for modeling:\n\nDownload and prepare occurrence data\nDownload and prepare environmental data\nGlobal species distribution model\nUsing a polygon to define the study area\nRunning GLOSSA on a regional scale",
    "crumbs": [
      "Documentation",
      "GLOSSA documentation guide"
    ]
  },
  {
    "objectID": "pages/documentation/index.html#explanations-and-reference-docs",
    "href": "pages/documentation/index.html#explanations-and-reference-docs",
    "title": "GLOSSA documentation guide",
    "section": "Explanations and reference docs",
    "text": "Explanations and reference docs\nWant to know what’s happening behind the scenes? Here’s where you can dive deeper into the concepts, like how GLOSSA’s BART model works, how we generate pseudo-absences or detailed technical information about data formats and specific settings:\n\nHow does GLOSSA work?\nWhat is Bayesian Additive Regression Trees (BART)?\nGenerating pseudo-absences for presence-only data",
    "crumbs": [
      "Documentation",
      "GLOSSA documentation guide"
    ]
  },
  {
    "objectID": "pages/documentation/index.html#common-issues-and-troubleshooting",
    "href": "pages/documentation/index.html#common-issues-and-troubleshooting",
    "title": "GLOSSA documentation guide",
    "section": "Common issues and troubleshooting",
    "text": "Common issues and troubleshooting\nFind solutions to common issues, such as installation problems, data formatting errors, and model performance concerns:\n\nTroubleshooting\nFAQs",
    "crumbs": [
      "Documentation",
      "GLOSSA documentation guide"
    ]
  },
  {
    "objectID": "pages/documentation/index.html#contributing-to-glossa",
    "href": "pages/documentation/index.html#contributing-to-glossa",
    "title": "GLOSSA documentation guide",
    "section": "Contributing to GLOSSA",
    "text": "Contributing to GLOSSA\nInterested in contributing to GLOSSA? Learn how to get involved and what kinds of contributions we’re looking for:\n\nContribution guidelines",
    "crumbs": [
      "Documentation",
      "GLOSSA documentation guide"
    ]
  },
  {
    "objectID": "pages/documentation/index.html#glossa-resources",
    "href": "pages/documentation/index.html#glossa-resources",
    "title": "GLOSSA documentation guide",
    "section": "GLOSSA resources",
    "text": "GLOSSA resources\nHere are some useful links to access GLOSSA resources:\n\nCRAN repository\nGitHub repository\niMARES website\nIssues and feature requests",
    "crumbs": [
      "Documentation",
      "GLOSSA documentation guide"
    ]
  },
  {
    "objectID": "pages/documentation/pseudo_absences.html",
    "href": "pages/documentation/pseudo_absences.html",
    "title": "Generating pseudo-absences for presence-only data",
    "section": "",
    "text": "When dealing with presence-only species distribution data, two main approaches are commonly used to handle the absence of true absence points. The first approach is to model the presence points directly as a point process, which treats presences as an observed spatial pattern. This approach doesn’t require absence data and instead estimates the intensity of occurrences across the study area. The second approach is to generate pseudo-absence points, creating false absences across the study region, allowing the model to learn from contrasting presence and absence locations.\nIn GLOSSA, we take the second approach as we fit a Bayesian Additive Regression Trees (BART) model using a binary response variable (presence = 1, absence = 0) with a probit link function. Generating pseudo-absences allows the model to estimate the probability of species presence by learning from both the observed presences and the generated pseudo-absences.",
    "crumbs": [
      "Documentation",
      "Explanations and reference docs"
    ]
  },
  {
    "objectID": "pages/documentation/pseudo_absences.html#the-challenge-of-generating-pseudo-absences",
    "href": "pages/documentation/pseudo_absences.html#the-challenge-of-generating-pseudo-absences",
    "title": "Generating pseudo-absences for presence-only data",
    "section": "The challenge of generating pseudo-absences",
    "text": "The challenge of generating pseudo-absences\nGenerating pseudo-absences for presence-only data is a critical decision in species distribution modeling. There exist multiple methods for generating pseudo-absences, each with its own strengths and limitations, and there is currently no consensus on a standard approach. Some common methods include random sampling, geographic exclusion (e.g., excluding areas close to presence points), and environmental filtering (e.g., selecting pseudo-absences in contrasting environmental conditions).\nEach method can influence model outcomes differently, depending on the study area, species, and modeling objectives. Given the absence of a definitive standard, GLOSSA uses a balanced random sampling approach that provides broad applicability and ease of use across diverse datasets.",
    "crumbs": [
      "Documentation",
      "Explanations and reference docs"
    ]
  },
  {
    "objectID": "pages/documentation/pseudo_absences.html#pseudo-absence-generation-with-glossa",
    "href": "pages/documentation/pseudo_absences.html#pseudo-absence-generation-with-glossa",
    "title": "Generating pseudo-absences for presence-only data",
    "section": "Pseudo-absence generation with GLOSSA",
    "text": "Pseudo-absence generation with GLOSSA\nWhen you upload an occurrence file with presence-only data (the pa column just includes ones or is missing), GLOSSA uses a balanced random sampling approach for generating pseudo-absences, that is, GLOSSA generates pseudo-absence points randomly across the defined study area, ensuring that the number of pseudo-absence points matches the number of presence points.\n\n\n\n\n\n\nNote\n\n\n\nThe spatial extent within which pseudo-absences are generated is an essential factor.\n\n\nWhen the presence data includes several timestamps, GLOSSA generates the same number of pseudo-absences as presences for each time period. This approach ensures that the model captures temporal variation in species occurrence, which can be important for species with temporal patterns.\n\n\n\n\n\n\nNote\n\n\n\nIf you prefer an alternative method for generating pseudo-absences, you can create them outside of GLOSSA using your preferred method and include them in your occurrence putting a 0 in the pa column of these records.",
    "crumbs": [
      "Documentation",
      "Explanations and reference docs"
    ]
  },
  {
    "objectID": "pages/documentation/what_is_glossa.html",
    "href": "pages/documentation/what_is_glossa.html",
    "title": "What is GLOSSA?",
    "section": "",
    "text": "GLOSSA is a simple, interactive R Shiny app designed to help you model species distributions without needing to code. Using Bayesian Additive Regression Trees (BART), GLOSSA predicts species habitats based on presence and environmental data, allowing you to visualize both current and future distributions.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?"
    ]
  },
  {
    "objectID": "pages/documentation/what_is_glossa.html#how-glossa-works",
    "href": "pages/documentation/what_is_glossa.html#how-glossa-works",
    "title": "What is GLOSSA?",
    "section": "How GLOSSA works?",
    "text": "How GLOSSA works?\nGLOSSA’s workflow is designed to be intuitive, with four main steps:\n\n1. Data upload\n\nSpecies occurrence: Upload your species data with either presence/absence or presence-only records. GLOSSA can work with multiple species in one session, generating pseudo-absences when needed.\nEnvironmental data: Upload environmental rasters (e.g., from sources like Bio-ORACLE) matched to the occurrence points by location and time.\nProjections: Include layers representing future time periods or climate scenarios to generate predictions for each.\nStudy area: Upload a polygon to define your study region, or let GLOSSA use the extent of your environmental data.\n\n\n\n2. Data processing\n\nCoordinate cleaning: GLOSSA automatically removes duplicate records, filters out invalid points, and applies spatial thinning if needed.\nLayer processing: Environmental rasters are cropped, masked, and can be standardized.\nGenerate pseudo-absences: For presence-only data, GLOSSA creates randomly distributed pseudo-absences across the study area.\n\n\n\n3. Model fitting and prediction\n\nFit BART model: GLOSSA fits a BART model, generating predictions for suitable habitats and native ranges, and computes an optimal threshold for presence/absence classification.\nModel output: The output includes metrics like AUC, ROC curve, confusion matrix, functional response curves, and variable importance.\nProjections: Projections can be made to different areas, time periods, and climate scenarios.\n\n\n\n4. Visualization and export\nAfter the analysis, GLOSSA provides interactive maps and charts to explore model results, allowing you to export data and visuals for further use.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?"
    ]
  },
  {
    "objectID": "pages/documentation/what_is_glossa.html#why-use-glossa",
    "href": "pages/documentation/what_is_glossa.html#why-use-glossa",
    "title": "What is GLOSSA?",
    "section": "Why use GLOSSA?",
    "text": "Why use GLOSSA?\nGLOSSA simplifies species distribution modeling by:\n\nMaking modeling accessible with a no-code interface.\nOffering robust machine learning predictions with BART.\nSupporting independent multi-species and scenario-based analysis in a single run.\nProviding detailed visualizations and export options for further analysis or publication.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?"
    ]
  },
  {
    "objectID": "pages/documentation/what_is_glossa.html#ready-to-get-started",
    "href": "pages/documentation/what_is_glossa.html#ready-to-get-started",
    "title": "What is GLOSSA?",
    "section": "Ready to get started?",
    "text": "Ready to get started?\nLearn how to install and set up GLOSSA: Installation guide.",
    "crumbs": [
      "Documentation",
      "New to GLOSSA?"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html",
    "title": "Example 2",
    "section": "",
    "text": "This vignette provides a detailed example of fitting a single-species distribution model within a user-defined region. We will walk through the steps of obtaining occurrence data, gathering environmental data, and defining the study area using a polygon. The example focuses on the distribution of the loggerhead sea turtle (Caretta caretta) in the Mediterranean Sea. Occurrence data will be retrieved from GBIF for the years 2000 to 2019, and environmental data from Bio-ORACLE v3.0 (Assis et al., 2024). We will use GLOSSA to fit a species distribution model and project it under different future climate scenarios.\nTo get started, we load the glossa package, as well as terra (Hijmans, 2024) and sf (Pebesma, 2018) to work with spatial rasters and vector data. We also load rgbif (Chamberlain, 2017) and biooracler (Fernandez Bejarano and Salazar, 2024) for downloading species occurrences and environmental data, respectively. Additionally, the dplyr (Wickham et al., 2023) package will be used for data manipulation.\n\nlibrary(glossa)\nlibrary(terra)\nlibrary(sf)\nlibrary(rgbif)\nlibrary(biooracler)\nlibrary(dplyr)",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Using a polygon to define the study area"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#introduction",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#introduction",
    "title": "Example 2",
    "section": "",
    "text": "This vignette provides a detailed example of fitting a single-species distribution model within a user-defined region. We will walk through the steps of obtaining occurrence data, gathering environmental data, and defining the study area using a polygon. The example focuses on the distribution of the loggerhead sea turtle (Caretta caretta) in the Mediterranean Sea. Occurrence data will be retrieved from GBIF for the years 2000 to 2019, and environmental data from Bio-ORACLE v3.0 (Assis et al., 2024). We will use GLOSSA to fit a species distribution model and project it under different future climate scenarios.\nTo get started, we load the glossa package, as well as terra (Hijmans, 2024) and sf (Pebesma, 2018) to work with spatial rasters and vector data. We also load rgbif (Chamberlain, 2017) and biooracler (Fernandez Bejarano and Salazar, 2024) for downloading species occurrences and environmental data, respectively. Additionally, the dplyr (Wickham et al., 2023) package will be used for data manipulation.\n\nlibrary(glossa)\nlibrary(terra)\nlibrary(sf)\nlibrary(rgbif)\nlibrary(biooracler)\nlibrary(dplyr)",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Using a polygon to define the study area"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#data-preparation",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#data-preparation",
    "title": "Example 2",
    "section": "Data preparation",
    "text": "Data preparation\n\nDownload occurrence data\nWe will download occurrence data for Caretta caretta in the Mediterranean Sea using the GBIF API. The data will be filtered by date and geographic boundaries to fit the Mediterranean region and align with the temporal scale of the environmental data from Bio-ORACLE. To use the GBIF API, you need to have a registered account (user, pwd, and email). First, we retrieve the taxon key for C. caretta, and then use occ_download() from the rgbif package to download the occurrences. To restrict the download to the Mediterranean Sea, we use the pred_within() argument and define a polygon of interest (\"POLYGON ((-10 28, 38 28, 38 51, -10 51, -10 28))\"). Since Bio-ORACLE provides environmental layers in decadal steps, we limit the occurrence data to points between 2000 and 2019.\n\n# Function to retrieve taxon key from scientific name\nget_taxon_key &lt;- function(name) {\n  result &lt;- rgbif::occ_search(scientificName = name, hasCoordinate = TRUE)\n  if (!is.null(result$data)) {\n    return(as.character(result$data$taxonKey[1]))\n  } else {\n    warning(paste(\"No taxon key found for\", name))\n    return(NA)\n  }\n}\n\n# Define GBIF credentials and species information\nuser &lt;- \"&lt;gbif username&gt;\"\npwd &lt;- \"&lt;password&gt;\"\nemail &lt;- \"&lt;gbif mail&gt;\"\ntaxon_key &lt;- get_taxon_key(\"Caretta caretta\")\n\n# Download GBIF occurrence data\nrequest_id &lt;- as.character(rgbif::occ_download(\n  rgbif::pred(\"taxonKey\", taxon_key),\n  rgbif::pred(\"hasCoordinate\", TRUE),\n  rgbif::pred_within(\"POLYGON ((-10 28, 38 28, 38 51, -10 51, -10 28))\"),\n  rgbif::pred(\"occurrenceStatus\", \"PRESENT\"),\n  rgbif::pred_gte(\"YEAR\", 2000),\n  rgbif::pred_lte(\"YEAR\", 2019),\n  format = \"SIMPLE_CSV\",\n  user = user,\n  pwd = pwd,\n  email = email\n))\n\nresponse &lt;- rgbif::occ_download_wait(request_id)\n\nif (response$status == \"SUCCEEDED\"){\n  temp &lt;- tempfile(fileext = \".csv\")\n  download.file(response$downloadLink, temp, mode = \"wb\")\n  caretta &lt;- read.csv(unz(temp, paste0(response$key, \".csv\")),\n                        header = TRUE, sep = \"\\t\", dec = \".\")\n}\n\nOnce we have the dataset, we prepare it to fit the GLOSSA format. For this, we extract the required record locations (decimalLongitude and decimalLatitude) and assign a timestamp for each record. As Bio-ORACLE provides environmental layers for two decades (2000s and 2010s), we assign points from 2000 to 2009 to the first time period and points from 2010 to 2019 to the second time period. We then create a presence/absence column (pa), setting all downloaded records as presences. Finally, we save the file as a tab-separated CSV and retrieve the DOI of the downloaded data from GBIF (GBIF.org, 2024).\n\n# Prepare the data for modeling\n# Separate timestamp in two decades to fit Bio-ORACLE format (2000s and 2010s)\ncaretta &lt;- data.frame(\n  decimalLongitude = caretta$decimalLongitude,\n  decimalLatitude = caretta$decimalLatitude,\n  timestamp = ifelse(caretta$year %in% 2000:2009, 1, 2),\n  pa = 1\n)\ncaretta &lt;- caretta[complete.cases(caretta),]\n\n# Save the data\ndir.create(\"data\")\nwrite.table(caretta, file = \"data/Caretta_caretta_occ.csv\", sep = \"\\t\",\n            dec = \".\", quote = FALSE, row.names = FALSE)\n\n# Get citation for the downloaded data\ncitation &lt;- rgbif::gbif_citation(request_id)$download\ncitation\n# GBIF Occurrence Download https://doi.org/10.15468/dl.es7562\n# Accessed from R via rgbif (https://github.com/ropensci/rgbif) on 2024-08-27\n\n\n\nDownload environmental data\nBio-ORACLE provides environmental layers at a spatial resolution of 0.05 degrees and in decadal steps. We will download environmental variables such as ocean surface temperature (thetao in \\(^{\\circ}\\)C), primary productivity (phyc in \\(\\text{mmol} \\cdot \\text{m}^{-3}\\)), and salinity (so). Bathymetry (bat) will be obtained from the ETOPO 2022 Global Relief Model by NOAA (https://www.ncei.noaa.gov/products/etopo-global-relief-model).\n\n# Define temporal and spatial constraints\ntime = c(\"2000-01-01T00:00:00Z\", \"2010-01-01T00:00:00Z\")\nlatitude = c(28, 51)\nlongitude = c(-10, 38)\nconstraints = list(time, longitude, latitude)\nnames(constraints) = c(\"time\", \"longitude\", \"latitude\")\n\n# Download environmental layers from Bio-Oracle\nthetao_hist &lt;- download_layers(\"thetao_baseline_2000_2019_depthsurf\", \n                               \"thetao_mean\", constraints)\nphyc_hist &lt;- download_layers(\"phyc_baseline_2000_2020_depthsurf\", \n                             \"phyc_mean\", constraints)\nso_hist &lt;- download_layers(\"so_baseline_2000_2019_depthsurf\", \n                           \"so_mean\", constraints)\n\n# Download bathymetry data\ndownload.file(\"https://www.ngdc.noaa.gov/thredds/fileServer/global/ETOPO2022/60s/60s_bed_elev_netcdf/ETOPO_2022_v1_60s_N90W180_bed.nc\",\n              destfile = \"data/ETOPO_2022_v1_60s_N90W180_bed.nc\")\n\nWe will prepare the downloaded environmental data and save it in the directory structure required by GLOSSA.\n\ndir.create(\"data/fit_layers\")\n\n# Prepare environmental variables for modeling\nenv_var &lt;- list(thetao_hist, so_hist, phyc_hist)\nnames(env_var) &lt;- c(\"thetao\", \"so\", \"phyc\")\n\nfor (i in seq_along(env_var)) {\n  var_dir &lt;- paste0(\"data/fit_layers/\", names(env_var)[i])\n  dir.create(var_dir)\n  \n  terra::writeRaster(\n    env_var[[i]][[1]], \n    filename = paste0(var_dir, \"/\", names(env_var)[i], \"_1.tif\")\n  )\n  terra::writeRaster(\n    env_var[[i]][[2]],\n    filename = paste0(var_dir, \"/\", names(env_var)[i], \"_2.tif\")\n  )\n  \n  # Clean up auxiliary files generated by terra - Optional\n  aux_files &lt;- list.files(var_dir, pattern = \"\\\\.aux\\\\.json$\", full.names = TRUE)\n  file.remove(aux_files)\n}\n\n# Prepare bathymetry data\nbat &lt;- terra::rast(\"data/ETOPO_2022_v1_60s_N90W180_bed.nc\")\nbat &lt;- -1*bat # Change sign from elvation to bathymetry\n\n# Aggregate to match resolution with Bio-ORACLE layers\nbat &lt;- terra::aggregate(bat, fact = 3, fun = \"mean\")\nr &lt;- terra::rast(terra::ext(env_var[[1]]), res = terra::res(env_var[[1]]))\nbat &lt;- terra::resample(bat, r)\n\ndir.create(\"data/fit_layers/bat\")\nfor (i in 1:2){\n  terra::writeRaster(bat, filename = paste0(\"data/fit_layers/bat/bat_\", i, \".tif\"))\n}\n\n# Zip all prepared layers\nzip(zipfile = \"data/fit_layers.zip\", files = \"data/fit_layers\")\n\n\n\nClimate projections: SSP1 2.6 and SSP5 8.5\nWe will now download climate projections for SSP1 2.6 (sustainable development scenario where global CO2 emissions are strongly reduced but less rapidly) and SSP5 8.5 (a high emissions scenario) scenarios, and prepare the data for model projection.\n\nSSP1 2.6\n\n# SSP1 2.6 projections constraints\ntime = c(\"2020-01-01T00:00:00Z\", \"2090-01-01T00:00:00Z\")\nlatitude = c(28, 51)\nlongitude = c(-10, 38)\nconstraints = list(time, longitude, latitude)\nnames(constraints) = c(\"time\", \"longitude\", \"latitude\")\n\nenv_var_ssp126 &lt;- list(\n  thetao_ssp126 = download_layers(\"thetao_ssp126_2020_2100_depthsurf\", \n                                  \"thetao_mean\", constraints),\n  so_ssp126 = download_layers(\"so_ssp126_2020_2100_depthsurf\", \n                              \"so_mean\", constraints),\n  phyc_ssp126 = download_layers(\"phyc_ssp126_2020_2100_depthsurf\", \n                                \"phyc_mean\", constraints)\n)\nnames(env_var_ssp126) &lt;- c(\"thetao\", \"so\", \"phyc\")\n\ndir.create(\"data/proj_ssp126\")\nfor (i in seq_along(env_var_ssp126)) {\n  var_dir &lt;- paste0(\"data/proj_ssp126/\", names(env_var_ssp126)[i])\n  dir.create(var_dir, showWarnings = FALSE)\n  \n  terra::writeRaster(\n    env_var_ssp126[[i]][[1]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp126)[i], \"_1.tif\")\n  )\n  terra::writeRaster(\n    env_var_ssp126[[i]][[4]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp126)[i], \"_2.tif\")\n  )\n  terra::writeRaster(\n    env_var_ssp126[[i]][[8]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp126)[i], \"_3.tif\")\n  )\n  \n  # Clean up auxiliary files\n  aux_files &lt;- list.files(var_dir, pattern = \"\\\\.aux\\\\.json$\", full.names = TRUE)\n  file.remove(aux_files)\n}\n\n\n# Prepare bathymetry data for SSP1 2.6\ndir.create(\"data/proj_ssp126/bat\", showWarnings = FALSE)\nfor (i in 1:3) {\n  terra::writeRaster(bat, filename = paste0(\"data/proj_ssp126/bat/bat_\", i, \".tif\"))\n}\n\n# Zip the data\nzip(zipfile = \"data/proj_ssp126.zip\", files = \"data/proj_ssp126\")\n\n\n\nSSP5 8.5\n\n# SSP5 8.5 projections constraints\ntime = c(\"2020-01-01T00:00:00Z\", \"2090-01-01T00:00:00Z\")\nlatitude = c(28, 51)\nlongitude = c(-10, 38)\nconstraints = list(time, longitude, latitude)\nnames(constraints) = c(\"time\", \"longitude\", \"latitude\")\n\nenv_var_ssp585 &lt;- list(\n  thetao_ssp585 = download_layers(\"thetao_ssp585_2020_2100_depthsurf\", \n                                  \"thetao_mean\", constraints),\n  so_ssp585 = download_layers(\"so_ssp585_2020_2100_depthsurf\", \n                              \"so_mean\", constraints),\n  phyc_ssp585 = download_layers(\"phyc_ssp585_2020_2100_depthsurf\", \n                                \"phyc_mean\", constraints)\n)\nnames(env_var_ssp585) &lt;- c(\"thetao\", \"so\", \"phyc\")\n\n\ndir.create(\"data/proj_ssp585\")\nfor (i in seq_along(env_var_ssp585)) {\n  var_dir &lt;- paste0(\"data/proj_ssp585/\", names(env_var_ssp585)[i])\n  dir.create(var_dir, showWarnings = FALSE)\n  \n  terra::writeRaster(\n    env_var_ssp585[[i]][[1]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp585)[i], \"_1.tif\")\n  )\n  terra::writeRaster(\n    env_var_ssp585[[i]][[4]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp585)[i], \"_2.tif\")\n  )\n  terra::writeRaster(\n    env_var_ssp585[[i]][[8]], \n    filename = paste0(var_dir, \"/\", names(env_var_ssp585)[i], \"_3.tif\")\n  )\n  \n  # Clean up auxiliary files\n  aux_files &lt;- list.files(var_dir, pattern = \"\\\\.aux\\\\.json$\", full.names = TRUE)\n  file.remove(aux_files)\n}\n\n# Prepare bathymetry data for SSP5 8.5\ndir.create(\"data/proj_ssp585/bat\", showWarnings = FALSE)\nfor (i in 1:3) {\n  terra::writeRaster(bat, filename = paste0(\"data/proj_ssp585/bat/bat_\", i, \".tif\"))\n}\n\n# Zip the data\nzip(zipfile = \"data/proj_ssp585.zip\", files = \"data/proj_ssp585\")\n\n\n\n\nStudy area polygon\nTo define the study area, we downloaded a polygon representing the Mediterranean Sea, which restricts the analysis to this region as the environmental layers extend beyond it. The shapefile was obtained from the Marine Regions repository. The specific download link for the Mediterranean Sea polygon can be found here.\n\n\n\nPolygon defining the study area for the Mediterranean Sea. This polygon restricts the analysis to the Mediterranean region, considering the extent of the environmental layers. Image obtained from the Marine Regions website.\n\n\n\ntmpdir &lt;- tempdir()\nzip_contents &lt;- utils::unzip(\"data/iho.zip\", unzip = getOption(\"unzip\"), exdir = tmpdir)\nmed_sea &lt;- list.files(tmpdir, pattern = \"\\\\.shp$\", full.names = TRUE) %&gt;% \n  sf::st_read() %&gt;% \n  sf::st_geometry() %&gt;% \n  sf::st_union() %&gt;%\n  sf::st_make_valid()\nmed_sea &lt;- sf::st_geometry(med_sea[[2]]) %&gt;% \n  sf::st_make_valid()\nsf::st_crs(med_sea) &lt;- \"epsg:4326\"\nsf::st_write(med_sea, \"data/mediterranean_sea.gpkg\")",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Using a polygon to define the study area"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#glossa-modeling",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#glossa-modeling",
    "title": "Example 2",
    "section": "GLOSSA modeling",
    "text": "GLOSSA modeling\nWith the data prepared and formatted for the GLOSSA framework, we use the glossa::run_glossa() function to launch the GLOSSA Shiny app for species distribution modeling and projection under different climate scenarios.\n\nrun_glossa()\n\nUpload the occurrence file for C. caretta and the environmental layers for model fitting and projection scenarios. For habitat suitability analysis, select the Model fitting and Model projection options from the Suitable habitat model. Enable Variable importance in the Others section to check the decrease of the F-score.\nIn the advanced options, set the following parameters: thinning precision to 2, standardize environmental data, and enlarge the polygon by 0.01 degrees to account for the lower resolution of the polygon and ensure the boundaries match properly -previously, many points near the coast were lost due to an insufficient buffer size-. Select the BART (Chipman, et al., 2010; Dorie, 2024) model and set the seed to 5648 for reproducibility.\n\n\n\nAnalysis options set in GLOSSA for modeling the distribution of Caretta caretta.",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Using a polygon to define the study area"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#results",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#results",
    "title": "Example 2",
    "section": "Results",
    "text": "Results\nOnce the analysis is completed, we observe occurrence records that were excluded due to being outside the study area or too close to other points (orange points in Figure 3), as we applied a thinning precision of 2. We are left with 5572 presence points and an equal number of pseudo-absences.\n\n\n\nValidation of presence points for Caretta caretta. Points are filtered based on their proximity and location relative to the study area, resulting in 5572 presence records (blue points).\n\n\nThe model summary indicates a decent predictive capacity with an AUC of 0.9 and an F-score of 0.8 The distribution of fitted values can also be explored. Note that the model was fitted using randomly generated pseudo-absences rather than real absences.\n\n\n\nSummary of the fitted model for Caretta caretta. From left to right: the ROC curve, a plot representing the confusion matrix with the associated probabilities, and the distribution on the fitted values.\n\n\nWe can also determine which predictors play a more significant role in predicting the outcome by examining the variable importance plot. This plot shows the decrease in model F-score using permutation feature importance, which measures the increase in prediction error after shuffling the predictor values. We see that the most important variable is the sea surface temperature followed by the bathymetry. Sea turtles live their entire lives in the ocean, but they migrate to nest on coastal land, with their sex being determined by the temperature during incubation (Mancino et al., 2022).\n\n\n\nVariable importance plot using the F-score as the performance metric\n\n\nFor the climate projections, there is a predicted decrease of 16.4% in suitable habitat area (\\(\\text{km}^2\\)) in the SSP1-2.6 scenario and about 78.2% in the SSP5-8.5 scenario from the 2020s to the 2090s. The western Mediterranean and the Adriatic Sea are projected to have relatively better habitat suitability.\n\n\n\nProjected changes in suitable habitat for Caretta caretta under different climate scenarios.",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Using a polygon to define the study area"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#conclusion",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#conclusion",
    "title": "Example 2",
    "section": "Conclusion",
    "text": "Conclusion\nThis example shows how GLOSSA can be used to model the distribution of Caretta caretta in the Mediterranean Sea. The steps included downloading occurrence data from GBIF, obtaining environmental layers from Bio-ORACLE, and preparing the data for model fitting and projections under various climate change scenarios.",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Using a polygon to define the study area"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#computation-time",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#computation-time",
    "title": "Example 2",
    "section": "Computation time",
    "text": "Computation time\nThe analysis was performed on a single Windows 11 machine equipped with 64 GB of RAM and an Intel(R) Core(TM) i7-1165G7 processor. This processor features 4 cores and 8 threads, with a base clock speed of 2.80 GHz. The following table summarizes the computation times for various stages of the GLOSSA analysis. This provides an overview of the computational resources required for each step in the analysis.\n\nTable 1. Computation times for different stages of the GLOSSA analysis for the loggerhead sea turtle (Caretta caretta) in the Mediterranean Sea.\n\n\n\n\nTask\nExecution Time\n\n\n\n\nLoading input data\n33.65 secs\n\n\nProcessing P/A coordinates\n0.21 secs\n\n\nProcessing covariate layers\n9.80 secs\n\n\nBuilding model matrix\n12.01 secs\n\n\nFitting native range models\n0.232 mins\n\n\nVariable importance (native range)\n8.31 mins\n\n\nP/A cutoff (native range)\n0.191 mins\n\n\nProjections on fit layers (native range)\n9.84 mins\n\n\nNative range projections\n9.47 mins\n\n\nNative range\n19.74 mins\n\n\nFitting suitable habitat models\n0.232 mins\n\n\nVariable importance (suitable habitat)\n8.31 mins\n\n\nP/A cutoff (suitable habitat)\n0.191 mins\n\n\nProjections on fit layers (suitable habitat)\n9.84 mins\n\n\nSuitable habitat projections\n9.47 mins\n\n\nHabitat suitability\n0.006 mins\n\n\nSuitable habitat\n19.74 mins\n\n\nComputing functional responses\n13.35 mins\n\n\nCross-validation\n4.19 mins\n\n\nModel summary\n0.18 mins\n\n\nTotal GLOSSA analysis\n38.39 mins",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Using a polygon to define the study area"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Caretta_caretta_example2.html#references",
    "href": "pages/tutorials_examples/Caretta_caretta_example2.html#references",
    "title": "Example 2",
    "section": "References",
    "text": "References\n\nAssis, J., Fernández Bejarano, S. J., Salazar, V. W., Schepers, L., Gouvea, L., Fragkopoulou, E., … & De Clerck, O. (2024). Bio‐ORACLE v3. 0. Pushing marine data layers to the CMIP6 Earth System Models of climate change research. Global Ecology and Biogeography, 33(4), e13813.\nChamberlain, S. (2017). rgbif: Interface to the Global Biodiversity Information Facility API. R package version 0.9.8. https://CRAN.R-project.org/package=rgbif\nChipman, H. A., George, E. I., & McCulloch, R. E. (2010). BART: Bayesian additive regression trees.\nDorie V (2024). dbarts: Discrete Bayesian Additive Regression Trees Sampler. R package version 0.9-28, https://CRAN.R-project.org/package=dbarts.\nFernandez-Bejarano, S. J. & Salazar, V. W. (2024). biooraclee: R package to access Bio-Oracle data via ERDDAP. R package version 0.0.0.9, https://github.com/bio-oracle/biooracler\nGBIF.org (26 August 2024) GBIF Occurrence Download https://doi.org/10.15468/dl.es7562\nHijmans, R. (2024). terra: Spatial Data Analysis. R package version 1.7-81, https://rspatial.github.io/terra/, https://rspatial.org/.\nMancino, C., Canestrelli, D., & Maiorano, L. (2022). Going west: Range expansion for loggerhead sea turtles in the Mediterranean Sea under climate change. Global Ecology and Conservation, 38, e02264.\nPebesma E (2018). Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal, 10(1), 439–446. doi:10.32614/RJ-2018-009.\nWickham H, François R, Henry L, Müller K & Vaughan D (2023). dplyr: A Grammar of Data Manipulation. R package version 1.1.4, https://github.com/tidyverse/dplyr, https://dplyr.tidyverse.org.",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Using a polygon to define the study area"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html",
    "title": "Example 3",
    "section": "",
    "text": "This example shows how to use GLOSSA to predict the suitable habitat of Siganus luridus in the Greek Seas and identify potential areas at risk of invasion. We achieve this by comparing the species’ native range with its projected suitable habitat. Occurrence data (2000-2020) is obtained from the GreekMarineICAS geodataset, created under the ALAS: Aliens in the Aegean – A Sea Under Siege project (https://alas.edu.gr/). Environmental data is obtained from the EU Copernicus Marine Environment Monitoring Service (https://marine.copernicus.eu/).\nWe start by loading the required R packages.\n\nlibrary(glossa)\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Running GLOSSA on a regional scale"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#introduction",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#introduction",
    "title": "Example 3",
    "section": "",
    "text": "This example shows how to use GLOSSA to predict the suitable habitat of Siganus luridus in the Greek Seas and identify potential areas at risk of invasion. We achieve this by comparing the species’ native range with its projected suitable habitat. Occurrence data (2000-2020) is obtained from the GreekMarineICAS geodataset, created under the ALAS: Aliens in the Aegean – A Sea Under Siege project (https://alas.edu.gr/). Environmental data is obtained from the EU Copernicus Marine Environment Monitoring Service (https://marine.copernicus.eu/).\nWe start by loading the required R packages.\n\nlibrary(glossa)\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Running GLOSSA on a regional scale"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#data-preparation",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#data-preparation",
    "title": "Example 3",
    "section": "Data preparation",
    "text": "Data preparation\n\nDownload occurrence data\nWe download occurrence data for Siganus luridus from the MedOBIS node, specifically from the GreekMarineICAS geodataset, part of the ALAS project. The dataset is available here (Sini et al, 2024).\nAfter downloading and unzipping the data, we filter it to retain records of Siganus luridus from 2000 to 2020, aligning with the environmental data’s timeframe. Since all records are presences, we set the pa column to 1. We then format the data, saving it as a tab-separated file with decimalLongitude, decimalLatitude, timestamp, and pa columns.\n\n# Unzip and read the data\ntmpdir &lt;- tempdir()\nzip_contents &lt;- utils::unzip(\"data/dwca-greekmarineicas_geodataset-v2.0.zip\", \n                             unzip = getOption(\"unzip\"), exdir = tmpdir)\nluridus &lt;- list.files(tmpdir, pattern = \"\\\\.txt$\", full.names = TRUE)\nluridus &lt;- read.csv2(luridus, header = TRUE, sep = \"\\t\")\n\n# Filter data for *Siganus luridus*\nluridus &lt;- luridus[luridus$scientificName == \"Siganus luridus\", ]\n\n# Select and rename columns of interest\nluridus &lt;- luridus[, c(\"decimalLongitude\", \"decimalLatitude\", \n                       \"eventDate\", \"occurrenceStatus\")]\nluridus$eventDate &lt;- as.numeric(sapply(strsplit(luridus$eventDate, \"/|-\"), \n                                       function(x) x[[1]]))\n\n# Filter data by event date to match COPERNICUS layers (2000-2020)\nluridus &lt;- luridus[luridus$eventDate &gt;= 2000 & luridus$eventDate &lt;= 2020, ]\n\n# Convert occurrence status to binary presence/absence\ntable(luridus$occurrenceStatus)\nluridus$occurrenceStatus &lt;- 1\ncolnames(luridus) &lt;- c(\"decimalLongitude\", \"decimalLatitude\", \"timestamp\", \"pa\")\n\n# Remove incomplete occurrences\nluridus &lt;- luridus[complete.cases(luridus), ]\n\n# Save cleaned data to file\nwrite.table(luridus, file = \"data/Siganus_luridus_occ.csv\", sep = \"\\t\", \n            dec = \".\", quote = FALSE)\n\n\n\nDownload environmental data\nWe download environmental data from the Copernicus Marine Service using their Python API. The data includes sea water temperature (thetao) and salinity (so) from the Mediterranean Sea Physics Reanalysis, and primary production (nppv) and dissolved oxygen (o2) from the Mediterranean Sea Biogeochemistry Reanalysis product. The data is downloaded yearly (2000-2020) at a grid resolution of 1/24 degrees for a depth range of 2 to 40 meters, which corresponds to the species’ preferred habitat, as described in FishBase (Gothel, 1992).\nimport copernicusmarine\n\n# Download temperature data version \"202211\"\ncopernicusmarine.subset(\n  dataset_id=\"cmems_mod_med_phy-tem_my_4.2km_P1Y-m\",\n  variables=[\"thetao\"],\n  minimum_longitude=19,\n  maximum_longitude=30,\n  minimum_latitude=34,\n  maximum_latitude=41.1,\n  start_datetime=\"2000-01-01T00:00:00\",\n  end_datetime=\"2020-12-31T23:59:00\",\n  minimum_depth=2,\n  maximum_depth=40,\n)\n\n# Download salinity data version \"202211\"\ncopernicusmarine.subset(\n  dataset_id=\"cmems_mod_med_phy-sal_my_4.2km_P1Y-m\",\n  variables=[\"so\"],\n  minimum_longitude=19,\n  maximum_longitude=30,\n  minimum_latitude=34,\n  maximum_latitude=41.1,\n  start_datetime=\"2000-01-01T00:00:00\",\n  end_datetime=\"2020-12-31T23:59:00\",\n  minimum_depth=2,\n  maximum_depth=40,\n)\n\n# Download biogeochemical data version \"202211\"\ncopernicusmarine.subset(\n  dataset_id=\"cmems_mod_med_bgc-bio_my_4.2km_P1Y-m\",\n  variables=[\"nppv\", \"o2\"],\n  minimum_longitude=19,\n  maximum_longitude=30,\n  minimum_latitude=34,\n  maximum_latitude=41.1,\n  start_datetime=\"2000-01-01T00:00:00\",\n  end_datetime=\"2020-12-31T23:59:00\",\n  minimum_depth=2,\n  maximum_depth=40,\n)\nOnce the data is downloaded, we compute the mean values of the environmental variables within the specified depth range. Apart from the dynamic variables, we also include the bathymetry as a static variable. We obtained the bathymetry from the ETOPO Global Relief Model, which can be downloaded from here. We downloaded the bedrock elevation netCDF version ETOPO 2022 with a 60 arc-second resolution.\n\n# Load biogeochemical variables\nbiogeochem_variables &lt;- terra::rast(\"data/cmems_mod_med_bgc-bio_my_4.2km_P1Y-m_nppv-o2_19.00E-30.00E_34.02N-41.06N_3.17-37.85m_2000-01-01-2020-01-01.nc\")\n\n# Extract and process layer names\nlayer_names &lt;- names(biogeochem_variables)\nlayer_names &lt;- strsplit(layer_names, \"_\")\nlayer_names &lt;- do.call(rbind, layer_names)\nlayer_names &lt;- as.data.frame(layer_names)\ncolnames(layer_names) &lt;- c(\"variable\", \"depth\", \"year\")\n\n# Compute mean values for each year across depths\nenv_data_year &lt;- list(\"nppv\" = list(), \"o2\" = list(), \"thetao\" = list(), \"so\" = list()) \nfor (variable in unique(layer_names$variable)){\n  for (year in unique(layer_names$year)){\n    indices &lt;- which(layer_names$variable == variable & layer_names$year == year)\n    print(paste(variable, year))\n    mean_water_column &lt;- terra::mean(biogeochem_variables[[indices]], na.rm = TRUE)\n    env_data_year[[variable]][[year]] &lt;- mean_water_column\n  }\n}\n\n# Load physical variables\nphysical_variables &lt;- c(\n  terra::rast(\"data/cmems_mod_med_phy-tem_my_4.2km_P1Y-m_thetao_19.00E-30.00E_34.02N-41.06N_3.17-37.85m_2000-01-01-2020-01-01.nc\"),\n  terra::rast(\"data/cmems_mod_med_phy-sal_my_4.2km_P1Y-m_so_19.00E-30.00E_34.02N-41.06N_3.17-37.85m_2000-01-01-2020-01-01.nc\")\n)\n\n# Process physical variables\nlayer_names &lt;- names(physical_variables)\nlayer_names &lt;- strsplit(layer_names, \"_\")\nlayer_names &lt;- do.call(rbind, layer_names)\nlayer_names &lt;- as.data.frame(layer_names)\ncolnames(layer_names) &lt;- c(\"variable\", \"depth\", \"year\")\n\n# Mean between different depths\nfor (variable in unique(layer_names$variable)){\n  for (year in unique(layer_names$year)){\n    indices &lt;- which(layer_names$variable == variable & layer_names$year == year)\n    print(paste(variable, year))\n    mean_water_column &lt;- terra::mean(physical_variables[[indices]], na.rm = TRUE)\n    env_data_year[[variable]][[year]] &lt;- mean_water_column\n  }\n}\n\n# Load and process bathymetry data\nbat &lt;- terra::rast(\"data/ETOPO_2022_v1_60s_N90W180_bed.nc\")\nbat &lt;- -1*bat\nbat &lt;- terra::aggregate(bat, fact = 5, fun = \"mean\")\nr &lt;- terra::rast(terra::ext(env_data_year[[1]][[1]]), \n                 res = terra::res(env_data_year[[1]][[1]]))\nbat &lt;- terra::resample(bat, r)\nfor (i in seq_len(length(env_data_year[[1]]))){\n  env_data_year[[\"bat\"]][[i]] &lt;- bat\n}\n\n# Save processed layers to files\ndir.create(\"data/fit_layers\")\ndir.create(\"data/fit_layers/bat\")\ndir.create(\"data/fit_layers/thetao\")\ndir.create(\"data/fit_layers/so\")\ndir.create(\"data/fit_layers/nppv\")\ndir.create(\"data/fit_layers/o2\")\nfor (i in seq_len(length(env_data_year[[1]]))){\n  for (j in names(env_data_year)){\n    terra::writeRaster(\n      env_data_year[[j]][[i]], \n      filename = paste0(\"data/fit_layers/\", j ,\"/\", j, \"_\", i, \".tif\")\n    )\n  }\n}\n\n# Compress the layers into a zip file\nzip(zipfile = \"data/fit_layers.zip\", files = \"data/fit_layers\")\n\nAdditionally, we created a proj_layers.zip file containing the layers for 2020, allowing us to predict the native range and suitable habitat under “present” conditions.\nproj_layers.zip\n    ├───bat\n    │       bat_21.tif\n    ├───nppv\n    │       nppv_21.tif\n    ├───o2\n    │       o2_21.tif\n    ├───so\n    │       so_21.tif\n    └───thetao\n            thetao_21.tif",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Running GLOSSA on a regional scale"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#glossa-modeling",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#glossa-modeling",
    "title": "Example 3",
    "section": "GLOSSA modeling",
    "text": "GLOSSA modeling\nWith the data formatted and ready, we run the GLOSSA Shiny app.\n\nrun_glossa()\n\nWe upload the occurrence file for S. luridus and the environmental layers for model fitting and projection (in this case, the year 2020). To compute the native range and suitable habitat, we select the Model fitting and Model projection options for both the Native range and Suitable habitat models. Additionally, we enable Functional responses, Variable importance and Cross-validation in the Others section.\nIn the advanced options, we standardize the environmental data, choose the BART model (Chipman, et al., 2010; Dorie, 2024), and set the seed to 1984 for reproducibility.\n\n\n\nAnalysis options set in GLOSSA for modeling the distribution of Siganus luridus.",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Running GLOSSA on a regional scale"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#results",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#results",
    "title": "Example 3",
    "section": "Results",
    "text": "Results\nThe model was fitted with 256 presences and 256 pseudo-absences. Many presence points were excluded due to their proximity to the coast, where the environmental layers from COPERNICUS lack data. Future analyses could benefit from using environmental data sources with better coastal resolution or imputing values. For this example, this exclusion demonstrates how GLOSSA handles these situations.\n\n\n\nDiscarded records of Siganus luridus during occurrence processing.\n\n\nDespite the data limitations, the model summary indicates good performance, with clear classification between presences and pseudo-absences for both models.\n\n\n\nSummary of the fitted models for Siganus luridus.\n\n\nSimilarly, the 5-fold cross-validation results show a high F-score for both the native range and suitable habitat models, suggesting strong predictive performance. However, keep in mind that this k-fold cross-validation is random and does not use temporal or spatial blocks, which could be relevant for more robust assessments.\n\n\n\nK-fold cross-validation for the native range and suitable habitat models.\n\n\nThe figure below shows the native range and suitable habitat predictions for 2020. The native range represents the species’ current distribution, considering spatial smoothing with latitude and longitude included as predictors in the model. In contrast, the suitable habitat map highlights areas with favorable environmental conditions for the species, which may not yet be occupied.\n\n\n\nPredicted native range and suitable habitat of Siganus luridus in 2020.\n\n\nFor example, the Thracian Sea and northeast Aegean islands were identified as high-risk areas for potential invasion, even though the species is not currently established there according to the predicted native range in 2020.\n\n\n\nDifference between mean suitable habitat and native range.",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Running GLOSSA on a regional scale"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#conclusion",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#conclusion",
    "title": "Example 3",
    "section": "Conclusion",
    "text": "Conclusion\nUsing GLOSSA and environmental data, we successfully modeled the potential suitable habitat for Siganus luridus and identified areas at risk of invasion. This information is critical for understanding and managing the impacts of invasive species on marine ecosystems.",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Running GLOSSA on a regional scale"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#computation-time",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#computation-time",
    "title": "Example 3",
    "section": "Computation time",
    "text": "Computation time\nThe table below summarizes the computation times for the various steps of the GLOSSA analysis, providing an overview of the computational resources required at each step. The analysis was performed on a single Windows 11 machine equipped with 64 GB of RAM and an Intel(R) Core(TM) i7-1165G7 processor. This processor features 4 cores and 8 threads, with a base clock speed of 2.80 GHz.\n\nTable 1. Computation times for different steps of the GLOSSA analysis for Siganus luridus in the Greek Seas.\n\n\n\n\nTask\nExecution Time\n\n\n\n\nLoading input data\n1.92 secs\n\n\nProcessing P/A coordinates\n0.004 secs\n\n\nProcessing covariate layers\n1.23 secs\n\n\nBuilding model matrix\n1.26 secs\n\n\nFitting native range models\n0.018 mins\n\n\nVariable importance (native range)\n0.60 mins\n\n\nP/A cutoff (native range)\n0.009 mins\n\n\nProjections on fit layers (native range)\n0.997 mins\n\n\nNative range projections\n0.42 mins\n\n\nNative range\n1.45 mins\n\n\nFitting suitable habitat models\n0.016 mins\n\n\nVariable importance (suitable habitat)\n0.41 mins\n\n\nP/A cutoff (suitable habitat)\n0.008 mins\n\n\nProjections on fit layers (suitable habitat)\n0.79 mins\n\n\nSuitable habitat projections\n0.38 mins\n\n\nHabitat suitability\n0.002 mins\n\n\nSuitable habitat\n1.20 mins\n\n\nComputing functional responses\n0.69 mins\n\n\nCross-validation\n0.48 mins\n\n\nModel summary\n0.016 mins\n\n\nTotal GLOSSA analysis\n3.91 mins",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Running GLOSSA on a regional scale"
    ]
  },
  {
    "objectID": "pages/tutorials_examples/Siganus_luridus_example3.html#references",
    "href": "pages/tutorials_examples/Siganus_luridus_example3.html#references",
    "title": "Example 3",
    "section": "References",
    "text": "References\n\nChipman, H. A., George, E. I., & McCulloch, R. E. (2010). BART: Bayesian additive regression trees.\nDorie V (2024). dbarts: Discrete Bayesian Additive Regression Trees Sampler. R package version 0.9-28, https://CRAN.R-project.org/package=dbarts.\nGothel, H. (1992). Fauna marina del Mediterráneo. Ediciones Omega, SA, Barcelona, 319.\nSini M, Ragkousis M, Koukourouvli N, Katsanevakis S & Zenetos A (2024). Marine impactful cryptogenic and alien species in the Greek Seas: A georeferenced dataset (1893-2020). Version 2.0. Hellenic Center for Marine Research. Occurrence dataset. https://doi.org/10.25607/t2smha",
    "crumbs": [
      "Documentation",
      "Tutorials and examples",
      "Running GLOSSA on a regional scale"
    ]
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Explore research articles, reports, and publications that use GLOSSA for species distribution modeling in marine and ecological research.\n\n\n\n\n\nFuster-Alonso, A., Mestre-Tomás, J., Baez, J. C., Pennino, M. G., Barber, X., Bellido, J. M., … & Coll, M. (2024). Machine learning applied to global scale species distribution models (SDMs). PREPRINT (Version 1) available at Research Square. doi: https://doi.org/10.21203/rs.3.rs-4411399/v1\n\n\n\n\n\nHave you used GLOSSA in your research? We would love to showcase your publication! Please contact us with the details of your work."
  },
  {
    "objectID": "publications.html#full-list-of-publications",
    "href": "publications.html#full-list-of-publications",
    "title": "Publications",
    "section": "",
    "text": "Fuster-Alonso, A., Mestre-Tomás, J., Baez, J. C., Pennino, M. G., Barber, X., Bellido, J. M., … & Coll, M. (2024). Machine learning applied to global scale species distribution models (SDMs). PREPRINT (Version 1) available at Research Square. doi: https://doi.org/10.21203/rs.3.rs-4411399/v1"
  },
  {
    "objectID": "publications.html#submit-your-work",
    "href": "publications.html#submit-your-work",
    "title": "Publications",
    "section": "",
    "text": "Have you used GLOSSA in your research? We would love to showcase your publication! Please contact us with the details of your work."
  }
]